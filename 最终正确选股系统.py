#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - æ ¹æ®æ¯å¤©å®æ—¶æ•°æ®ç­›é€‰
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ†
æ–°å¢ï¼š
1. ä½¿ç”¨ Optuna è¿›è¡Œç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–ï¼Œæå‡è¯„åˆ†ç²¾åº¦å’ŒåŒºåˆ†åº¦ã€‚
2. å¼•å…¥å…³è”è§„åˆ™æŒ–æ˜ï¼Œåˆ†æå“ªäº›æ¡ä»¶ç»„åˆæ›´å®¹æ˜“äº§ç”Ÿé«˜æ”¶ç›Šï¼Œæä¾›ç­–ç•¥æ´å¯Ÿã€‚
3. ä¼˜åŒ–æ•°æ®å¤„ç†å’Œè¾“å‡ºå±•ç¤ºã€‚
4. ä¼˜åŒ–ä»£ç å‡†ç¡®æ€§ã€è´¨é‡å’Œæ•ˆç‡ã€‚
5. ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç¥ç»ç½‘ç»œçš„ç›®æ ‡å˜é‡ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚
6. é›†æˆ XGBoost å’Œ TA-Lib è¿›è¡Œç‰¹å¾å·¥ç¨‹å’Œæ¨¡å‹è®­ç»ƒã€‚
7. åŸºäºç‰¹å¾å’Œé¢„æµ‹ç»“æœï¼Œæä¾›çŸ­æœŸ/é•¿æœŸä¹°å…¥/å–å‡ºç­–ç•¥å»ºè®®ã€‚
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ç¥ç»ç½‘ç»œç›¸å…³åº“
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score

# å¯¼å…¥ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# å¯¼å…¥ XGBoost
import xgboost as xgb

# å¯¼å…¥ TA-Lib (å¦‚æœæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ï¼špip install TA-Lib)
import talib

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 10000
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 100000000
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) / 10000
        return float(s)
    except ValueError:
        return np.nan

def calculate_features(row):
    """
    æ ¹æ®è‹æ°é‡åŒ–ç­–ç•¥è®¡ç®—ç‰¹å¾å€¼ï¼Œç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒå’Œå…³è”è§„åˆ™æŒ–æ˜ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ•°å€¼ç‰¹å¾çš„åˆ—è¡¨ã€‚
    """
    features = []

    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶ (0æˆ–1)
    try:
        low = safe_float(row.get('æœ€ä½'))
        ma60 = safe_float(row.get('60æ—¥å‡ä»·'))
        ma20 = safe_float(row.get('20æ—¥å‡ä»·'))
        current = safe_float(row.get('æœ€æ–°'))

        f_condition = 0
        if pd.notna(low) and pd.notna(ma60) and ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
            f_condition = 1
        elif pd.notna(current) and pd.notna(ma20) and ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
            f_condition = 1
        features.append(f_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½® (0æˆ–1)
    try:
        change = safe_float(row.get('æ¶¨å¹…%'))
        current = safe_float(row.get('æœ€æ–°'))
        high = safe_float(row.get('æœ€é«˜'))
        low = safe_float(row.get('æœ€ä½'))

        g_condition = 0
        if pd.notna(change) and change >= 5.0 and pd.notna(current) and pd.notna(high) and pd.notna(low):
            if (high - low) > 0: # é¿å…é™¤ä»¥é›¶
                threshold = high - (high - low) * 0.30
                if current >= threshold:
                    g_condition = 1
            elif current == high: # å¦‚æœæœ€é«˜æœ€ä½ç›¸åŒï¼Œä¸”æ¶¨å¹…>=5ï¼Œä¹Ÿç®—æ»¡è¶³
                g_condition = 1
        features.append(g_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # Håˆ—ï¼šå½’å±å‡€åˆ©æ¶¦ (æ•°å€¼ï¼Œå•ä½äº¿)
    try:
        profit = safe_float(row.get('å½’å±å‡€åˆ©æ¶¦'))
        features.append(profit if pd.notna(profit) else 0)
    except:
        features.append(0)

    # Iåˆ—ï¼šå®é™…æ¢æ‰‹ç‡ (æ•°å€¼)
    try:
        turnover = safe_float(row.get('å®é™…æ¢æ‰‹%'))
        features.append(turnover if pd.notna(turnover) else 100) # ç¼ºå¤±æ—¶ç»™ä¸€ä¸ªè¾ƒå¤§å€¼
    except:
        features.append(100)

    # Jåˆ—ï¼šæ€»å¸‚å€¼ (æ•°å€¼ï¼Œå•ä½äº¿)
    try:
        cap = safe_float(row.get('æ€»å¸‚å€¼'))
        features.append(cap if pd.notna(cap) else 0)
    except:
        features.append(0)

    return features

def add_technical_indicators(df):
    """
    ä½¿ç”¨ TA-Lib æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ã€‚
    """
    # ç¡®ä¿æ‰€éœ€çš„åˆ—å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
    for col in ['æœ€æ–°', 'æœ€é«˜', 'æœ€ä½', 'å¼€ç›˜', 'æ˜¨æ”¶']:
        if col not in df.columns:
            df[col] = np.nan
        df[col] = df[col].apply(safe_float)

    # ç¡®ä¿æ•°æ®æŒ‰æ—¶é—´é¡ºåºæ’åˆ—(å¦‚æœé€‚ç”¨)
    # df = df.sort_index(ascending=True)

    # ç®€å•ç§»åŠ¨å¹³å‡çº¿ (SMA)
    df['SMA_5'] = talib.SMA(df['æœ€æ–°'], timeperiod=5)
    df['SMA_20'] = talib.SMA(df['æœ€æ–°'], timeperiod=20)

    # æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿ (EMA)
    df['EMA_12'] = talib.EMA(df['æœ€æ–°'], timeperiod=12)
    df['EMA_26'] = talib.EMA(df['æœ€æ–°'], timeperiod=26)

    # ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ (RSI)
    df['RSI'] = talib.RSI(df['æœ€æ–°'], timeperiod=14)

    # ç§»åŠ¨å¹³å‡æ”¶æ•›/å‘æ•£ (MACD)
    macd, macdsignal, macdhist = talib.MACD(df['æœ€æ–°'], fastperiod=12, slowperiod=26, signalperiod=9)
    df['MACD'] = macd
    df['MACD_SIGNAL'] = macdsignal
    df['MACD_HIST'] = macdhist

    # å¸ƒæ—å¸¦ (Bollinger Bands)
    upper, middle, lower = talib.BBANDS(df['æœ€æ–°'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)
    df['BB_UPPER'] = upper
    df['BB_MIDDLE'] = middle
    df['BB_LOWER'] = lower

    # æˆäº¤é‡æŒ‡æ ‡ (ä¾‹å¦‚ï¼Œæˆäº¤é‡å˜åŒ–ç‡)
    # ç¡®ä¿ 'å®é™…æ¢æ‰‹%' åˆ—å­˜åœ¨ä¸”ä¸ºæ•°å€¼ç±»å‹
    if 'å®é™…æ¢æ‰‹%' not in df.columns:
        df['å®é™…æ¢æ‰‹%'] = np.nan
    df['å®é™…æ¢æ‰‹%'] = df['å®é™…æ¢æ‰‹%'].apply(safe_float)

    df['VOLUME_CHG'] = df['å®é™…æ¢æ‰‹%'].diff()

    # åŠ¨é‡æŒ‡æ ‡ (ä¾‹å¦‚ï¼ŒåŠ¨é‡å˜åŒ–ç‡)
    df['MOM'] = talib.MOM(df['æœ€æ–°'], timeperiod=10)

    return df

def objective(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•° (XGBoost)
    """
    params = {
        'objective': 'reg:squarederror',  # å›å½’ä»»åŠ¡
        'eval_metric': 'rmse',
        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),
        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),
        'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.1),
        'random_state': 42,
    }

    if params['booster'] == 'gbtree' or params['booster'] == 'dart':
        params['max_depth'] = trial.suggest_int('max_depth', 3, 9)
        params['min_child_weight'] = trial.suggest_int('min_child_weight', 1, 10)
        params['subsample'] = trial.suggest_float('subsample', 0.5, 1.0)
        params['colsample_bytree'] = trial.suggest_float('colsample_bytree', 0.5, 1.0)
        params['colsample_bylevel'] = trial.suggest_float('colsample_bylevel', 0.5, 1.0)
    elif params['booster'] == 'gblinear':
        params['updater'] = trial.suggest_categorical('updater', ['coord_descent', 'shotgun'])

    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))  # ä½¿ç”¨ RMSE
    return rmse

def train_xgboost_model(df):
    """
    è®­ç»ƒ XGBoost æ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†ï¼Œä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
    ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç›®æ ‡å˜é‡ã€‚
    """
    print("\n   å‡†å¤‡è®­ç»ƒæ•°æ® (XGBoost)...")
    X = []

    for _, row in df.iterrows():
        features = calculate_features(row)
        X.append(features)

    X = np.array(X)

    # æå–ç”¨äºè®¡ç®—è´¨é‡è¯„åˆ†çš„åˆ—
    change = df['æ¶¨å¹…%'].apply(safe_float)
    profit = df['å½’å±å‡€åˆ©æ¶¦'].apply(safe_float)
    turnover = df['å®é™…æ¢æ‰‹%'].apply(safe_float)
    market_cap = df['æ€»å¸‚å€¼'].apply(safe_float)
    pe_ratio = df['å¸‚ç›ˆç‡(åŠ¨)'].apply(safe_float)

    # å½’ä¸€åŒ–å„ä¸ªæŒ‡æ ‡ (ä½¿ç”¨ min-max å½’ä¸€åŒ–)
    change_norm = (change - change.min()) / (change.max() - change.min())
    profit_norm = (profit - profit.min()) / (profit.max() - profit.min())
    turnover_norm = (turnover - turnover.min()) / (turnover.max() - turnover.min())
    market_cap_norm = (market_cap - market_cap.min()) / (market_cap.max() - market_cap.min())
    pe_ratio_norm = (pe_ratio - pe_ratio.min()) / (pe_ratio.max() - pe_ratio.min())

    # å¤„ç† NaN å€¼ï¼Œç”¨ 0 å¡«å……
    change_norm = change_norm.fillna(0)
    profit_norm = profit_norm.fillna(0)
    turnover_norm = turnover_norm.fillna(0)
    market_cap_norm = market_cap_norm.fillna(0)
    pe_ratio_norm = pe_ratio_norm.fillna(0)

    # è®¡ç®—å¤åˆè´¨é‡è¯„åˆ† (å¯ä»¥è°ƒæ•´æƒé‡)
    df['quality_score'] = (
        0.3 * change_norm +  # æ¶¨å¹…
        0.25 * profit_norm +  # å‡€åˆ©æ¶¦
        0.15 * (1 - abs(turnover_norm - 0.5)) +  # æ¢æ‰‹ç‡ (é€‚ä¸­æœ€å¥½)
        0.2 * market_cap_norm +  # å¸‚å€¼
        0.1 * (1 - pe_ratio_norm)  # å¸‚ç›ˆç‡ (è¶Šä½è¶Šå¥½)
    )

    y = df['quality_score'].values

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]

    if len(X) < 20: # è‡³å°‘éœ€è¦ä¸€äº›æ•°æ®æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print("   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ XGBoost æ¨¡å‹ã€‚")
        return None, None

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•° (XGBoost): {len(X)}")

    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    print("   æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾...")
    df = add_technical_indicators(df.copy()) # ä½¿ç”¨å‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    technical_indicators = [col for col in df.columns if col.startswith(('SMA', 'EMA', 'RSI', 'MACD', 'BB', 'VOLUME', 'MOM'))]

    # æå–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
    X_technical = df[technical_indicators].values
    X_technical = X_technical[mask] # åº”ç”¨ç›¸åŒçš„ mask

    # åˆå¹¶åŸºæœ¬ç‰¹å¾å’ŒæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
    X = np.concatenate((X, X_technical), axis=1)

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Optuna è¶…å‚æ•°ä¼˜åŒ–
    print("   å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (XGBoost, å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
    try:
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=30, show_progress_bar=True) # å‡å°‘ trials
    except Exception as e:
        print(f"   Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ (XGBoost): {e}")
        print("   å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
        # å¦‚æœOptunaå¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªåˆç†çš„é»˜è®¤é…ç½®
        best_params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'booster': 'gbtree',
            'lambda': 1.0,
            'alpha': 0.0,
            'learning_rate': 0.1,
            'max_depth': 6,
            'min_child_weight': 1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'colsample_bylevel': 0.8,
            'random_state': 42
        }
    else:
        print("\n   Optuna ä¼˜åŒ–å®Œæˆ (XGBoost)ã€‚")
        print(f"   æœ€ä½³ RMSE: {study.best_value:.4f}")
        print(f"   æœ€ä½³è¶…å‚æ•°: {study.best_params}")
        best_params = study.best_params
        best_params['objective'] = 'reg:squarederror'
        best_params['eval_metric'] = 'rmse'
        best_params['random_state'] = 42

    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆ XGBoost æ¨¡å‹...")
    model = xgb.XGBRegressor(**best_params)
    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=50, verbose=False)

    # è¯„ä¼°æ¨¡å‹
    print("   è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"   æœ€ç»ˆæ¨¡å‹å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   æœ€ç»ˆæ¨¡å‹RÂ²åˆ†æ•°: {r2:.4f}")

    return model, scaler, technical_indicators

def predict_score_with_xgboost(row, model, scaler, technical_indicators, df):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ XGBoost æ¨¡å‹é¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """
    features = calculate_features(row)

    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    row_df = pd.DataFrame([row])  # å°†è¡Œè½¬æ¢ä¸º DataFrame
    row_df = add_technical_indicators(row_df)  # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡

    # æå–æŠ€æœ¯æŒ‡æ ‡å€¼
    technical_values = row_df[technical_indicators].values.flatten().tolist()

    # åˆå¹¶åŸºæœ¬ç‰¹å¾å’ŒæŠ€æœ¯æŒ‡æ ‡
    features.extend(technical_values)
    features = np.array(features).reshape(1, -1)  # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„

    # æ£€æŸ¥ç‰¹å¾ä¸­æ˜¯å¦æœ‰NaNæˆ–Infï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¿”å›ä¸€ä¸ªé»˜è®¤å€¼æˆ–NaN
    if any(pd.isna(f) or np.isinf(f) for f in features.flatten()):
        return np.nan  # æˆ–è€…ä¸€ä¸ªéå¸¸ä½çš„é»˜è®¤åˆ†æ•°

    try:
        features_scaled = scaler.transform(features)
        score = model.predict(features_scaled)[0]
        return score
    except Exception as e:
        # print(f"é¢„æµ‹åˆ†æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}, ç‰¹å¾: {features}")
        return np.nan  # é¢„æµ‹å¤±è´¥æ—¶è¿”å›NaN

def generate_trading_strategy(stock, df, model, scaler, technical_indicators):
    """
    åŸºäºè‚¡ç¥¨ç‰¹å¾å’Œé¢„æµ‹ç»“æœï¼Œç”Ÿæˆäº¤æ˜“ç­–ç•¥å»ºè®®ã€‚
    """
    code = stock['ä»£ç ']
    stock_row = df[df['åŸå§‹ä»£ç '] == code].iloc[0]  # è·å–è‚¡ç¥¨çš„å®Œæ•´è¡Œæ•°æ®

    # è·å–è‚¡ç¥¨çš„è¯¦ç»†æ•°æ®
    current_price = safe_float(stock_row['æœ€æ–°'])
    change_percent = safe_float(stock_row['æ¶¨å¹…%'])
    volume = safe_float(stock_row['å®é™…æ¢æ‰‹%'])
    market_cap = safe_float(stock_row['æ€»å¸‚å€¼'])
    quality_score = stock['ä¼˜è´¨ç‡']

    # æŠ€æœ¯æŒ‡æ ‡åˆ†æ
    sma_5 = safe_float(stock_row.get('SMA_5', np.nan))
    sma_20 = safe_float(stock_row.get('SMA_20', np.nan))
    rsi = safe_float(stock_row.get('RSI', np.nan))
    macd = safe_float(stock_row.get('MACD', np.nan))
    macd_signal = safe_float(stock_row.get('MACD_SIGNAL', np.nan))

    # åŸºæœ¬é¢åˆ†æ
    profit = safe_float(stock_row['å½’å±å‡€åˆ©æ¶¦'])
    pe_ratio = safe_float(stock_row['å¸‚ç›ˆç‡(åŠ¨)'])

    strategy = {
        'ä»£ç ': code,
        'åç§°': stock['åç§°'],
        'å»ºè®®': 'æŒæœ‰è§‚æœ›',  # é»˜è®¤å»ºè®®
        'ç†ç”±': [],
        'ä»“ä½': 'è½»ä»“', # é»˜è®¤è½»ä»“
        'æ­¢æŸ': None,
        'æ­¢ç›ˆ': None,
        'å‘¨æœŸ': 'çŸ­æœŸ' # é»˜è®¤çŸ­æœŸ
    }

    # çŸ­æœŸç­–ç•¥ (1-5å¤©)
    if change_percent > 3 and volume > 5:
        strategy['å‘¨æœŸ'] = 'çŸ­æœŸ'
        if current_price > sma_5 and macd > macd_signal:
            strategy['å»ºè®®'] = 'è°¨æ…ä¹°å…¥'
            strategy['ç†ç”±'].append('çŸ­æœŸå‡çº¿å‘ˆä¸Šå‡è¶‹åŠ¿')
            strategy['ç†ç”±'].append('MACDé‡‘å‰')
            strategy['ä»“ä½'] = 'ä¸­ä»“'
            strategy['æ­¢æŸ'] = current_price * 0.98 # 2% æ­¢æŸ
            strategy['æ­¢ç›ˆ'] = current_price * 1.05 # 5% æ­¢ç›ˆ
        elif rsi > 70:
            strategy['å»ºè®®'] = 'ä¸å»ºè®®è¿½é«˜'
            strategy['ç†ç”±'].append('RSIè¿‡é«˜ï¼Œå¯èƒ½è¶…ä¹°')
        else:
            strategy['å»ºè®®'] = 'æŒæœ‰è§‚æœ›'
            strategy['ç†ç”±'].append('çŸ­æœŸè¶‹åŠ¿ä¸æ˜æœ—')

    # é•¿æœŸç­–ç•¥ (1ä¸ªæœˆä»¥ä¸Š)
    elif quality_score > 0.7 and profit > 0 and pe_ratio > 0 and pe_ratio < 30:
        strategy['å‘¨æœŸ'] = 'é•¿æœŸ'
        strategy['å»ºè®®'] = 'é€¢ä½ä¹°å…¥'
        strategy['ç†ç”±'].append('åŸºæœ¬é¢è‰¯å¥½ï¼Œç›ˆåˆ©ç¨³å®š')
        strategy['ç†ç”±'].append('å¸‚ç›ˆç‡åˆç†')
        strategy['ä»“ä½'] = 'å°ä»“ä½'
        strategy['æ­¢æŸ'] = current_price * 0.9 # 10% æ­¢æŸ
        strategy['æ­¢ç›ˆ'] = None  # é•¿æœŸæŠ•èµ„ä¸è®¾æ­¢ç›ˆ

    # é£é™©æ§åˆ¶
    elif change_percent < -3 or volume > 20:
        strategy['å»ºè®®'] = 'è°¨æ…ï¼Œæ³¨æ„é£é™©'
        strategy['ç†ç”±'].append('ä»Šæ—¥è·Œå¹…è¾ƒå¤§æˆ–æ¢æ‰‹ç‡è¿‡é«˜')
        strategy['ä»“ä½'] = 'ç©ºä»“'

    # è¡¥å……ç†ç”±
    if quality_score > 0.8:
        strategy['ç†ç”±'].append('ç¥ç»ç½‘ç»œè¯„åˆ†é«˜')
    elif quality_score < 0.3:
        strategy['ç†ç”±'].append('ç¥ç»ç½‘ç»œè¯„åˆ†ä½')

    return strategy

def perform_association_rule_mining(df):
    """
    ä½¿ç”¨å…³è”è§„åˆ™æŒ–æ˜æ¥å‘ç°è‹æ°é‡åŒ–ç­–ç•¥æ¡ä»¶ä¸é«˜æ¶¨å¹…ä¹‹é—´çš„å…³ç³»ã€‚
    """
    print("\n4. æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜...")

    # å‡†å¤‡æ•°æ®ï¼šå°†ç‰¹å¾å’Œç›®æ ‡å˜é‡äºŒå€¼åŒ–
    data_for_ar = []
    for _, row in df.iterrows():
        features = calculate_features(row)
        items = []

        # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶
        if features[0] == 1:
            items.append("F_ä»·æ ¼ä½ç½®_æ»¡è¶³")
        else:
            items.append("F_ä»·æ ¼ä½ç½®_ä¸æ»¡è¶³")

        # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½®
        if features[1] == 1:
            items.append("G_æ¶¨å¹…ä½ç½®_æ»¡è¶³")
        else:
            items.append("G_æ¶¨å¹…ä½ç½®_ä¸æ»¡è¶³")

        # Håˆ—ï¼šå‡€åˆ©æ¶¦>=3000ä¸‡ (0.3äº¿)
        if features[2] >= 0.3:
            items.append("H_å‡€åˆ©æ¶¦_é«˜")
        else:
            items.append("H_å‡€åˆ©æ¶¦_ä½")

        # Iåˆ—ï¼šæ¢æ‰‹ç‡<=20%
        if features[3] <= 20:
            items.append("I_æ¢æ‰‹ç‡_ä½")
        else:
            items.append("I_æ¢æ‰‹ç‡_é«˜")

        # Jåˆ—ï¼šå¸‚å€¼>=300äº¿
        if features[4] >= 300:
            items.append("J_å¸‚å€¼_å¤§")
        else:
            items.append("J_å¸‚å€¼_å°")

        # ç›®æ ‡å˜é‡ï¼šé«˜æ¶¨å¹… (ä¾‹å¦‚ï¼Œæ¶¨å¹… > 2%)
        change = safe_float(row.get('æ¶¨å¹…%'))
        if pd.notna(change) and change > 2.0: # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
            items.append("é«˜æ¶¨å¹…")
        else:
            items.append("ä½æ¶¨å¹…")

        data_for_ar.append(items)

    if not data_for_ar:
        print("   âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ã€‚")
        return

    te = TransactionEncoder()
    te_ary = te.fit(data_for_ar).transform(data_for_ar)
    df_ar = pd.DataFrame(te_ary, columns=te.columns_)

    # æŸ¥æ‰¾é¢‘ç¹é¡¹é›†
    # min_support å¯ä»¥æ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼Œå¤ªå°è§„åˆ™å¤ªå¤šï¼Œå¤ªå¤§è§„åˆ™å¤ªå°‘
    frequent_itemsets = apriori(df_ar, min_support=0.01, use_colnames=True)
    if frequent_itemsets.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·å°è¯•é™ä½ min_supportã€‚")
        return

    # ç”Ÿæˆå…³è”è§„åˆ™
    # min_confidence è¶Šé«˜ï¼Œè§„åˆ™è¶Šå¯é 
    # lift > 1 è¡¨ç¤ºå‰ä»¶å’Œåä»¶æ­£ç›¸å…³
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.1)

    if rules.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°æœ‰æ„ä¹‰çš„å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½ min_threshold æˆ–æ£€æŸ¥æ•°æ®ã€‚")
        return

    # ç­›é€‰å¹¶æ‰“å°ä¸â€œé«˜æ¶¨å¹…â€ç›¸å…³çš„è§„åˆ™
    high_return_rules = rules[rules['consequents'].apply(lambda x: 'é«˜æ¶¨å¹…' in x)]
    high_return_rules = high_return_rules.sort_values(by=['lift', 'confidence'], ascending=False)

    print("\n   å‘ç°ä»¥ä¸‹ä¸ 'é«˜æ¶¨å¹…' ç›¸å…³çš„å…³è”è§„åˆ™ (æŒ‰ Lift é™åº):")
    if high_return_rules.empty:
        print("   æœªæ‰¾åˆ°ç›´æ¥å¯¼è‡´ 'é«˜æ¶¨å¹…' çš„å…³è”è§„åˆ™ã€‚")
    else:
        for i, rule in high_return_rules.head(10).iterrows(): # åªæ˜¾ç¤ºå‰10æ¡
            antecedent_str = ', '.join(list(rule['antecedents']))
            consequent_str = ', '.join(list(rule['consequents']))
            print(f"   è§„åˆ™ {i+1}: {antecedent_str} => {consequent_str}")
            print(f"     æ”¯æŒåº¦ (Support): {rule['support']:.4f}")
            print(f"     ç½®ä¿¡åº¦ (Confidence): {rule['confidence']:.4f}")
            print(f"     æå‡åº¦ (Lift): {rule['lift']:.4f}")
            print("-" * 40)

    print("\n   å…³è”è§„åˆ™æŒ–æ˜å®Œæˆã€‚è¿™äº›è§„åˆ™å¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ´å¯Ÿã€‚")

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆ XGBoost, TA-Lib ä¸å…³è”è§„åˆ™)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ®...")

    df = pd.DataFrame()
    # å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        # ç»Ÿä¸€åˆ—å
        df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°',
            'æ¶¨è·Œå¹…': 'æ¶¨å¹…%',
            'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)' # ç¡®ä¿åˆ—åä¸€è‡´
        }, inplace=True)

        # ä¿å­˜åŸå§‹ä»£ç 
        df_realtime['åŸå§‹ä»£ç '] = df_realtime['ä»£ç '].copy()

        # æ ¼å¼åŒ–ä»£ç 
        df_realtime['ä»£ç '] = df_realtime['ä»£ç '].apply(lambda x: f'= "{str(x)}"')

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨ï¼Œå¹¶åˆå§‹åŒ–ä¸ºNoneæˆ–é»˜è®¤å€¼
        required_cols = ['ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%',
                         'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼',
                         'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'åŸå§‹ä»£ç ']
        for col in required_cols:
            if col not in df_realtime.columns:
                df_realtime[col] = np.nan # ä½¿ç”¨NaNæ–¹ä¾¿åç»­å¤„ç†

        df = df_realtime

    except Exception as e:
        print(f"   âŒ å®æ—¶è·å–å¤±è´¥: {e}")
        print("   ä½¿ç”¨å‚è€ƒæ•°æ®ä½œä¸ºå¤‡é€‰...")

        # ä½¿ç”¨å‚è€ƒæ•°æ®
        try:
            df_ref = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
            print(f"   âœ… ä»å‚è€ƒæ–‡ä»¶åŠ è½½äº† {len(df_ref)} æ¡æ•°æ®")
            df_ref['åŸå§‹ä»£ç '] = df_ref['ä»£ç '].str.replace('= "', '').str.replace('"', '')
            df = df_ref
        except Exception as e2:
            print(f"   âŒ æ— æ³•åŠ è½½å‚è€ƒæ•°æ®: {e2}")
            return

    # å°è¯•è¡¥å……å‡çº¿å’Œè´¢åŠ¡æ•°æ® (å¦‚æœå®æ—¶æ•°æ®ç¼ºå¤±)
    try:
        ref_df_path = 'å‚è€ƒæ•°æ®/Table.xls'
        if os.path.exists(ref_df_path):
            ref_df = pd.read_csv(ref_df_path, sep='\t', encoding='gbk', dtype=str)
            ref_map = {}
            for _, row in ref_df.iterrows():
                code = str(row['ä»£ç ']).replace('= "', '').replace('"', '')
                ref_map[code] = row.to_dict()

            # åˆå¹¶å‚è€ƒæ•°æ®
            merged_count = 0
            for i, row in df.iterrows():
                code = row.get('åŸå§‹ä»£ç ')
                if code and code in ref_map:
                    ref = ref_map[code]
                    # è¡¥å……ç¼ºå¤±çš„æ•°æ®
                    for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'æ‰€å±è¡Œä¸š', 'å½’å±å‡€åˆ©æ¶¦', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡(åŠ¨)']:
                        if col in ref and pd.isna(df.loc[i, col]): # åªè¡¥å……NaNçš„å€¼
                            df.loc[i, col] = ref[col]
                    merged_count += 1
            print(f"   âœ… è¡¥å……äº† {merged_count} æ¡å‚è€ƒæ•°æ®")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°å‚è€ƒæ•°æ®æ–‡ä»¶ 'å‚è€ƒæ•°æ®/Table.xls'ï¼Œæ— æ³•è¡¥å……æ•°æ®ã€‚")
    except Exception as e:
        print(f"   âš ï¸ è¡¥å……å‚è€ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # ç»Ÿä¸€æ•°æ®æ ¼å¼
    for col in ['æœ€æ–°', 'æœ€é«˜', 'æœ€ä½', 'å¼€ç›˜', 'æ˜¨æ”¶', 'æ¶¨å¹…%', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']:
        df[col] = df[col].apply(safe_float)

    # æ·»åŠ åºå·
    df['åº'] = range(1, len(df) + 1)
    df['Unnamed: 16'] = '' # ä¿æŒä¸åŸæ–‡ä»¶æ ¼å¼ä¸€è‡´

    # é€‰æ‹©è¾“å‡ºåˆ—
    output_columns = [
        'åº', 'ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½',
        'å®é™…æ¢æ‰‹%', 'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
        'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'Unnamed: 16'
    ]

    # ç¡®ä¿æ‰€æœ‰è¾“å‡ºåˆ—éƒ½å­˜åœ¨ï¼Œå¹¶å¡«å……é»˜è®¤å€¼
    for col in output_columns:
        if col not in df.columns:
            df[col] = np.nan if col not in ['ä»£ç ', 'åç§°', 'Unnamed: 16'] else ' --'

    # æ ¼å¼åŒ–è¾“å‡ºåˆ°CSVçš„æ•°å€¼åˆ—
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']:
        df[col] = df[col].apply(lambda x: f" {x:.2f}" if pd.notna(x) else " --")

    # æ ¼å¼åŒ–ä»£ç å’Œåç§°
    df['ä»£ç '] = df['ä»£ç '].apply(lambda x: f'= "{str(x)}"' if not str(x).startswith('=') else x)
    df['åç§°'] = df['åç§°'].apply(lambda x: f" {x}" if not str(x).startswith(' ') else x)

    final_df_for_output = df[output_columns].copy()

    # ä¿å­˜Aè‚¡æ•°æ®
    output_file1 = 'è¾“å‡ºæ•°æ®/Aè‚¡æ•°æ®.csv'
    final_df_for_output.to_csv(output_file1, index=False, encoding='utf-8-sig')
    print(f"\nâœ… Aè‚¡æ•°æ®å·²ä¿å­˜: {output_file1}")
    print(f"   å…± {len(final_df_for_output)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒ XGBoost æ¨¡å‹ ==========
    print("\n2. è®­ç»ƒ XGBoost æ¨¡å‹...")
    model, scaler, technical_indicators = train_xgboost_model(df.copy()) # ä¼ å…¥åŸå§‹æ•°å€¼çš„dfå‰¯æœ¬

    if model is None:
        print("   âŒ XGBoost æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ç­›é€‰ã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨ ==========
    print("\n3. åŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨ (åŸºäº XGBoost è¯„åˆ†)...")

    quality_stocks = []

    # é‡æ–°åŠ è½½åŸå§‹æ•°å€¼çš„dfï¼Œå› ä¸ºä¸Šé¢ä¸ºäº†è¾“å‡ºcsvå·²ç»æ ¼å¼åŒ–äº†
    df_for_scoring = df.copy()
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']:
        df_for_scoring[col] = df_for_scoring[col].apply(safe_float)
        
    df_for_scoring['åŸå§‹ä»£ç '] = df_for_scoring['ä»£ç '].apply(lambda x: str(x).replace('= "', '').replace('"', ''))

    for idx, row in df_for_scoring.iterrows():
        score = predict_score_with_xgboost(row, model, scaler, technical_indicators, df_for_scoring)

        if pd.notna(score):  # ç¡®ä¿åˆ†æ•°æœ‰æ•ˆ
            code = str(row['åŸå§‹ä»£ç ']).strip()
            quality_stocks.append({
                'ä»£ç ': code,
                'åç§°': str(row['åç§°']).strip(),
                'è¡Œä¸š': str(row['æ‰€å±è¡Œä¸š']).strip(),
                'ä¼˜è´¨ç‡': score,
                'æ¶¨å¹…': f"{safe_float(row['æ¶¨å¹…%']):.2f}%" if pd.notna(safe_float(row['æ¶¨å¹…%'])) else "--"
            })

    # æŒ‰ä¼˜è´¨ç‡é™åºæ’åº
    quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)

    # ç¡®å®šç­›é€‰é˜ˆå€¼ï¼šå–å‰Nä¸ªï¼Œæˆ–è€…æ ¹æ®åˆ†æ•°åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
    display_count = 15  # é»˜è®¤æ˜¾ç¤ºå‰15ä¸ª
    if len(quality_stocks) > display_count:
        # å¦‚æœè‚¡ç¥¨æ•°é‡è¶³å¤Ÿï¼Œå–å‰Nä¸ªçš„æœ€ä½åˆ†æ•°ä½œä¸ºé˜ˆå€¼
        threshold = quality_stocks[display_count - 1]['ä¼˜è´¨ç‡']
        quality_stocks_filtered = quality_stocks[:display_count]
    elif len(quality_stocks) > 0:
        threshold = quality_stocks[-1]['ä¼˜è´¨ç‡']  # æ‰€æœ‰è‚¡ç¥¨çš„æœ€ä½åˆ†
        quality_stocks_filtered = quality_stocks
    else:
        threshold = 0.0
        quality_stocks_filtered = []

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨
    output_file2 = 'è¾“å‡ºæ•°æ®/ä¼˜è´¨è‚¡ç¥¨.txt'
    with open(output_file2, 'w', encoding='utf-8') as f:
        f.write("è‹æ°é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœ (XGBoost è¯„åˆ†)\n")
        f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœ€ä½ä¼˜è´¨ç‡é˜ˆå€¼ (åŸºäºå‰{display_count}åæˆ–å…¨éƒ¨): {threshold:.4f}\n")
        f.write(f"ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {len(quality_stocks_filtered)}\n")
        f.write("=" * 50 + "\n\n")

        for stock in quality_stocks_filtered:
            f.write(f"è‚¡ç¥¨ä»£ç : {stock['ä»£ç ']}\n")
            f.write(f"è‚¡ç¥¨åç§°: {stock['åç§°']}\n")
            f.write(f"æ‰€å±è¡Œä¸š: {stock['è¡Œä¸š']}\n")
            f.write(f"ä¼˜è´¨ç‡ (XGBoost è¯„åˆ†): {stock['ä¼˜è´¨ç‡']:.4f}\n")
            f.write(f"ä»Šæ—¥æ¶¨å¹…: {stock['æ¶¨å¹…']}\n")
            f.write("-" * 30 + "\n")

    print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨å·²ä¿å­˜: {output_file2}")
    print(f"   æ‰¾åˆ° {len(quality_stocks_filtered)} åªä¼˜è´¨è‚¡ç¥¨ï¼ˆæœ€ä½ä¼˜è´¨ç‡={threshold:.4f}ï¼‰")

    # ç”Ÿæˆäº¤æ˜“ç­–ç•¥
    print("\n   ç”Ÿæˆäº¤æ˜“ç­–ç•¥å»ºè®®...")
    trading_strategies = []
    for stock in quality_stocks_filtered:
        strategy = generate_trading_strategy(stock, df_for_scoring, model, scaler, technical_indicators)
        trading_strategies.append(strategy)

    if len(quality_stocks_filtered) > 0:
        print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨åŠäº¤æ˜“ç­–ç•¥å»ºè®® (å‰{len(quality_stocks_filtered)}å)ï¼š")
        print("=" * 100)
        print(f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'æ¶¨å¹…':<8} {'ä¼˜è´¨ç‡':<10} {'æ‰€å±è¡Œä¸š':<15} {'å»ºè®®':<10} {'å‘¨æœŸ':<8} {'ä»“ä½':<8} {'ç†ç”±':<30}")
        print("-" * 100)
        for strategy in trading_strategies:
            print(f"{strategy['ä»£ç ']:<10} {strategy['åç§°']:<12} {df_for_scoring[df_for_scoring['åŸå§‹ä»£ç '] == strategy['ä»£ç ']]['æ¶¨å¹…%'].values[0]:<8} {df_for_scoring[df_for_scoring['åŸå§‹ä»£ç '] == strategy['ä»£ç ']]['quality_score'].values[0]:.4f}   {df_for_scoring[df_for_scoring['åŸå§‹ä»£ç '] == strategy['ä»£ç ']]['æ‰€å±è¡Œä¸š'].values[0]:<15} {strategy['å»ºè®®']:<10} {strategy['å‘¨æœŸ']:<8} {strategy['ä»“ä½']:<8} {', '.join(strategy['ç†ç”±']):<30}")
    else:
        print("\nâš ï¸ ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å¸‚åœºæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œæ¶¨å¹…ä¸è¶³")
        print("   2. æ•°æ®è·å–ä¸å®Œæ•´æˆ–è´¨é‡ä¸ä½³")
        print("   3. XGBoost æ¨¡å‹éœ€è¦æ›´å¤šæ•°æ®æˆ–ä¼˜åŒ–")

    # ========== ç¬¬å››æ­¥ï¼šå…³è”è§„åˆ™æŒ–æ˜ ==========
    # åœ¨è¿™é‡Œè°ƒç”¨å…³è”è§„åˆ™æŒ–æ˜å‡½æ•°
    perform_association_rule_mining(df_for_scoring.copy())  # ä¼ å…¥åŸå§‹æ•°å€¼çš„dfå‰¯æœ¬

    print("\n" + "=" * 60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)

if __name__ == "__main__":
    main()
