#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - æ ¹æ®æ¯å¤©å®æ—¶æ•°æ®ç­›é€‰
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ†
æ–°å¢ï¼š
1. ä½¿ç”¨ Optuna è¿›è¡Œç¥ç»ç½‘ç»œè¶…å‚æ•°ä¼˜åŒ–ï¼Œæå‡è¯„åˆ†ç²¾åº¦å’ŒåŒºåˆ†åº¦ã€‚
2. å¼•å…¥å…³è”è§„åˆ™æŒ–æ˜ï¼Œåˆ†æå“ªäº›æ¡ä»¶ç»„åˆæ›´å®¹æ˜“äº§ç”Ÿé«˜æ”¶ç›Šï¼Œæä¾›ç­–ç•¥æ´å¯Ÿã€‚
3. ä¼˜åŒ–æ•°æ®å¤„ç†å’Œè¾“å‡ºå±•ç¤ºã€‚
4. ä¼˜åŒ–ä»£ç å‡†ç¡®æ€§ã€è´¨é‡å’Œæ•ˆç‡ã€‚
5. ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç¥ç»ç½‘ç»œçš„ç›®æ ‡å˜é‡ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚
6. **é‡å¤§å‡çº§ï¼šå¼•å…¥æ›´å¤šæŠ€æœ¯æŒ‡æ ‡ï¼Œå¹¶å®šä¹‰çŸ­æœŸ/é•¿æœŸä¹°å–ç­–ç•¥ã€‚**
7. **ä¿®å¤ï¼šakshareå†å²æ•°æ®åˆ—æ•°ä¸åŒ¹é…é—®é¢˜ã€‚**
8. **å…³é”®ä¿®æ”¹ï¼šç¥ç»ç½‘ç»œé¢„æµ‹ä¸å†ç›´æ¥ä½¿ç”¨å†å²æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºè¾“å…¥ï¼Œä½†ç­–ç•¥æŠ¥å‘Šä¼šç»“åˆã€‚**
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ç¥ç»ç½‘ç»œç›¸å…³åº“
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler # æ–°å¢MinMaxScalerç”¨äºç›®æ ‡å˜é‡å½’ä¸€åŒ–
from sklearn.metrics import mean_squared_error, r2_score

# å¯¼å…¥ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

# å…¨å±€å˜é‡ï¼Œç”¨äºå­˜å‚¨å†å²æ•°æ®ï¼Œé¿å…é‡å¤è·å–
GLOBAL_HIST_DATA = {}

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 10000
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 100000000
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) / 10000
        return float(s)
    except ValueError:
        return np.nan

def get_stock_history_data(symbol, start_date, end_date):
    """
    è·å–å•åªè‚¡ç¥¨çš„å†å²è¡Œæƒ…æ•°æ®ï¼Œå¹¶ç¼“å­˜ã€‚
    symbol: è‚¡ç¥¨ä»£ç ï¼Œå¦‚ '000001'
    start_date, end_date: æ—¥æœŸå­—ç¬¦ä¸² 'YYYYMMDD'
    """
    if symbol in GLOBAL_HIST_DATA:
        # æ£€æŸ¥ç¼“å­˜æ•°æ®æ˜¯å¦è¦†ç›–æ‰€éœ€æ—¥æœŸèŒƒå›´
        cached_df = GLOBAL_HIST_DATA[symbol]
        if not cached_df.empty and \
           pd.to_datetime(cached_df['æ—¥æœŸ'].min()) <= pd.to_datetime(start_date) and \
           pd.to_datetime(cached_df['æ—¥æœŸ'].max()) >= pd.to_datetime(end_date):
            return cached_df[(cached_df['æ—¥æœŸ'] >= start_date) & (cached_df['æ—¥æœŸ'] <= end_date)].copy()

    try:
        # å°è¯•ä» akshare è·å–æ•°æ®
        df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        if df.empty:
            # print(f"   âš ï¸ æœªè·å–åˆ° {symbol} çš„å†å²æ•°æ®ã€‚") # å‡å°‘æ‰“å°ï¼Œé¿å…åˆ·å±
            return pd.DataFrame()

        # å®šä¹‰æˆ‘ä»¬æœŸæœ›çš„åˆ—å
        expected_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ 'Unnamed: 0' åˆ—ï¼Œè¿™æ˜¯ akshare å¸¸è§çš„å¤šä½™ç´¢å¼•åˆ—
        if 'Unnamed: 0' in df.columns:
            df = df.drop(columns=['Unnamed: 0'])
        
        # å¦‚æœç»è¿‡å¤„ç†åï¼Œåˆ—æ•°ä»ç„¶ä¸åŒ¹é…ï¼Œåˆ™æ‰“å°é”™è¯¯å¹¶è¿”å›ç©ºDataFrame
        if len(df.columns) != len(expected_cols):
            # print(f"   âŒ {symbol} å†å²æ•°æ®åˆ—æ•°ä¸åŒ¹é…ã€‚é¢„æœŸ {len(expected_cols)} åˆ—ï¼Œå®é™… {len(df.columns)} åˆ—ã€‚") # å‡å°‘æ‰“å°
            # print(f"   å®é™…åˆ—å: {df.columns.tolist()}")
            return pd.DataFrame()

        # é‡æ–°èµ‹å€¼åˆ—å
        df.columns = expected_cols

        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.strftime('%Y%m%d')
        df = df.sort_values(by='æ—¥æœŸ').reset_index(drop=True)
        GLOBAL_HIST_DATA[symbol] = df.copy() # ç¼“å­˜æ•°æ®
        return df
    except Exception as e:
        # print(f"   âŒ è·å– {symbol} å†å²æ•°æ®å¤±è´¥: {e}") # å‡å°‘æ‰“å°
        return pd.DataFrame()

def calculate_technical_indicators(df_hist):
    """
    è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼Œå¹¶è¿”å›æœ€æ–°ä¸€å¤©çš„æŒ‡æ ‡å€¼ã€‚
    df_hist: åŒ…å«å†å²è¡Œæƒ…æ•°æ®çš„DataFrameï¼Œè‡³å°‘åŒ…å« 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡'
    """
    if df_hist.empty or len(df_hist) < 200: # è‡³å°‘éœ€è¦200å¤©æ•°æ®æ¥è®¡ç®—é•¿æœŸå‡çº¿å’Œå¸ƒæ—å¸¦
        return {
            'MA5': np.nan, 'MA10': np.nan, 'MA20': np.nan, 'MA60': np.nan, 'MA120': np.nan, 'MA200': np.nan,
            'RSI': np.nan, 'MACD_DIF': np.nan, 'MACD_DEA': np.nan, 'MACD_HIST': np.nan,
            'BOLL_UP': np.nan, 'BOLL_MID': np.nan, 'BOLL_LOW': np.nan,
            'VOL_MA5': np.nan, 'VOL_MA10': np.nan, 'VOL_CHANGE': np.nan
        }

    # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
    df_hist['æ”¶ç›˜'] = pd.to_numeric(df_hist['æ”¶ç›˜'], errors='coerce')
    df_hist['æœ€é«˜'] = pd.to_numeric(df_hist['æœ€é«˜'], errors='coerce')
    df_hist['æœ€ä½'] = pd.to_numeric(df_hist['æœ€ä½'], errors='coerce')
    df_hist['æˆäº¤é‡'] = pd.to_numeric(df_hist['æˆäº¤é‡'], errors='coerce')
    df_hist['æ¶¨è·Œå¹…'] = pd.to_numeric(df_hist['æ¶¨è·Œå¹…'], errors='coerce')

    # ç§»åŠ¨å¹³å‡çº¿ (MA)
    df_hist['MA5'] = df_hist['æ”¶ç›˜'].rolling(window=5).mean()
    df_hist['MA10'] = df_hist['æ”¶ç›˜'].rolling(window=10).mean()
    df_hist['MA20'] = df_hist['æ”¶ç›˜'].rolling(window=20).mean()
    df_hist['MA60'] = df_hist['æ”¶ç›˜'].rolling(window=60).mean()
    df_hist['MA120'] = df_hist['æ”¶ç›˜'].rolling(window=120).mean()
    df_hist['MA200'] = df_hist['æ”¶ç›˜'].rolling(window=200).mean()

    # ç›¸å¯¹å¼ºå¼±æŒ‡æ•° (RSI)
    delta = df_hist['æ”¶ç›˜'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df_hist['RSI'] = 100 - (100 / (1 + rs))

    # å¼‚åŒç§»åŠ¨å¹³å‡çº¿ (MACD)
    exp1 = df_hist['æ”¶ç›˜'].ewm(span=12, adjust=False).mean()
    exp2 = df_hist['æ”¶ç›˜'].ewm(span=26, adjust=False).mean()
    df_hist['MACD_DIF'] = exp1 - exp2
    df_hist['MACD_DEA'] = df_hist['MACD_DIF'].ewm(span=9, adjust=False).mean()
    df_hist['MACD_HIST'] = (df_hist['MACD_DIF'] - df_hist['MACD_DEA']) * 2 # MACDæŸ±

    # å¸ƒæ—å¸¦ (Bollinger Bands)
    df_hist['BOLL_MID'] = df_hist['æ”¶ç›˜'].rolling(window=20).mean()
    df_hist['BOLL_STD'] = df_hist['æ”¶ç›˜'].rolling(window=20).std()
    df_hist['BOLL_UP'] = df_hist['BOLL_MID'] + (df_hist['BOLL_STD'] * 2)
    df_hist['BOLL_LOW'] = df_hist['BOLL_MID'] - (df_hist['BOLL_STD'] * 2)

    # æˆäº¤é‡å‡çº¿
    df_hist['VOL_MA5'] = df_hist['æˆäº¤é‡'].rolling(window=5).mean()
    df_hist['VOL_MA10'] = df_hist['æˆäº¤é‡'].rolling(window=10).mean()
    df_hist['VOL_CHANGE'] = df_hist['æˆäº¤é‡'].pct_change() # æˆäº¤é‡å˜åŒ–ç‡

    # è·å–æœ€æ–°ä¸€å¤©çš„æŒ‡æ ‡å€¼
    latest_data = df_hist.iloc[-1]
    
    return {
        'MA5': latest_data.get('MA5'), 'MA10': latest_data.get('MA10'), 'MA20': latest_data.get('MA20'),
        'MA60': latest_data.get('MA60'), 'MA120': latest_data.get('MA120'), 'MA200': latest_data.get('MA200'),
        'RSI': latest_data.get('RSI'),
        'MACD_DIF': latest_data.get('MACD_DIF'), 'MACD_DEA': latest_data.get('MACD_DEA'), 'MACD_HIST': latest_data.get('MACD_HIST'),
        'BOLL_UP': latest_data.get('BOLL_UP'), 'BOLL_MID': latest_data.get('BOLL_MID'), 'BOLL_LOW': latest_data.get('BOLL_LOW'),
        'VOL_MA5': latest_data.get('VOL_MA5'), 'VOL_MA10': latest_data.get('VOL_MA10'), 'VOL_CHANGE': latest_data.get('VOL_CHANGE')
    }

def calculate_nn_features(row):
    """
    è®¡ç®—ç¥ç»ç½‘ç»œçš„è¾“å…¥ç‰¹å¾å€¼ã€‚
    è¿™äº›ç‰¹å¾ä¸ä¾èµ–äºå†å²æ•°æ®ï¼Œåªä½¿ç”¨å®æ—¶æ•°æ®å’Œè‹æ°é‡åŒ–ç­–ç•¥ã€‚
    """
    features = []

    # 1. è‹æ°é‡åŒ–ç­–ç•¥ç‰¹å¾ (F, G)
    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶ (0æˆ–1)
    try:
        low = safe_float(row.get('æœ€ä½'))
        ma60 = safe_float(row.get('60æ—¥å‡ä»·'))
        ma20 = safe_float(row.get('20æ—¥å‡ä»·'))
        current = safe_float(row.get('æœ€æ–°'))

        f_condition = 0
        if pd.notna(low) and pd.notna(ma60) and ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
            f_condition = 1
        elif pd.notna(current) and pd.notna(ma20) and ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
            f_condition = 1
        features.append(f_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½® (0æˆ–1)
    try:
        change = safe_float(row.get('æ¶¨å¹…%'))
        current = safe_float(row.get('æœ€æ–°'))
        high = safe_float(row.get('æœ€é«˜'))
        low = safe_float(row.get('æœ€ä½'))

        g_condition = 0
        if pd.notna(change) and change >= 5.0 and pd.notna(current) and pd.notna(high) and pd.notna(low):
            if (high - low) > 0: # é¿å…é™¤ä»¥é›¶
                threshold = high - (high - low) * 0.30
                if current >= threshold:
                    g_condition = 1
            elif current == high: # å¦‚æœæœ€é«˜æœ€ä½ç›¸åŒï¼Œä¸”æ¶¨å¹…>=5ï¼Œä¹Ÿç®—æ»¡è¶³
                g_condition = 1
        features.append(g_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # 2. åŸºæœ¬é¢ç‰¹å¾ (H, I, J, K)
    # Håˆ—ï¼šå½’å±å‡€åˆ©æ¶¦ (æ•°å€¼ï¼Œå•ä½äº¿)
    try:
        profit = safe_float(row.get('å½’å±å‡€åˆ©æ¶¦'))
        features.append(profit if pd.notna(profit) else 0)
    except:
        features.append(0)

    # Iåˆ—ï¼šå®é™…æ¢æ‰‹ç‡ (æ•°å€¼)
    try:
        turnover = safe_float(row.get('å®é™…æ¢æ‰‹%'))
        features.append(turnover if pd.notna(turnover) else 0) # ç¼ºå¤±æ—¶ç»™0
    except:
        features.append(0)

    # Jåˆ—ï¼šæ€»å¸‚å€¼ (æ•°å€¼ï¼Œå•ä½äº¿)
    try:
        cap = safe_float(row.get('æ€»å¸‚å€¼'))
        features.append(cap if pd.notna(cap) else 0)
    except:
        features.append(0)

    # Kåˆ—ï¼šå¸‚ç›ˆç‡(åŠ¨) (æ•°å€¼)
    try:
        pe = safe_float(row.get('å¸‚ç›ˆç‡(åŠ¨)'))
        features.append(pe if pd.notna(pe) else 0) # ç¼ºå¤±æ—¶ç»™0
    except:
        features.append(0)

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½æ˜¯æ•°å€¼ç±»å‹ï¼Œå¹¶å¤„ç†NaN
    final_features = [f if pd.notna(f) else 0 for f in features] # å°†æ‰€æœ‰NaNå¡«å……ä¸º0

    return final_features

def objective(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    """
    hidden_layer_sizes = []
    n_layers = trial.suggest_int('n_layers', 1, 3)
    for i in range(n_layers):
        hidden_layer_sizes.append(trial.suggest_int(f'n_units_l{i}', 32, 256)) # å¢åŠ ç¥ç»å…ƒæ•°é‡èŒƒå›´

    activation = trial.suggest_categorical('activation', ['relu', 'tanh']) # ç§»é™¤logisticï¼Œreluå’Œtanhé€šå¸¸è¡¨ç°æ›´å¥½
    solver = trial.suggest_categorical('solver', ['adam']) # ç®€åŒ–ä¸ºadamï¼Œé€šå¸¸æ•ˆæœæœ€å¥½
    alpha = trial.suggest_loguniform('alpha', 1e-6, 1e-2) # è°ƒæ•´æ­£åˆ™åŒ–å¼ºåº¦èŒƒå›´
    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-5, 1e-2)

    model = MLPRegressor(
        hidden_layer_sizes=tuple(hidden_layer_sizes),
        activation=activation,
        solver=solver,
        alpha=alpha,
        learning_rate_init=learning_rate_init,
        random_state=42,
        max_iter=1000, # å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
        early_stopping=True,
        n_iter_no_change=30, # å¢åŠ è€å¿ƒ
        tol=1e-5 # å¢åŠ å®¹å¿åº¦
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse # Optuna é»˜è®¤æœ€å°åŒ–ç›®æ ‡

def train_neural_network(df):
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†ï¼Œä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
    ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç›®æ ‡å˜é‡ã€‚
    """
    print("\n   å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X = []
    
    # æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ hist_data_map æ¥è®¡ç®— X çš„ç‰¹å¾ï¼Œå› ä¸º X åªåŒ…å«å®æ—¶æ•°æ®ç‰¹å¾ã€‚
    # ä½†å¦‚æœä½ çš„ç›®æ ‡å˜é‡ y çš„è®¡ç®—ä¾èµ–äºå†å²æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œæœªæ¥æ¶¨å¹…ï¼‰ï¼Œé‚£ä¹ˆä½ ä»ç„¶éœ€è¦å†å²æ•°æ®æ¥æ„å»º yã€‚
    # å½“å‰çš„ quality_score ç›®æ ‡å˜é‡åªä¾èµ–äºå®æ—¶åŸºæœ¬é¢æ•°æ®ï¼Œæ‰€ä»¥è¿™é‡Œä¸éœ€è¦é¢å¤–çš„å†å²æ•°æ®è·å–ã€‚

    for _, row in df.iterrows():
        features = calculate_nn_features(row) # ä½¿ç”¨æ–°çš„å‡½æ•°ï¼Œåªè®¡ç®—å®æ—¶ç‰¹å¾
        X.append(features)

    X = np.array(X)

    # æå–ç”¨äºè®¡ç®—å¤åˆè´¨é‡è¯„åˆ†çš„åˆ— (ä½¿ç”¨åŸå§‹æ•°å€¼ï¼Œæœªæ ¼å¼åŒ–çš„df)
    df_raw_values = df.copy()
    for col in ['æ¶¨å¹…%', 'å½’å±å‡€åˆ©æ¶¦', 'å®é™…æ¢æ‰‹%', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡(åŠ¨)']:
        df_raw_values[col] = df_raw_values[col].apply(safe_float)

    change = df_raw_values['æ¶¨å¹…%']
    profit = df_raw_values['å½’å±å‡€åˆ©æ¶¦']
    turnover = df_raw_values['å®é™…æ¢æ‰‹%']
    market_cap = df_raw_values['æ€»å¸‚å€¼']
    pe_ratio = df_raw_values['å¸‚ç›ˆç‡(åŠ¨)']

    # å½’ä¸€åŒ–å„ä¸ªæŒ‡æ ‡ (ä½¿ç”¨ MinMaxScalerï¼Œå¤„ç†NaNå€¼)
    # æ¶¨å¹…ï¼šè¶Šé«˜è¶Šå¥½
    change_norm = MinMaxScaler().fit_transform(change.fillna(change.median()).values.reshape(-1, 1)).flatten()
    # å‡€åˆ©æ¶¦ï¼šè¶Šé«˜è¶Šå¥½
    profit_norm = MinMaxScaler().fit_transform(profit.fillna(profit.median()).values.reshape(-1, 1)).flatten()
    # æ¢æ‰‹ç‡ï¼šé€‚ä¸­æœ€å¥½ï¼Œè¿‡é«˜æˆ–è¿‡ä½éƒ½ä¸å¥½ã€‚è¿™é‡Œç®€å•å¤„ç†ä¸ºè¶Šä½è¶Šå¥½ï¼Œæˆ–è€…å¯ä»¥è®¾è®¡ä¸€ä¸ªäºŒæ¬¡å‡½æ•°
    # æš‚æ—¶ä¿æŒè¶Šä½è¶Šå¥½ï¼Œä½†å¯ä»¥æ ¹æ®ç­–ç•¥è°ƒæ•´
    turnover_norm = MinMaxScaler().fit_transform(turnover.fillna(turnover.median()).values.reshape(-1, 1)).flatten()
    # å¸‚å€¼ï¼šè¶Šå¤§è¶Šå¥½ (å€¾å‘äºå¤§ä¸­ç›˜è‚¡)
    market_cap_norm = MinMaxScaler().fit_transform(market_cap.fillna(market_cap.median()).values.reshape(-1, 1)).flatten()
    # å¸‚ç›ˆç‡ï¼šè¶Šä½è¶Šå¥½ (ä½†è¦é¿å…è´Ÿå€¼æˆ–è¿‡é«˜å¼‚å¸¸å€¼)
    # å¯¹PEè¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œé¿å…è´Ÿå€¼å’Œæç«¯é«˜å€¼å½±å“å½’ä¸€åŒ–
    pe_ratio_filtered = pe_ratio.copy()
    pe_ratio_filtered[pe_ratio_filtered <= 0] = np.nan # è´ŸPEé€šå¸¸è¡¨ç¤ºäºæŸï¼Œä¸å‚ä¸æ­£å¸¸ä¼°å€¼
    pe_ratio_filtered[pe_ratio_filtered > 500] = 500 # é™åˆ¶æç«¯é«˜å€¼
    pe_ratio_norm = MinMaxScaler().fit_transform(pe_ratio_filtered.fillna(pe_ratio_filtered.median()).values.reshape(-1, 1)).flatten()
    pe_ratio_norm = 1 - pe_ratio_norm # è¶Šä½è¶Šå¥½ï¼Œæ‰€ä»¥1-å½’ä¸€åŒ–å€¼

    # è®¡ç®—å¤åˆè´¨é‡è¯„åˆ† (å¯ä»¥è°ƒæ•´æƒé‡ï¼Œè¿™é‡Œæ›´ä¾§é‡åŸºæœ¬é¢å’Œåˆç†ä¼°å€¼)
    # æƒé‡åˆ†é…ï¼š
    # æ¶¨å¹… (çŸ­æœŸåŠ¨é‡): 0.15
    # å‡€åˆ©æ¶¦ (ç›ˆåˆ©èƒ½åŠ›): 0.30
    # æ¢æ‰‹ç‡ (æµåŠ¨æ€§/æ´»è·ƒåº¦ï¼Œè¿™é‡Œå‡è®¾é€‚ä¸­åä½ä¸ºå¥½): 0.10
    # å¸‚å€¼ (è§„æ¨¡/ç¨³å®šæ€§): 0.25
    # å¸‚ç›ˆç‡ (ä¼°å€¼åˆç†æ€§): 0.20
    df_raw_values['quality_score'] = (
        0.15 * change_norm +
        0.30 * profit_norm +
        0.10 * (1 - turnover_norm) + # æ¢æ‰‹ç‡è¶Šä½ï¼Œè¿™ä¸ªå€¼è¶Šé«˜
        0.25 * market_cap_norm +
        0.20 * pe_ratio_norm
    )

    y = df_raw_values['quality_score'].values

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]

    if len(X) < 50: # è‡³å°‘éœ€è¦æ›´å¤šæ•°æ®æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œå¹¶è¿›è¡ŒOptunaä¼˜åŒ–
        print("   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒç¥ç»ç½‘ç»œã€‚è‡³å°‘éœ€è¦50ä¸ªæ ·æœ¬ã€‚")
        return None, None

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Optuna è¶…å‚æ•°ä¼˜åŒ–
    print("   å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
    try:
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=70, show_progress_bar=True) # å¢åŠ è¯•éªŒæ¬¡æ•°
    except Exception as e:
        print(f"   Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("   å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
        # å¦‚æœOptunaå¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªåˆç†çš„é»˜è®¤é…ç½®
        best_params = {
            'n_layers': 2,
            'n_units_l0': 128,
            'n_units_l1': 64,
            'activation': 'relu',
            'solver': 'adam',
            'alpha': 0.0001,
            'learning_rate_init': 0.001
        }
    else:
        print("\n   Optuna ä¼˜åŒ–å®Œæˆã€‚")
        print(f"   æœ€ä½³å‡æ–¹è¯¯å·® (MSE): {study.best_value:.4f}")
        print(f"   æœ€ä½³è¶…å‚æ•°: {study.best_params}")
        best_params = study.best_params

    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆç¥ç»ç½‘ç»œæ¨¡å‹...")
    hidden_layer_sizes = []
    for i in range(best_params['n_layers']):
        hidden_layer_sizes.append(best_params[f'n_units_l{i}'])

    model = MLPRegressor(
        hidden_layer_sizes=tuple(hidden_layer_sizes),
        activation=best_params['activation'],
        solver=best_params['solver'],
        alpha=best_params['alpha'],
        learning_rate_init=best_params['learning_rate_init'],
        random_state=42,
        max_iter=1500, # è¿›ä¸€æ­¥å¢åŠ æœ€å¤§è¿­ä»£æ¬¡æ•°
        early_stopping=True,
        n_iter_no_change=40, # è¿›ä¸€æ­¥å¢åŠ è€å¿ƒ
        tol=1e-5 # å¢åŠ å®¹å¿åº¦
    )
    model.fit(X_train, y_train)

    # è¯„ä¼°æ¨¡å‹
    print("   è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"   æœ€ç»ˆæ¨¡å‹å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   æœ€ç»ˆæ¨¡å‹RÂ²åˆ†æ•°: {r2:.4f}")

    return model, scaler

def predict_score_with_nn(row, model, scaler):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹è‚¡ç¥¨è¯„åˆ†ã€‚
    æ³¨æ„ï¼šè¿™é‡Œä¸å†éœ€è¦ hist_data_mapï¼Œå› ä¸º NN çš„è¾“å…¥ç‰¹å¾ä¸åŒ…å«æŠ€æœ¯æŒ‡æ ‡ã€‚
    """
    features = calculate_nn_features(row) # ä½¿ç”¨åªåŒ…å«å®æ—¶ç‰¹å¾çš„å‡½æ•°
    # æ£€æŸ¥ç‰¹å¾ä¸­æ˜¯å¦æœ‰NaNæˆ–Infï¼Œå¦‚æœæœ‰ï¼Œåˆ™è¿”å›ä¸€ä¸ªé»˜è®¤å€¼æˆ–NaN
    if any(pd.isna(f) or np.isinf(f) for f in features):
        return np.nan # æˆ–è€…ä¸€ä¸ªéå¸¸ä½çš„é»˜è®¤åˆ†æ•°

    features = np.array(features).reshape(1, -1)  # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„
    try:
        features_scaled = scaler.transform(features)
        score = model.predict(features_scaled)[0]
        return score
    except Exception as e:
        # print(f"é¢„æµ‹åˆ†æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}, ç‰¹å¾: {features}")
        return np.nan # é¢„æµ‹å¤±è´¥æ—¶è¿”å›NaN

def generate_strategy_signals(row, nn_score, tech_indicators):
    """
    æ ¹æ®ç¥ç»ç½‘ç»œè¯„åˆ†å’ŒæŠ€æœ¯æŒ‡æ ‡ç”ŸæˆçŸ­æœŸ/é•¿æœŸä¹°å–ä¿¡å·ã€‚
    nn_score: ç¥ç»ç½‘ç»œé¢„æµ‹çš„è´¨é‡è¯„åˆ†
    tech_indicators: è¯¥è‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡å­—å…¸ (è¿™é‡Œä¼šç”¨åˆ°ï¼Œå³ä½¿NNé¢„æµ‹æ—¶æ²¡ç”¨)
    """
    signals = []
    reasons = [] # å­˜å‚¨ç”Ÿæˆä¿¡å·çš„åŸå› 

    current_price = safe_float(row.get('æœ€æ–°'))
    change_percent = safe_float(row.get('æ¶¨å¹…%'))
    turnover_rate = safe_float(row.get('å®é™…æ¢æ‰‹%'))
    
    # è·å–æŠ€æœ¯æŒ‡æ ‡
    ma5 = tech_indicators.get('MA5', np.nan)
    ma10 = tech_indicators.get('MA10', np.nan)
    ma20 = tech_indicators.get('MA20', np.nan)
    ma60 = tech_indicators.get('MA60', np.nan)
    ma120 = tech_indicators.get('MA120', np.nan)
    ma200 = tech_indicators.get('MA200', np.nan)
    rsi = tech_indicators.get('RSI', np.nan)
    macd_dif = tech_indicators.get('MACD_DIF', np.nan)
    macd_dea = tech_indicators.get('MACD_DEA', np.nan)
    macd_hist = tech_indicators.get('MACD_HIST', np.nan)
    boll_up = tech_indicators.get('BOLL_UP', np.nan)
    boll_mid = tech_indicators.get('BOLL_MID', np.nan)
    boll_low = tech_indicators.get('BOLL_LOW', np.nan)
    vol_ma5 = tech_indicators.get('VOL_MA5', np.nan)
    vol_ma10 = tech_indicators.get('VOL_MA10', np.nan)
    vol_change = tech_indicators.get('VOL_CHANGE', np.nan)

    # ç¡®ä¿æ‰€æœ‰å…³é”®æŒ‡æ ‡éNaN
    if pd.isna(nn_score) or pd.isna(current_price):
        return ["æ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­"], ["æ ¸å¿ƒæ•°æ®ç¼ºå¤±"]

    # --- ç¥ç»ç½‘ç»œè¯„åˆ†è€ƒé‡ ---
    if nn_score > 0.85:
        signals.append("å¼ºåŠ›ä¹°å…¥")
        reasons.append(f"NNè¯„åˆ†æé«˜ ({nn_score:.4f})ï¼Œæ˜¾ç¤ºæä½³çš„ç»¼åˆè´¨é‡ã€‚")
    elif nn_score > 0.7:
        signals.append("ä¹°å…¥")
        reasons.append(f"NNè¯„åˆ†è¾ƒé«˜ ({nn_score:.4f})ï¼Œæ˜¾ç¤ºè‰¯å¥½çš„ç»¼åˆè´¨é‡ã€‚")
    elif nn_score < 0.3:
        signals.append("å–å‡º")
        reasons.append(f"NNè¯„åˆ†è¾ƒä½ ({nn_score:.4f})ï¼Œæ˜¾ç¤ºæ½œåœ¨é£é™©æˆ–è´¨é‡ä¸ä½³ã€‚")
    else:
        signals.append("è§‚æœ›")
        reasons.append(f"NNè¯„åˆ†ä¸­ç­‰ ({nn_score:.4f})ï¼Œéœ€ç»“åˆå…¶ä»–å› ç´ åˆ¤æ–­ã€‚")

    # --- çŸ­æœŸç­–ç•¥ä¿¡å· (åå‘åŠ¨é‡å’Œè¶…è·Œåå¼¹) ---
    short_term_buy_reasons = []
    short_term_sell_reasons = []

    if pd.notna(rsi) and pd.notna(macd_hist) and pd.notna(ma5) and pd.notna(ma10) and pd.notna(turnover_rate):
        # RSIè¶…å–åå¼¹
        if rsi < 30 and current_price > ma5:
            short_term_buy_reasons.append("RSIè¶…å–ååå¼¹ï¼ŒçŸ­æœŸåŠ¨èƒ½å¢å¼ºã€‚")
        # MACDé‡‘å‰
        if macd_dif > macd_dea and macd_hist > 0 and macd_dif < 0: # é‡‘å‰ä¸”åœ¨é›¶è½´ä¸‹æ–¹
            short_term_buy_reasons.append("MACDåœ¨é›¶è½´ä¸‹æ–¹å½¢æˆé‡‘å‰ï¼Œå¯èƒ½å¤„äºåº•éƒ¨åè½¬åŒºåŸŸã€‚")
        # ä»·æ ¼çªç ´çŸ­æœŸå‡çº¿
        if current_price > ma5 and ma5 > ma10 and change_percent > 2.0: # ä»·æ ¼ç«™ä¸Š5æ—¥çº¿ï¼Œ5æ—¥çº¿å‘ä¸Šï¼Œä¸”æœ‰ä¸€å®šæ¶¨å¹…
            short_term_buy_reasons.append("ä»·æ ¼å¼ºåŠ¿ç«™ä¸Š5æ—¥å‡çº¿ï¼Œä¸”5æ—¥å‡çº¿å‘ä¸Šï¼ŒçŸ­æœŸè¶‹åŠ¿å‘å¥½ã€‚")
        # æ”¾é‡ä¸Šæ¶¨
        if pd.notna(vol_change) and vol_change > 0.5 and change_percent > 3.0: # æˆäº¤é‡æ”¾å¤§50%ä¸”æ¶¨å¹…è¶…è¿‡3%
            short_term_buy_reasons.append("æˆäº¤é‡æ˜¾è‘—æ”¾å¤§ï¼Œé…åˆä»·æ ¼ä¸Šæ¶¨ï¼Œæ˜¾ç¤ºèµ„é‡‘ç§¯æä»‹å…¥ã€‚")
        # ä»·æ ¼æ¥è¿‘å¸ƒæ—å¸¦ä¸‹è½¨å¹¶åå¼¹
        if pd.notna(boll_low) and current_price > boll_low and (current_price - boll_low) / boll_low < 0.01 and change_percent > 0:
            short_term_buy_reasons.append("ä»·æ ¼è§¦åŠå¸ƒæ—å¸¦ä¸‹è½¨ååå¼¹ï¼Œè·å¾—æ”¯æ’‘ã€‚")

        # RSIè¶…ä¹°
        if rsi > 70:
            short_term_sell_reasons.append("RSIè¿›å…¥è¶…ä¹°åŒºåŸŸï¼ŒçŸ­æœŸå›è°ƒé£é™©å¢åŠ ã€‚")
        # MACDæ­»å‰
        if macd_dif < macd_dea and macd_hist < 0 and macd_dif > 0: # æ­»å‰ä¸”åœ¨é›¶è½´ä¸Šæ–¹
            short_term_sell_reasons.append("MACDåœ¨é›¶è½´ä¸Šæ–¹å½¢æˆæ­»å‰ï¼ŒçŸ­æœŸä¸Šæ¶¨åŠ¨èƒ½å‡å¼±ã€‚")
        # ä»·æ ¼è·Œç ´çŸ­æœŸå‡çº¿
        if current_price < ma5 and ma5 < ma10 and change_percent < -2.0:
            short_term_sell_reasons.append("ä»·æ ¼è·Œç ´5æ—¥å‡çº¿ï¼Œä¸”5æ—¥å‡çº¿å‘ä¸‹ï¼ŒçŸ­æœŸè¶‹åŠ¿è½¬å¼±ã€‚")
        # ä»·æ ¼è·Œç ´å¸ƒæ—å¸¦ä¸­è½¨æˆ–ä¸‹è½¨
        if pd.notna(boll_mid) and current_price < boll_mid and change_percent < -1.0:
            short_term_sell_reasons.append("ä»·æ ¼è·Œç ´å¸ƒæ—å¸¦ä¸­è½¨ï¼ŒçŸ­æœŸæ”¯æ’‘å¤±æ•ˆã€‚")
        if pd.notna(boll_low) and current_price < boll_low:
            short_term_sell_reasons.append("ä»·æ ¼è·Œç ´å¸ƒæ—å¸¦ä¸‹è½¨ï¼Œå¯èƒ½è¿›å…¥ä¸‹è·Œé€šé“ã€‚")

    if short_term_buy_reasons:
        signals.append("çŸ­æœŸä¹°å…¥")
        reasons.append("çŸ­æœŸæŠ€æœ¯é¢ç§¯æä¿¡å·ï¼š" + " ".join(short_term_buy_reasons))
    if short_term_sell_reasons:
        signals.append("çŸ­æœŸå–å‡º")
        reasons.append("çŸ­æœŸæŠ€æœ¯é¢æ¶ˆæä¿¡å·ï¼š" + " ".join(short_term_sell_reasons))

    # --- é•¿æœŸç­–ç•¥ä¿¡å· (åå‘è¶‹åŠ¿å’Œä»·å€¼) ---
    long_term_buy_reasons = []
    long_term_sell_reasons = []

    if pd.notna(ma60) and pd.notna(ma120) and pd.notna(ma200):
        # é•¿æœŸå‡çº¿å¤šå¤´æ’åˆ— (æˆ–æ¥è¿‘å¤šå¤´æ’åˆ—)
        if ma5 > ma10 > ma20 > ma60 and current_price > ma60:
            long_term_buy_reasons.append("å‡çº¿å‘ˆå¤šå¤´æ’åˆ—ï¼Œæ˜¾ç¤ºé•¿æœŸä¸Šæ¶¨è¶‹åŠ¿å¼ºåŠ²ã€‚")
        # ä»·æ ¼ç«™ä¸Šé•¿æœŸå‡çº¿
        if current_price > ma60 and ma60 > ma120 and ma120 > ma200:
            long_term_buy_reasons.append("ä»·æ ¼ç«™ä¸Šé•¿æœŸå‡çº¿ï¼Œé•¿æœŸè¶‹åŠ¿ç¨³å¥å‘ä¸Šã€‚")
        # ä»·å€¼æŠ•èµ„è€ƒé‡ (ç»“åˆNNè¯„åˆ†)
        pe_ratio = safe_float(row.get('å¸‚ç›ˆç‡(åŠ¨)'))
        if nn_score > 0.8 and pd.notna(pe_ratio) and pe_ratio > 0 and pe_ratio < 30: # NNé«˜è¯„åˆ†ä¸”PEåˆç†
            long_term_buy_reasons.append("NNé«˜è¯„åˆ†ç»“åˆåˆç†å¸‚ç›ˆç‡ï¼Œå…·å¤‡é•¿æœŸæŠ•èµ„ä»·å€¼ã€‚")

        # é•¿æœŸå‡çº¿æ­»å‰
        if ma60 < ma120 and current_price < ma60:
            long_term_sell_reasons.append("é•¿æœŸå‡çº¿å½¢æˆæ­»å‰ï¼Œé•¿æœŸè¶‹åŠ¿å¯èƒ½åè½¬å‘ä¸‹ã€‚")
        # ä»·æ ¼è·Œç ´é•¿æœŸè¶‹åŠ¿çº¿
        if current_price < ma60 and ma60 < ma200:
            long_term_sell_reasons.append("ä»·æ ¼è·Œç ´é•¿æœŸè¶‹åŠ¿çº¿ï¼Œé•¿æœŸæ”¯æ’‘å¤±æ•ˆã€‚")
        # åŸºæœ¬é¢æ¶åŒ– (ä¾‹å¦‚ï¼Œå‡€åˆ©æ¶¦ä¸ºè´Ÿæˆ–å¤§å¹…ä¸‹é™ï¼Œè¿™é‡Œéœ€è¦æ›´å¤šå†å²è´¢åŠ¡æ•°æ®æ¥åˆ¤æ–­)
        if safe_float(row.get('å½’å±å‡€åˆ©æ¶¦')) < 0 and safe_float(row.get('æ€»å¸‚å€¼')) > 0: # äºæŸä¸”éSTè‚¡
            long_term_sell_reasons.append("å…¬å¸å½’å±å‡€åˆ©æ¶¦ä¸ºè´Ÿï¼ŒåŸºæœ¬é¢æ¶åŒ–ï¼Œä¸é€‚åˆé•¿æœŸæŒæœ‰ã€‚")

    if long_term_buy_reasons:
        signals.append("é•¿æœŸä¹°å…¥")
        reasons.append("é•¿æœŸè¶‹åŠ¿/ä»·å€¼ç§¯æä¿¡å·ï¼š" + " ".join(long_term_buy_reasons))
    if long_term_sell_reasons:
        signals.append("é•¿æœŸå–å‡º")
        reasons.append("é•¿æœŸè¶‹åŠ¿/ä»·å€¼æ¶ˆæä¿¡å·ï¼š" + " ".join(long_term_sell_reasons))

    # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ä¹°å–ä¿¡å·ï¼Œä½†NNè¯„åˆ†ä¸­ç­‰ï¼Œåˆ™å»ºè®®è§‚æœ›
    if not short_term_buy_reasons and not short_term_sell_reasons and \
       not long_term_buy_reasons and not long_term_sell_reasons and \
       "è§‚æœ›" not in signals:
        signals.append("è§‚æœ›")
        reasons.append("æ— æ˜ç¡®æŠ€æœ¯æˆ–åŸºæœ¬é¢ä¿¡å·ï¼Œå»ºè®®è§‚æœ›ã€‚")
    
    # ç¡®ä¿ä¿¡å·å’ŒåŸå› åˆ—è¡¨ä¸ä¸ºç©º
    if not signals:
        signals = ["æ— æ˜ç¡®ä¿¡å·"]
    if not reasons:
        reasons = ["æ— å…·ä½“åŸå› "]

    return signals, reasons

def perform_association_rule_mining(df):
    """
    ä½¿ç”¨å…³è”è§„åˆ™æŒ–æ˜æ¥å‘ç°è‹æ°é‡åŒ–ç­–ç•¥æ¡ä»¶ã€åŸºæœ¬é¢å’ŒæŠ€æœ¯æŒ‡æ ‡ä¸é«˜æ¶¨å¹…ä¹‹é—´çš„å…³ç³»ã€‚
    """
    print("\n4. æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜...")

    # å‡†å¤‡æ•°æ®ï¼šå°†ç‰¹å¾å’Œç›®æ ‡å˜é‡äºŒå€¼åŒ–
    data_for_ar = []
    
    # è·å–æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    today_str = datetime.now().strftime('%Y%m%d')
    start_date_hist = (datetime.now() - timedelta(days=300)).strftime('%Y%m%d')
    
    hist_data_map = {}
    for symbol in df['åŸå§‹ä»£ç '].unique():
        hist_data_map[symbol] = calculate_technical_indicators(
            get_stock_history_data(symbol, start_date_hist, today_str)
        )

    for _, row in df.iterrows():
        # è¿™é‡Œä»ç„¶ä½¿ç”¨ calculate_nn_features è·å–åŸºæœ¬é¢å’Œè‹æ°ç­–ç•¥ç‰¹å¾
        nn_features_list = calculate_nn_features(row)
        items = []

        # è‹æ°é‡åŒ–ç­–ç•¥ç‰¹å¾
        if nn_features_list[0] == 1: items.append("F_ä»·æ ¼ä½ç½®_æ»¡è¶³")
        else: items.append("F_ä»·æ ¼ä½ç½®_ä¸æ»¡è¶³")
        if nn_features_list[1] == 1: items.append("G_æ¶¨å¹…ä½ç½®_æ»¡è¶³")
        else: items.append("G_æ¶¨å¹…ä½ç½®_ä¸æ»¡è¶³")

        # åŸºæœ¬é¢ç‰¹å¾
        if nn_features_list[2] >= 0.3: items.append("H_å‡€åˆ©æ¶¦_é«˜") # 0.3äº¿
        else: items.append("H_å‡€åˆ©æ¶¦_ä½")
        if nn_features_list[3] <= 20: items.append("I_æ¢æ‰‹ç‡_ä½") # 20%
        else: items.append("I_æ¢æ‰‹ç‡_é«˜")
        if nn_features_list[4] >= 300: items.append("J_å¸‚å€¼_å¤§") # 300äº¿
        else: items.append("J_å¸‚å€¼_å°")
        if nn_features_list[5] > 0 and nn_features_list[5] < 50: items.append("K_å¸‚ç›ˆç‡_åˆç†") # 0-50
        else: items.append("K_å¸‚ç›ˆç‡_ä¸åˆç†")

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (äºŒå€¼åŒ–) - å…³è”è§„åˆ™æŒ–æ˜å¯ä»¥ç»§ç»­ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡æ¥å‘ç°æ¨¡å¼
        symbol = row.get('åŸå§‹ä»£ç ')
        tech_indicators = hist_data_map.get(symbol, {})

        current_price = safe_float(row.get('æœ€æ–°'))
        ma20 = tech_indicators.get('MA20', np.nan)
        ma60 = tech_indicators.get('MA60', np.nan)
        rsi = tech_indicators.get('RSI', np.nan)
        macd_hist = tech_indicators.get('MACD_HIST', np.nan)

        if pd.notna(current_price) and pd.notna(ma20) and current_price > ma20: items.append("æŠ€æœ¯_ä»·æ ¼é«˜äºMA20")
        else: items.append("æŠ€æœ¯_ä»·æ ¼ä½äºMA20")
        if pd.notna(current_price) and pd.notna(ma60) and current_price > ma60: items.append("æŠ€æœ¯_ä»·æ ¼é«˜äºMA60")
        else: items.append("æŠ€æœ¯_ä»·æ ¼ä½äºMA60")
        if pd.notna(rsi) and rsi < 30: items.append("æŠ€æœ¯_RSIè¶…å–")
        if pd.notna(rsi) and rsi > 70: items.append("æŠ€æœ¯_RSIè¶…ä¹°")
        if pd.notna(macd_hist) and macd_hist > 0: items.append("æŠ€æœ¯_MACDé‡‘å‰")
        else: items.append("æŠ€æœ¯_MACDæ­»å‰")

        # ç›®æ ‡å˜é‡ï¼šé«˜æ¶¨å¹… (ä¾‹å¦‚ï¼Œæ¶¨å¹… > 2%)
        change = safe_float(row.get('æ¶¨å¹…%'))
        if pd.notna(change) and change > 2.0: # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
            items.append("é«˜æ¶¨å¹…")
        else:
            items.append("ä½æ¶¨å¹…")

        data_for_ar.append(items)

    if not data_for_ar:
        print("   âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ã€‚")
        return

    te = TransactionEncoder()
    te_ary = te.fit(data_for_ar).transform(data_for_ar)
    df_ar = pd.DataFrame(te_ary, columns=te.columns_)

    # æŸ¥æ‰¾é¢‘ç¹é¡¹é›†
    frequent_itemsets = apriori(df_ar, min_support=0.005, use_colnames=True) # é™ä½min_supportä»¥å‘ç°æ›´å¤šè§„åˆ™
    if frequent_itemsets.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·å°è¯•é™ä½ min_supportã€‚")
        return

    # ç”Ÿæˆå…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.2) # æé«˜min_thresholdä»¥è·å–æ›´å¼ºçš„å…³è”
    if rules.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°æœ‰æ„ä¹‰çš„å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½ min_threshold æˆ–æ£€æŸ¥æ•°æ®ã€‚")
        return

    # ç­›é€‰å¹¶æ‰“å°ä¸â€œé«˜æ¶¨å¹…â€ç›¸å…³çš„è§„åˆ™
    high_return_rules = rules[rules['consequents'].apply(lambda x: 'é«˜æ¶¨å¹…' in x)]
    high_return_rules = high_return_rules.sort_values(by=['lift', 'confidence'], ascending=False)

    print("\n   å‘ç°ä»¥ä¸‹ä¸ 'é«˜æ¶¨å¹…' ç›¸å…³çš„å…³è”è§„åˆ™ (æŒ‰ Lift é™åº):")
    if high_return_rules.empty:
        print("   æœªæ‰¾åˆ°ç›´æ¥å¯¼è‡´ 'é«˜æ¶¨å¹…' çš„å…³è”è§„åˆ™ã€‚")
    else:
        for i, rule in high_return_rules.head(15).iterrows(): # åªæ˜¾ç¤ºå‰15æ¡
            antecedent_str = ', '.join(list(rule['antecedents']))
            consequent_str = ', '.join(list(rule['consequents']))
            print(f"   è§„åˆ™ {i+1}: {antecedent_str} => {consequent_str}")
            print(f"     æ”¯æŒåº¦ (Support): {rule['support']:.4f}")
            print(f"     ç½®ä¿¡åº¦ (Confidence): {rule['confidence']:.4f}")
            print(f"     æå‡åº¦ (Lift): {rule['lift']:.4f}")
            print("-" * 40)

    print("\n   å…³è”è§„åˆ™æŒ–æ˜å®Œæˆã€‚è¿™äº›è§„åˆ™å¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ´å¯Ÿã€‚")


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆç¥ç»ç½‘ç»œä¸å…³è”è§„åˆ™)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ®...")

    df = pd.DataFrame()
    # å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        # ç»Ÿä¸€åˆ—å
        df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°',
            'æ¶¨è·Œå¹…': 'æ¶¨å¹…%',
            'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)' # ç¡®ä¿åˆ—åä¸€è‡´
        }, inplace=True)

        # ä¿å­˜åŸå§‹ä»£ç 
        df_realtime['åŸå§‹ä»£ç '] = df_realtime['ä»£ç '].copy()

        # æ ¼å¼åŒ–ä»£ç 
        df_realtime['ä»£ç '] = df_realtime['ä»£ç '].apply(lambda x: f'= "{str(x)}"')

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨ï¼Œå¹¶åˆå§‹åŒ–ä¸ºNoneæˆ–é»˜è®¤å€¼
        required_cols = ['ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%',
                         'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼',
                         'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'åŸå§‹ä»£ç ']
        for col in required_cols:
            if col not in df_realtime.columns:
                df_realtime[col] = np.nan # ä½¿ç”¨NaNæ–¹ä¾¿åç»­å¤„ç†

        df = df_realtime

    except Exception as e:
        print(f"   âŒ å®æ—¶è·å–å¤±è´¥: {e}")
        print("   ä½¿ç”¨å‚è€ƒæ•°æ®ä½œä¸ºå¤‡é€‰...")

        # ä½¿ç”¨å‚è€ƒæ•°æ®
        try:
            df_ref = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
            print(f"   âœ… ä»å‚è€ƒæ–‡ä»¶åŠ è½½äº† {len(df_ref)} æ¡æ•°æ®")
            df_ref['åŸå§‹ä»£ç '] = df_ref['ä»£ç '].str.replace('= "', '').str.replace('"', '')
            df = df_ref
        except Exception as e2:
            print(f"   âŒ æ— æ³•åŠ è½½å‚è€ƒæ•°æ®: {e2}")
            return

    # å°è¯•è¡¥å……å‡çº¿å’Œè´¢åŠ¡æ•°æ® (å¦‚æœå®æ—¶æ•°æ®ç¼ºå¤±)
    try:
        ref_df_path = 'å‚è€ƒæ•°æ®/Table.xls'
        if os.path.exists(ref_df_path):
            ref_df = pd.read_csv(ref_df_path, sep='\t', encoding='gbk', dtype=str)
            ref_map = {}
            for _, row in ref_df.iterrows():
                code = str(row['ä»£ç ']).replace('= "', '').replace('"', '')
                ref_map[code] = row.to_dict()

            # åˆå¹¶å‚è€ƒæ•°æ®
            merged_count = 0
            for i, row in df.iterrows():
                code = row.get('åŸå§‹ä»£ç ')
                if code and code in ref_map:
                    ref = ref_map[code]
                    # è¡¥å……ç¼ºå¤±çš„æ•°æ®
                    for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'æ‰€å±è¡Œä¸š', 'å½’å±å‡€åˆ©æ¶¦', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡(åŠ¨)']:
                        if col in ref and pd.isna(df.loc[i, col]): # åªè¡¥å……NaNçš„å€¼
                            df.loc[i, col] = ref[col]
                    merged_count += 1
            print(f"   âœ… è¡¥å……äº† {merged_count} æ¡å‚è€ƒæ•°æ®")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ°å‚è€ƒæ•°æ®æ–‡ä»¶ 'å‚è€ƒæ•°æ®/Table.xls'ï¼Œæ— æ³•è¡¥å……æ•°æ®ã€‚")
    except Exception as e:
        print(f"   âš ï¸ è¡¥å……å‚è€ƒæ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    # ç»Ÿä¸€æ•°æ®æ ¼å¼
    for col in ['æœ€æ–°', 'æœ€é«˜', 'æœ€ä½', 'å¼€ç›˜', 'æ˜¨æ”¶', 'æ¶¨å¹…%', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']:
        df[col] = df[col].apply(safe_float)

    # æ·»åŠ åºå·
    df['åº'] = range(1, len(df) + 1)
    df['Unnamed: 16'] = '' # ä¿æŒä¸åŸæ–‡ä»¶æ ¼å¼ä¸€è‡´

    # é€‰æ‹©è¾“å‡ºåˆ—
    output_columns = [
        'åº', 'ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½',
        'å®é™…æ¢æ‰‹%', 'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
        'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'Unnamed: 16'
    ]

    # ç¡®ä¿æ‰€æœ‰è¾“å‡ºåˆ—éƒ½å­˜åœ¨ï¼Œå¹¶å¡«å……é»˜è®¤å€¼
    for col in output_columns:
        if col not in df.columns:
            df[col] = np.nan if col not in ['ä»£ç ', 'åç§°', 'Unnamed: 16'] else ' --'

    # æ ¼å¼åŒ–è¾“å‡ºåˆ°CSVçš„æ•°å€¼åˆ—
    df_for_output_csv = df[output_columns].copy() # å¤åˆ¶ä¸€ä»½ç”¨äºCSVè¾“å‡º
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']:
        df_for_output_csv[col] = df_for_output_csv[col].apply(lambda x: f" {x:.2f}" if pd.notna(x) else " --")

    # æ ¼å¼åŒ–ä»£ç å’Œåç§°
    df_for_output_csv['ä»£ç '] = df_for_output_csv['ä»£ç '].apply(lambda x: f'= "{str(x)}"' if not str(x).startswith('=') else x)
    df_for_output_csv['åç§°'] = df_for_output_csv['åç§°'].apply(lambda x: f" {x}" if not str(x).startswith(' ') else x)

    # ä¿å­˜Aè‚¡æ•°æ®
    output_file1 = 'è¾“å‡ºæ•°æ®/Aè‚¡æ•°æ®.csv'
    df_for_output_csv.to_csv(output_file1, index=False, encoding='utf-8-sig')
    print(f"\nâœ… Aè‚¡æ•°æ®å·²ä¿å­˜: {output_file1}")
    print(f"   å…± {len(df_for_output_csv)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ ==========
    print("\n2. è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
    # ä¼ å…¥åŸå§‹æ•°å€¼çš„dfå‰¯æœ¬ï¼Œé¿å…æ ¼å¼åŒ–å½±å“è®­ç»ƒ
    df_for_training = df.copy()
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å¼€ç›˜', 'æ˜¨æ”¶', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']:
        df_for_training[col] = df_for_training[col].apply(safe_float)

    # ç¥ç»ç½‘ç»œè®­ç»ƒæ—¶ï¼Œå…¶è¾“å…¥ç‰¹å¾ä¸å†åŒ…å«æŠ€æœ¯æŒ‡æ ‡
    model, scaler = train_neural_network(df_for_training)

    if model is None:
        print("   âŒ ç¥ç»ç½‘ç»œè®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ç­›é€‰ã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨å¹¶ç”Ÿæˆç­–ç•¥ä¿¡å· ==========
    print("\n3. åŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨ (åŸºäºç¥ç»ç½‘ç»œè¯„åˆ†å’Œç­–ç•¥ä¿¡å·)...")

    quality_stocks = []
    
    # é‡æ–°åŠ è½½åŸå§‹æ•°å€¼çš„dfï¼Œå› ä¸ºä¸Šé¢ä¸ºäº†è¾“å‡ºcsvå·²ç»æ ¼å¼åŒ–äº†
    df_for_scoring = df.copy()
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']:
        df_for_scoring[col] = df_for_scoring[col].apply(safe_float)
    df_for_scoring['åŸå§‹ä»£ç '] = df_for_scoring['ä»£ç '].apply(lambda x: str(x).replace('= "', '').replace('"', ''))

    # é¢„å…ˆè·å–æ‰€æœ‰è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œç”¨äºè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (ä¾›ç­–ç•¥æŠ¥å‘Šä½¿ç”¨)
    today_str = datetime.now().strftime('%Y%m%d')
    start_date_hist = (datetime.now() - timedelta(days=300)).strftime('%Y%m%d') # è¶³å¤Ÿé•¿çš„æ—¶é—´æ¥è®¡ç®—200æ—¥å‡çº¿
    
    all_tech_indicators_map = {}
    print("   æ­£åœ¨è·å–æ‰€æœ‰è‚¡ç¥¨å†å²æ•°æ®å¹¶è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ (ä¾›ç­–ç•¥æŠ¥å‘Šä½¿ç”¨)...")
    # ç»Ÿè®¡æˆåŠŸå’Œå¤±è´¥çš„æ•°é‡
    success_count = 0
    fail_count = 0
    for symbol in df_for_scoring['åŸå§‹ä»£ç '].unique():
        tech_data = calculate_technical_indicators(
            get_stock_history_data(symbol, start_date_hist, today_str)
        )
        if tech_data and not all(pd.isna(v) for v in tech_data.values()): # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°æœ‰æ•ˆæŒ‡æ ‡
            all_tech_indicators_map[symbol] = tech_data
            success_count += 1
        else:
            fail_count += 1
    print(f"   âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆã€‚æˆåŠŸè·å– {success_count} åªè‚¡ç¥¨ï¼Œå¤±è´¥ {fail_count} åªã€‚")

    for idx, row in df_for_scoring.iterrows():
        symbol = str(row['åŸå§‹ä»£ç ']).strip()
        tech_indicators = all_tech_indicators_map.get(symbol, {}) # è·å–è¯¥è‚¡ç¥¨çš„æŠ€æœ¯æŒ‡æ ‡

        # ç¥ç»ç½‘ç»œé¢„æµ‹ï¼Œä¸ä½¿ç”¨å†å²æŠ€æœ¯æŒ‡æ ‡ä½œä¸ºè¾“å…¥
        nn_score = predict_score_with_nn(row, model, scaler)
        
        # ç”Ÿæˆç­–ç•¥ä¿¡å·ï¼Œè¿™é‡Œä¼šç»“åˆNNè¯„åˆ†å’ŒæŠ€æœ¯æŒ‡æ ‡
        signals, reasons = generate_strategy_signals(row, nn_score, tech_indicators)
        
        if pd.notna(nn_score): # ç¡®ä¿åˆ†æ•°æœ‰æ•ˆ
            quality_stocks.append({
                'ä»£ç ': symbol,
                'åç§°': str(row['åç§°']).strip(),
                'è¡Œä¸š': str(row['æ‰€å±è¡Œä¸š']).strip(),
                'ä¼˜è´¨ç‡': nn_score,
                'ä»Šæ—¥æ¶¨å¹…': f"{safe_float(row['æ¶¨å¹…%']):.2f}%" if pd.notna(safe_float(row['æ¶¨å¹…%'])) else "--",
                'ç­–ç•¥ä¿¡å·': ", ".join(signals),
                'ç­–ç•¥åŸå› ': "\n".join([f"- {r}" for r in reasons]) # æ ¼å¼åŒ–åŸå› åˆ—è¡¨
            })

    # æŒ‰ä¼˜è´¨ç‡é™åºæ’åº
    quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)

    # ç¡®å®šç­›é€‰é˜ˆå€¼ï¼šå–å‰Nä¸ªï¼Œæˆ–è€…æ ¹æ®åˆ†æ•°åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
    display_count = 20 # é»˜è®¤æ˜¾ç¤ºå‰20ä¸ª
    if len(quality_stocks) > display_count:
        threshold = quality_stocks[display_count-1]['ä¼˜è´¨ç‡']
        quality_stocks_filtered = quality_stocks[:display_count]
    elif len(quality_stocks) > 0:
        threshold = quality_stocks[-1]['ä¼˜è´¨ç‡'] # æ‰€æœ‰è‚¡ç¥¨çš„æœ€ä½åˆ†
        quality_stocks_filtered = quality_stocks
    else:
        threshold = 0.0
        quality_stocks_filtered = []

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨å’Œç­–ç•¥ä¿¡å·
    output_file2 = 'è¾“å‡ºæ•°æ®/ä¼˜è´¨è‚¡ç¥¨ä¸ç­–ç•¥æŠ¥å‘Š.txt'
    with open(output_file2, 'w', encoding='utf-8') as f:
        f.write("è‹æ°é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœ (ç¥ç»ç½‘ç»œè¯„åˆ†ä¸è¯¦ç»†ç­–ç•¥æŠ¥å‘Š)\n")
        f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ç¥ç»ç½‘ç»œè¯„åˆ†åŸºäºå®æ—¶åŸºæœ¬é¢å’Œè‹æ°é‡åŒ–ç‰¹å¾ã€‚\n")
        f.write(f"ç­–ç•¥ä¿¡å·ç»“åˆäº†NNè¯„åˆ†å’ŒæŠ€æœ¯æŒ‡æ ‡ï¼ˆæŠ€æœ¯æŒ‡æ ‡ä¸ä½œä¸ºNNè¯„åˆ†çš„ç›´æ¥è¾“å…¥ï¼‰ã€‚\n")
        f.write(f"æœ€ä½ä¼˜è´¨ç‡é˜ˆå€¼ (åŸºäºå‰{display_count}åæˆ–å…¨éƒ¨): {threshold:.4f}\n")
        f.write(f"ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {len(quality_stocks_filtered)}\n")
        f.write("="*80 + "\n\n")

        for stock in quality_stocks_filtered:
            f.write(f"è‚¡ç¥¨ä»£ç : {stock['ä»£ç ']}\n")
            f.write(f"è‚¡ç¥¨åç§°: {stock['åç§°']}\n")
            f.write(f"æ‰€å±è¡Œä¸š: {stock['è¡Œä¸š']}\n")
            f.write(f"ä¼˜è´¨ç‡ (NNè¯„åˆ†): {stock['ä¼˜è´¨ç‡']:.4f}\n")
            f.write(f"ä»Šæ—¥æ¶¨å¹…: {stock['ä»Šæ—¥æ¶¨å¹…']}\n")
            f.write(f"ç­–ç•¥ä¿¡å·: {stock['ç­–ç•¥ä¿¡å·']}\n")
            f.write(f"ç­–ç•¥åŸå› :\n{stock['ç­–ç•¥åŸå› ']}\n")
            f.write("-"*40 + "\n")

    print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨ä¸ç­–ç•¥æŠ¥å‘Šå·²ä¿å­˜: {output_file2}")
    print(f"   æ‰¾åˆ° {len(quality_stocks_filtered)} åªä¼˜è´¨è‚¡ç¥¨ï¼ˆæœ€ä½ä¼˜è´¨ç‡={threshold:.4f}ï¼‰")

    if len(quality_stocks_filtered) > 0:
        print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨åŠç­–ç•¥æŠ¥å‘Š (å‰{len(quality_stocks_filtered)}å)ï¼š")
        print("="*120)
        print(f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'æ¶¨å¹…':<8} {'ä¼˜è´¨ç‡':<10} {'æ‰€å±è¡Œä¸š':<15} {'ç­–ç•¥ä¿¡å·':<20} {'ç­–ç•¥åŸå› ':<40}")
        print("-"*120)
        for stock in quality_stocks_filtered:
            # ä¸ºäº†åœ¨æ§åˆ¶å°æ‰“å°æ—¶é¿å…è¿‡é•¿ï¼Œç­–ç•¥åŸå› åªå–ç¬¬ä¸€è¡Œ
            display_reason = stock['ç­–ç•¥åŸå› '].split('\n')[0].replace('- ', '')
            if len(display_reason) > 38:
                display_reason = display_reason[:35] + "..."
            print(f"{stock['ä»£ç ']:<10} {stock['åç§°']:<12} {stock['ä»Šæ—¥æ¶¨å¹…']:<8} {stock['ä¼˜è´¨ç‡']:.4f}   {stock['è¡Œä¸š']:<15} {stock['ç­–ç•¥ä¿¡å·']:<20} {display_reason:<40}")
    else:
        print("\nâš ï¸ ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å¸‚åœºæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œæ¶¨å¹…ä¸è¶³")
        print("   2. æ•°æ®è·å–ä¸å®Œæ•´æˆ–è´¨é‡ä¸ä½³")
        print("   3. ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦æ›´å¤šæ•°æ®æˆ–ä¼˜åŒ–")
        print("   4. ç­–ç•¥ä¿¡å·æ¡ä»¶è¿‡äºä¸¥æ ¼")

    # ========== ç¬¬å››æ­¥ï¼šå…³è”è§„åˆ™æŒ–æ˜ ==========
    # å…³è”è§„åˆ™æŒ–æ˜å¯ä»¥ç»§ç»­ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡æ¥å‘ç°æ¨¡å¼ï¼Œå› ä¸ºå®ƒæä¾›çš„æ˜¯æ´å¯Ÿï¼Œè€Œä¸æ˜¯å®æ—¶é¢„æµ‹çš„è¾“å…¥
    perform_association_rule_mining(df_for_scoring.copy()) # ä¼ å…¥åŸå§‹æ•°å€¼çš„dfå‰¯æœ¬

    print("\n" + "="*60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
