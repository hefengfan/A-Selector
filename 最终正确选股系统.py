#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - æ ¹æ®æ¯å¤©å®æ—¶æ•°æ®ç­›é€‰
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ† (TensorFlow + Optuna)
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ TensorFlow å’Œ Optuna ç›¸å…³åº“
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import optuna

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'


def calculate_features(row):
    """
    æ ¹æ®è‹æ°é‡åŒ–ç­–ç•¥è®¡ç®—ç‰¹å¾å€¼ï¼Œç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒ
    """
    features = []

    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶
    try:
        low_str = str(row['æœ€ä½']).strip()
        ma60_str = str(row['60æ—¥å‡ä»·']).strip()
        ma20_str = str(row['20æ—¥å‡ä»·']).strip()
        current_str = str(row['æœ€æ–°']).strip()

        if '--' not in low_str and '--' not in ma60_str:
            low = float(low_str)
            ma60 = float(ma60_str)
            current = float(current_str)
            ma20 = float(ma20_str) if '--' not in ma20_str else 0

            condition_met = False
            if ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
                condition_met = True
            if not condition_met and ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
                condition_met = True

            features.append(1 if condition_met else 0)  # 1 or 0
        else:
            features.append(0)
    except:
        features.append(0)

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½®
    try:
        change_str = str(row['æ¶¨å¹…%']).strip()
        current_str = str(row['æœ€æ–°']).strip()
        high_str = str(row['æœ€é«˜']).strip()
        low_str = str(row['æœ€ä½']).strip()

        if '--' not in change_str:
            change = float(change_str)
            current = float(current_str)
            high = float(high_str)
            low = float(low_str)

            if change >= 5.0:
                threshold = high - (high - low) * 0.30
                features.append(1 if current >= threshold else 0)
            else:
                features.append(0)
        else:
            features.append(0)
    except:
        features.append(0)

    # Håˆ—ï¼šå‡€åˆ©æ¶¦>=3000ä¸‡
    try:
        profit_str = str(row['å½’å±å‡€åˆ©æ¶¦']).strip()
        profit = 0

        if 'äº¿' in profit_str:
            profit = float(profit_str.replace('äº¿', ''))
        elif 'ä¸‡' in profit_str:
            profit = float(profit_str.replace('ä¸‡', '')) / 10000

        features.append(profit)  # ç›´æ¥ä½¿ç”¨å‡€åˆ©æ¶¦æ•°å€¼
    except:
        features.append(0)

    # Iåˆ—ï¼šæ¢æ‰‹ç‡<=20%
    try:
        turnover_str = str(row['å®é™…æ¢æ‰‹%']).strip()
        if '--' not in turnover_str:
            turnover = float(turnover_str)
            features.append(turnover)  # ç›´æ¥ä½¿ç”¨æ¢æ‰‹ç‡æ•°å€¼
        else:
            features.append(100) # æ¢æ‰‹ç‡ç¼ºå¤±æ—¶ï¼Œèµ‹äºˆä¸€ä¸ªè¾ƒå¤§çš„å€¼
    except:
        features.append(100)

    # Jåˆ—ï¼šå¸‚å€¼>=300äº¿
    try:
        cap_str = str(row['æ€»å¸‚å€¼']).strip()
        cap = 0

        if 'ä¸‡äº¿' in cap_str:
            cap = float(cap_str.replace('ä¸‡äº¿', '')) * 10000
        elif 'äº¿' in cap_str:
            cap = float(cap_str.replace('äº¿', ''))

        features.append(cap)  # ç›´æ¥ä½¿ç”¨å¸‚å€¼æ•°å€¼
    except:
        features.append(0)

    return features


def create_model(trial, input_shape):
    """
    ä½¿ç”¨ Optuna å»ºè®®çš„è¶…å‚æ•°åˆ›å»º TensorFlow ç¥ç»ç½‘ç»œæ¨¡å‹
    """
    n_layers = trial.suggest_int('n_layers', 1, 3)  # å»ºè®®å±‚æ•°
    model = tf.keras.models.Sequential()
    model.add(tf.keras.layers.Input(shape=input_shape))

    for i in range(n_layers):
        num_units = trial.suggest_int(f'n_units_{i}', 32, 256)  # å»ºè®®ç¥ç»å…ƒæ•°é‡
        activation = trial.suggest_categorical(f'activation_{i}', ['relu', 'tanh', 'sigmoid'])  # å»ºè®®æ¿€æ´»å‡½æ•°
        model.add(tf.keras.layers.Dense(num_units, activation=activation))
        dropout_rate = trial.suggest_float(f'dropout_{i}', 0.0, 0.5)  # å»ºè®® Dropout ç‡
        model.add(tf.keras.layers.Dropout(dropout_rate))

    model.add(tf.keras.layers.Dense(1))  # è¾“å‡ºå±‚
    return model


def objective(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°
    """
    model = create_model(trial, (X_train.shape[1],))
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'rmsprop', 'sgd'])  # å»ºè®®ä¼˜åŒ–å™¨
    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)  # å»ºè®®å­¦ä¹ ç‡

    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])

    # æ·»åŠ  EarlyStopping å›è°ƒ
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)

    _, mse = model.evaluate(X_test, y_test, verbose=0)
    return mse


def train_neural_network(df):
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ (TensorFlow + Optuna)ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n   å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X = []
    y = []  # ç›®æ ‡å˜é‡ï¼šæ¶¨å¹…ä½œä¸ºè¯„åˆ†çš„ä¾æ®
    for _, row in df.iterrows():
        features = calculate_features(row)
        X.append(features)

        # ä½¿ç”¨æ¶¨å¹…ä½œä¸ºç›®æ ‡å˜é‡ï¼Œä¹Ÿå¯ä»¥è€ƒè™‘å…¶ä»–æŒ‡æ ‡
        try:
            change_str = str(row['æ¶¨å¹…%']).strip()
            if '--' not in change_str:
                y.append(float(change_str))
            else:
                y.append(0)  # ç¼ºå¤±æ¶¨å¹…æ—¶ï¼Œèµ‹äºˆ0
        except:
            y.append(0)

    X = np.array(X)
    y = np.array(y)

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1)
    X = X[mask]
    y = y[mask]

    if len(X) == 0:
        print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è®­ç»ƒç¥ç»ç½‘ç»œã€‚")
        return None, None

    # 2. æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç†...")
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 4. ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°
    print("   ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°...")
    study = optuna.create_study(direction='minimize')
    study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=10)  # è°ƒæ•´ trials æ•°é‡

    # 5. ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åˆ›å»ºæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åˆ›å»ºæ¨¡å‹...")
    best_model = create_model(study.best_trial, (X_train.shape[1],))
    best_model.compile(optimizer=study.best_params['optimizer'], loss='mse', metrics=['mse'])

    # 6. è®­ç»ƒæœ€ä½³æ¨¡å‹
    print("   è®­ç»ƒæœ€ä½³æ¨¡å‹...")
    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    best_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, callbacks=[early_stopping], verbose=0)

    # 7. è¯„ä¼°æœ€ä½³æ¨¡å‹
    print("   è¯„ä¼°æœ€ä½³æ¨¡å‹...")
    y_pred = best_model.predict(X_test, verbose=0)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   R^2 Score: {r2:.4f}")

    print("   æœ€ä½³è¶…å‚æ•°:")
    print(study.best_params)

    return best_model, scaler, r2


def predict_score_with_nn(row, model, scaler):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ TensorFlow ç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """
    features = calculate_features(row)
    features = np.array(features).reshape(1, -1)  # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼æˆ–æ— ç©·å€¼
    if np.any(np.isnan(features)) or np.any(np.isinf(features)):
        return 0  # å¦‚æœæœ‰ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼

    features_scaled = scaler.transform(features)
    score = model.predict(features_scaled, verbose=0)[0][0]
    return score


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ")
    print("é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ† (TensorFlow + Optuna)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ®...")

    # å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        # ä¿å­˜åŸå§‹ä»£ç 
        df['åŸå§‹ä»£ç '] = df['ä»£ç '].copy()

        # æ ¼å¼åŒ–ä»£ç 
        df['ä»£ç '] = df['ä»£ç '].apply(lambda x: f'= "{str(x)}"')

        # æ ¼å¼åŒ–æ•°å€¼åˆ—
        for col in ['æœ€æ–°ä»·', 'æœ€é«˜', 'æœ€ä½', 'å¼€ç›˜', 'æ˜¨æ”¶']:
            if col in df.columns:
                new_col = col.replace('ä»·', '')
                df[new_col] = col.apply(
                    lambda x: f" {float(x):.2f}" if pd.notna(x) and str(x) not in ['--', '', None] else " --"
                )

        if 'æ¶¨è·Œå¹…' in df.columns:
            df['æ¶¨å¹…%'] = df['æ¶¨è·Œå¹…'].apply(
                lambda x: f" {float(x):.2f}" if pd.notna(x) else " --"
            )

        if 'æ¢æ‰‹ç‡' in df.columns:
            df['å®é™…æ¢æ‰‹%'] = df['æ¢æ‰‹ç‡'].apply(
                lambda x: f" {float(x):.2f}" if pd.notna(x) else " --"
            )

        # å¤„ç†åç§°
        df['åç§°'] = df['åç§°'].apply(lambda x: f" {x}" if not str(x).startswith(' ') else x)

        # è®¾ç½®é»˜è®¤å€¼
        df['æ‰€å±è¡Œä¸š'] = '  å…¶ä»–'
        df['20æ—¥å‡ä»·'] = ' --'
        df['60æ—¥å‡ä»·'] = ' --'
        df['å½’å±å‡€åˆ©æ¶¦'] = ' --'
        df['å¸‚ç›ˆç‡(åŠ¨)'] = ' --'
        df['æ€»å¸‚å€¼'] = ' --'

    except Exception as e:
        print(f"   âŒ å®æ—¶è·å–å¤±è´¥: {e}")
        print("   ä½¿ç”¨å‚è€ƒæ•°æ®ä½œä¸ºå¤‡é€‰...")

        # ä½¿ç”¨å‚è€ƒæ•°æ®
        try:
            df = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
            print(f"   âœ… ä»å‚è€ƒæ–‡ä»¶åŠ è½½äº† {len(df)} æ¡æ•°æ®")
            df['åŸå§‹ä»£ç '] = df['ä»£ç '].str.replace('= "', '').str.replace('"', '')
        except Exception as e2:
            print(f"   âŒ æ— æ³•åŠ è½½å‚è€ƒæ•°æ®: {e2}")
            return

    # å°è¯•è¡¥å……å‡çº¿å’Œè´¢åŠ¡æ•°æ®
    try:
        ref_df = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
        ref_map = {}
        for _, row in ref_df.iterrows():
            code = str(row['ä»£ç ']).replace('= "', '').replace('"', '')
            ref_map[code] = row.to_dict()

        # åˆå¹¶å‚è€ƒæ•°æ®
        for i, code in enumerate(df.get('åŸå§‹ä»£ç ', [])):
            if code in ref_map:
                ref = ref_map[code]
                # è¡¥å……ç¼ºå¤±çš„æ•°æ®
                for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'æ‰€å±è¡Œä¸š', 'å½’å±å‡€åˆ©æ¶¦', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡(åŠ¨)']:
                    if col in ref:
                        df.loc[i, col] = ref[col]

        print(f"   âœ… è¡¥å……äº† {len(ref_map)} æ¡å‚è€ƒæ•°æ®")
    except:
        print("   âš ï¸ æ— æ³•è¡¥å……å‚è€ƒæ•°æ®")

    # æ·»åŠ åºå·
    df['åº'] = range(1, len(df) + 1)
    df['Unnamed: 16'] = ''

    # é€‰æ‹©è¾“å‡ºåˆ—
    output_columns = [
        'åº', 'ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½',
        'å®é™…æ¢æ‰‹%', 'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
        'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'Unnamed: 16'
    ]

    for col in output_columns:
        if col not in df.columns:
            df[col] = ' --' if col != 'Unnamed: 16' else ''

    final_df = df[output_columns]

    # ä¿å­˜Aè‚¡æ•°æ®
    output_file1 = 'è¾“å‡ºæ•°æ®/Aè‚¡æ•°æ®.csv'
    final_df.to_csv(output_file1, index=False, encoding='utf-8-sig')
    print(f"\nâœ… Aè‚¡æ•°æ®å·²ä¿å­˜: {output_file1}")
    print(f"   å…± {len(final_df)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ ==========
    print("\n2. è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
    model, scaler, r2_score = train_neural_network(final_df)

    if model is None:
        print("   âŒ ç¥ç»ç½‘ç»œè®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ç­›é€‰ã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨ ==========
    print("\n3. åŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨...")

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è‚¡ç¥¨è¯„åˆ†çš„åˆ—
    final_df['ç¥ç»ç½‘ç»œè¯„åˆ†'] = final_df.apply(lambda row: predict_score_with_nn(row, model, scaler), axis=1)

    quality_stocks = []
    threshold = 0.0  # è°ƒæ•´é˜ˆå€¼ä»¥è·å¾—æ›´å¤šç»“æœ

    # ç»Ÿè®¡
    stats = {'F': 0, 'G': 0, 'H': 0, 'I': 0, 'J': 0}

    for idx, row in final_df.iterrows():
        score = row['ç¥ç»ç½‘ç»œè¯„åˆ†']  # ç›´æ¥ä½¿ç”¨ç¥ç»ç½‘ç»œè¯„åˆ†
        conditions = ""  # ç¥ç»ç½‘ç»œè¯„åˆ†ä¸éœ€è¦æ¡ä»¶

        # ç»Ÿè®¡ï¼ˆåŸå§‹è¯„åˆ†æ–¹å¼çš„ç»Ÿè®¡ï¼Œå¦‚æœåªç”¨ç¥ç»ç½‘ç»œï¼Œå¯ä»¥ç§»é™¤ï¼‰
        features = calculate_features(row)
        if features[0] == 1: stats['F'] += 1
        if features[1] == 1: stats['G'] += 1
        if features[2] > 0.3: stats['H'] += 1
        if features[3] <= 25: stats['I'] += 1
        if features[4] >= 200: stats['J'] += 1

        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if score >= threshold:
            code = str(row['ä»£ç ']).replace('= "', '').replace('"', '')
            quality_stocks.append({
                'ä»£ç ': code,
                'åç§°': str(row['åç§°']).strip(),
                'è¡Œä¸š': str(row['æ‰€å±è¡Œä¸š']).strip(),
                'ä¼˜è´¨ç‡': score,
                'æ»¡è¶³æ¡ä»¶': conditions,
                'æ¶¨å¹…': str(row['æ¶¨å¹…%']).strip()
            })

    # æ‰“å°ç»Ÿè®¡
    total = len(final_df)
    if total > 0:
        print(f"\n   æ¡ä»¶æ»¡è¶³ç»Ÿè®¡ï¼ˆå…±{total}åªè‚¡ç¥¨ï¼‰ï¼š")
        print(f"   Fåˆ—(ä»·æ ¼ä½ç½®): {stats['F']}åª ({stats['F']/total*100:.1f}%)")
        print(f"   Gåˆ—(æ¶¨å¹…æ¡ä»¶): {stats['G']}åª ({stats['G']/total*100:.1f}%)")
        print(f"   Håˆ—(å‡€åˆ©æ¶¦): {stats['H']}åª ({stats['H']/total*100:.1f}%)")
        print(f"   Iåˆ—(æ¢æ‰‹ç‡): {stats['I']}åª ({stats['I']/total*100:.1f}%)")
        print(f"   Jåˆ—(å¸‚å€¼): {stats['J']}åª ({stats['J']/total*100:.1f}%)")

    # æŒ‰ä¼˜è´¨ç‡é™åºæ’åº
    quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)

    # å¦‚æœç»“æœå¤ªå°‘ï¼Œå°è¯•é™ä½é˜ˆå€¼
    if len(quality_stocks) < 10:
        print(f"\n   âš ï¸ åªæ‰¾åˆ°{len(quality_stocks)}åªè‚¡ç¥¨ï¼Œå°è¯•é™ä½é˜ˆå€¼...")
        threshold = np.percentile([stock['ä¼˜è´¨ç‡'] for stock in quality_stocks], 25) if quality_stocks else 0  # ä½¿ç”¨25%åˆ†ä½æ•°ä½œä¸ºé˜ˆå€¼
        quality_stocks = []

        for idx, row in final_df.iterrows():
            score = row['ç¥ç»ç½‘ç»œè¯„åˆ†']
            if score >= threshold:
                code = str(row['ä»£ç ']).replace('= "', '').replace('"', '')
                quality_stocks.append({
                    'ä»£ç ': code,
                    'åç§°': str(row['åç§°']).strip(),
                    'è¡Œä¸š': str(row['æ‰€å±è¡Œä¸š']).strip(),
                    'ä¼˜è´¨ç‡': score,
                    'æ»¡è¶³æ¡ä»¶': "",  # ç¥ç»ç½‘ç»œè¯„åˆ†ä¸éœ€è¦æ¡ä»¶
                    'æ¶¨å¹…': str(row['æ¶¨å¹…%']).strip()
                })

        quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)
        quality_stocks = quality_stocks[:12]  # åªå–å‰12åª

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨
    output_file2 = 'è¾“å‡ºæ•°æ®/ä¼˜è´¨è‚¡ç¥¨.txt'
    with open(output_file2, 'w', encoding='utf-8') as f:
        f.write("è‹æ°é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœ (TensorFlow + Optuna ç¥ç»ç½‘ç»œè¯„åˆ†)\n")
        f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ¨¡å‹å‡†ç¡®ç‡ (R^2): {r2_score:.4f}\n") # æ˜¾ç¤ºæ¨¡å‹å‡†ç¡®ç‡
        f.write(f"ç­›é€‰é˜ˆå€¼: {threshold:.4f}\n")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„é˜ˆå€¼
        f.write(f"ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {len(quality_stocks)}\n")
        f.write("=" * 50 + "\n\n")

        for stock in quality_stocks:
            f.write(f"è‚¡ç¥¨ä»£ç : {stock['ä»£ç ']}\n")
            f.write(f"è‚¡ç¥¨åç§°: {stock['åç§°']}\n")
            f.write(f"æ‰€å±è¡Œä¸š: {stock['è¡Œä¸š']}\n")
            f.write(f"ä¼˜è´¨ç‡: {stock['ä¼˜è´¨ç‡']:.4f}\n")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„è¯„åˆ†
            f.write(f"æ»¡è¶³æ¡ä»¶: {stock['æ»¡è¶³æ¡ä»¶']}\n")
            f.write(f"ä»Šæ—¥æ¶¨å¹…: {stock['æ¶¨å¹…']}\n")
            f.write("-" * 30 + "\n")

    print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨å·²ä¿å­˜: {output_file2}")
    print(f"   æ‰¾åˆ° {len(quality_stocks)} åªä¼˜è´¨è‚¡ç¥¨ï¼ˆé˜ˆå€¼={threshold:.4f}ï¼‰")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„é˜ˆå€¼

    if len(quality_stocks) > 0:
        print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨ï¼š")
        print("=" * 60)
        print("è‚¡ç¥¨ä»£ç     è‚¡ç¥¨åç§°        æ¶¨å¹…%      ä¼˜è´¨ç‡")
        print("-" * 60)
        for stock in quality_stocks[:12]:
            print(f"{stock['ä»£ç ']:8}    {stock['åç§°']:12}    {stock['æ¶¨å¹…']:6}    {stock['ä¼˜è´¨ç‡']:.4f}")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„è¯„åˆ†
    else:
        print("\nâš ï¸ ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å¸‚åœºæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œæ¶¨å¹…ä¸è¶³")
        print("   2. æ•°æ®è·å–ä¸å®Œæ•´")
        print("   3. ç­›é€‰æ¡ä»¶è¿‡äºä¸¥æ ¼")

    # å°†åŒ…å«ç¥ç»ç½‘ç»œè¯„åˆ†çš„ DataFrame ä¿å­˜åˆ° CSV
    output_file1 = 'è¾“å‡ºæ•°æ®/Aè‚¡æ•°æ®.csv'
    final_df.to_csv(output_file1, index=False, encoding='utf-8-sig')
    print(f"\nâœ… åŒ…å«ç¥ç»ç½‘ç»œè¯„åˆ†çš„ A è‚¡æ•°æ®å·²ä¿å­˜: {output_file1}")

    print("\n" + "=" * 60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
