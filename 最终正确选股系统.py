#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆç¥ç»ç½‘ç»œä¸å…³è”è§„åˆ™)
ç»“åˆä¸“é—¨æŠ€æœ¯ä¼˜åŒ–é¢„æµ‹æ¨¡å‹ ç»“åˆç‰¹å¾å’Œé¢„æµ‹ç»“æœç»™æˆ‘ç­–ç•¥ é’ˆå¯¹æ¨èè‚¡ç¥¨ çŸ­æœŸ/é•¿æœŸ ä¹°å…¥/å–å‡º ç»“åˆä¸“ä¸šåˆ†æè¿›è¡Œè¡¥å…… å®Œæ•´ä»£ç 
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ç¥ç»ç½‘ç»œç›¸å…³åº“
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score

# å¯¼å…¥ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 10000
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 100000000
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) / 10000
        return float(s)
    except ValueError:
        return np.nan

def calculate_features(row):
    """
    æ ¹æ®è‹æ°é‡åŒ–ç­–ç•¥è®¡ç®—ç‰¹å¾å€¼ï¼Œç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒå’Œå…³è”è§„åˆ™æŒ–æ˜ã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ•°å€¼ç‰¹å¾çš„åˆ—è¡¨ã€‚
    """
    features = []

    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶ (0æˆ–1)
    try:
        low = safe_float(row.get('æœ€ä½'))
        ma60 = safe_float(row.get('60æ—¥å‡ä»·'))
        ma20 = safe_float(row.get('20æ—¥å‡ä»·'))
        current = safe_float(row.get('æœ€æ–°'))

        f_condition = 0
        if pd.notna(low) and pd.notna(ma60) and ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
            f_condition = 1
        elif pd.notna(current) and pd.notna(ma20) and ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
            f_condition = 1
        features.append(f_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½® (0æˆ–1)
    try:
        change = safe_float(row.get('æ¶¨è·Œå¹…')) # ä½¿ç”¨æ¶¨è·Œå¹…ä»£æ›¿æ¶¨å¹…%
        current = safe_float(row.get('æœ€æ–°'))
        high = safe_float(row.get('æœ€é«˜'))
        low = safe_float(row.get('æœ€ä½'))

        g_condition = 0
        if pd.notna(change) and change >= 5.0 and pd.notna(current) and pd.notna(high) and pd.notna(low):
            if (high - low) > 0: # é¿å…é™¤ä»¥é›¶
                threshold = high - (high - low) * 0.30
                if current >= threshold:
                    g_condition = 1
            elif current == high: # å¦‚æœæœ€é«˜æœ€ä½ç›¸åŒï¼Œä¸”æ¶¨å¹…>=5ï¼Œä¹Ÿç®—æ»¡è¶³
                g_condition = 1
        features.append(g_condition)
    except:
        features.append(0) # é»˜è®¤å€¼

    # Håˆ—ï¼šå½’å±å‡€åˆ©æ¶¦ (æ•°å€¼ï¼Œå•ä½äº¿) -  æ— æ³•è·å–ï¼Œç”¨0ä»£æ›¿
    features.append(0)

    # Iåˆ—ï¼šå®é™…æ¢æ‰‹ç‡ (æ•°å€¼)
    try:
        turnover = safe_float(row.get('æ¢æ‰‹ç‡')) # ä½¿ç”¨æ¢æ‰‹ç‡ä»£æ›¿å®é™…æ¢æ‰‹%
        features.append(turnover if pd.notna(turnover) else 100) # ç¼ºå¤±æ—¶ç»™ä¸€ä¸ªè¾ƒå¤§å€¼
    except:
        features.append(100)

    # Jåˆ—ï¼šæ€»å¸‚å€¼ (æ•°å€¼ï¼Œå•ä½äº¿)
    try:
        cap = safe_float(row.get('æ€»å¸‚å€¼'))
        features.append(cap if pd.notna(cap) else 0)
    except:
        features.append(0)

    return features

def calculate_technical_indicators(df_row):
    """
    è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼šç®€å•ç§»åŠ¨å¹³å‡çº¿äº¤å‰, RSI, ä»·æ ¼ä¸å¸ƒæ—å¸¦å…³ç³», æˆäº¤é‡å˜åŒ–ç‡
    ä½¿ç”¨ä¼ å…¥çš„å•è¡Œæ•°æ®ä»£æ›¿å†å²Kçº¿æ•°æ®
    """
    try:
        # è·å–å®æ—¶æ•°æ®
        close_price = safe_float(df_row.get('æœ€æ–°'))
        high_price = safe_float(df_row.get('æœ€é«˜'))
        low_price = safe_float(df_row.get('æœ€ä½'))

        # ç®€å•ç§»åŠ¨å¹³å‡çº¿äº¤å‰ä¿¡å· (1:é‡‘å‰, -1:æ­»å‰, 0:æ— ) - æ— æ³•è®¡ç®—ï¼Œç»™ä¸€ä¸ªä¸­æ€§å€¼
        sma_signal = 0

        # ç›¸å¯¹å¼ºå¼±æŒ‡æ ‡ (RSI) - ç®€åŒ–è®¡ç®—
        rsi = 50 # æ— æ³•è®¡ç®—ï¼Œç»™ä¸€ä¸ªä¸­æ€§å€¼

        # å¸ƒæ—å¸¦ä½ç½® (1:ä¸Šè½¨, -1:ä¸‹è½¨, 0:ä¸­è½¨)
        boll_position = 0 # æ— æ³•è®¡ç®—ï¼Œç»™ä¸€ä¸ªä¸­æ€§å€¼

        # æˆäº¤é‡å˜åŒ–ç‡ (5æ—¥å¹³å‡æˆäº¤é‡/20æ—¥å¹³å‡æˆäº¤é‡)
        vol_ratio = 1 # æ— æ³•è®¡ç®—ï¼Œç»™ä¸€ä¸ªä¸­æ€§å€¼

        return sma_signal, rsi, boll_position, vol_ratio

    except Exception as e:
        print(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        return 0, 50, 0, 1

def objective(trial, X_train, y_train, X_test, y_test, target_type):
    """
    Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•°
    """
    hidden_layer_sizes = []
    n_layers = trial.suggest_int('n_layers', 1, 3)
    for i in range(n_layers):
        hidden_layer_sizes.append(trial.suggest_int(f'n_units_l{i}', 16, 128))

    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic'])
    solver = trial.suggest_categorical('solver', ['adam', 'sgd'])
    alpha = trial.suggest_loguniform('alpha', 1e-5, 1e-1)
    learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-2)

    # æ ¹æ®ç›®æ ‡ç±»å‹è°ƒæ•´å‚æ•°èŒƒå›´
    if target_type == 'short_term':
        learning_rate_init = trial.suggest_loguniform('learning_rate_init', 1e-3, 0.1)
        n_iter_no_change = 10
    else:  # long_term
        n_iter_no_change = 30

    model = MLPRegressor(
        hidden_layer_sizes=tuple(hidden_layer_sizes),
        activation=activation,
        solver=solver,
        alpha=alpha,
        learning_rate_init=learning_rate_init,
        random_state=42,
        max_iter=500,
        early_stopping=True,
        n_iter_no_change=n_iter_no_change,
        tol=1e-4
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse

def train_neural_network(df, target_type='comprehensive'):
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†ï¼Œä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
    æ”¯æŒä¸‰ç§ç›®æ ‡ç±»å‹ï¼šçŸ­æœŸã€é•¿æœŸã€ç»¼åˆ
    """
    print(f"\n   å‡†å¤‡è®­ç»ƒæ•°æ® ({target_type}æ¨¡å‹)...")
    X = []

    for _, row in df.iterrows():
        features = calculate_features(row)
        X.append(features)

    X = np.array(X)

    # æå–ç”¨äºè®¡ç®—è´¨é‡è¯„åˆ†çš„åˆ—
    change = df['æ¶¨è·Œå¹…'].apply(safe_float) # ä½¿ç”¨æ¶¨è·Œå¹…ä»£æ›¿æ¶¨å¹…%
    profit = pd.Series([0] * len(df)) # æ— æ³•è·å–ï¼Œç”¨0ä»£æ›¿
    turnover = df['æ¢æ‰‹ç‡'].apply(safe_float) # ä½¿ç”¨æ¢æ‰‹ç‡ä»£æ›¿å®é™…æ¢æ‰‹%
    market_cap = df['æ€»å¸‚å€¼'].apply(safe_float)
    pe_ratio = df['å¸‚ç›ˆç‡(åŠ¨)'].apply(safe_float)

    # å½’ä¸€åŒ–å„ä¸ªæŒ‡æ ‡ (ä½¿ç”¨ min-max å½’ä¸€åŒ–)
    scaler = MinMaxScaler()
    change_norm = scaler.fit_transform(change.values.reshape(-1, 1)).flatten()
    profit_norm = scaler.fit_transform(profit.values.reshape(-1, 1)).flatten()
    turnover_norm = scaler.fit_transform(turnover.values.reshape(-1, 1)).flatten()
    market_cap_norm = scaler.fit_transform(market_cap.values.reshape(-1, 1)).flatten()
    pe_ratio_norm = scaler.fit_transform(pe_ratio.values.reshape(-1, 1)).flatten()

    # å¤„ç† NaN å€¼ï¼Œç”¨ 0 å¡«å……
    change_norm = np.nan_to_num(change_norm, nan=0)
    profit_norm = np.nan_to_num(profit_norm, nan=0)
    turnover_norm = np.nan_to_num(turnover_norm, nan=0)
    market_cap_norm = np.nan_to_num(market_cap_norm, nan=0)
    pe_ratio_norm = np.nan_to_num(pe_ratio_norm, nan=0)

    # æ ¹æ®ç›®æ ‡ç±»å‹è®¡ç®—ä¸åŒçš„è¯„åˆ†
    if target_type == 'short_term':
        # çŸ­æœŸè¯„åˆ†: ä¸»è¦å…³æ³¨æŠ€æœ¯é¢å’Œå¸‚åœºæƒ…ç»ª
        y = (0.5 * change_norm +
             0.3 * turnover_norm +
             0.2 * (1 - abs(turnover_norm - 0.5)))  # æ¢æ‰‹ç‡é€‚ä¸­æœ€å¥½

    elif target_type == 'long_term':
        # é•¿æœŸè¯„åˆ†: ä¸»è¦å…³æ³¨åŸºæœ¬é¢å’Œä»·å€¼
        y = (0.4 * profit_norm +
             0.3 * market_cap_norm +
             0.2 * (1 - pe_ratio_norm) +
             0.1 * change_norm)  # å¸‚ç›ˆç‡è¶Šä½è¶Šå¥½

    else:  # comprehensive
        # ç»¼åˆè¯„åˆ†
        y = (0.4 * change_norm +
             0.2 * profit_norm +
             0.15 * (1 - abs(turnover_norm - 0.5)) +
             0.15 * market_cap_norm +
             0.1 * (1 - pe_ratio_norm))

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]

    if len(X) < 20: # è‡³å°‘éœ€è¦ä¸€äº›æ•°æ®æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print(f"   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒ{target_type}ç¥ç»ç½‘ç»œã€‚")
        return None, None

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Optuna è¶…å‚æ•°ä¼˜åŒ–
    print("   å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
    try:
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test, target_type),
                       n_trials=50, show_progress_bar=True)
    except Exception as e:
        print(f"   Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("   å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
        # å¦‚æœOptunaå¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªé»˜è®¤çš„æ¨¡å‹é…ç½®
        best_params = {
            'n_layers': 2,
            'n_units_l0': 64,
            'n_units_l1': 32,
            'activation': 'relu',
            'solver': 'adam',
            'alpha': 0.0001,
            'learning_rate_init': 0.001
        }
    else:
        best_params = study.best_params

    print("   Optuna ä¼˜åŒ–å®Œæˆï¼Œæœ€ä½³å‚æ•°:", best_params)

    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹...")
    hidden_layer_sizes = []
    for i in range(best_params['n_layers']):
        hidden_layer_sizes.append(best_params[f'n_units_l{i}'])

    final_model = MLPRegressor(
        hidden_layer_sizes=tuple(hidden_layer_sizes),
        activation=best_params['activation'],
        solver=best_params['solver'],
        alpha=best_params['alpha'],
        learning_rate_init=best_params['learning_rate_init'],
        random_state=42,
        max_iter=500,
        early_stopping=True,
        n_iter_no_change=10,
        tol=1e-4
    )
    final_model.fit(X_train, y_train)

    # è¯„ä¼°æ¨¡å‹
    y_pred = final_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"   æ¨¡å‹è¯„ä¼°: MSE = {mse:.4f}, R^2 = {r2:.4f}")

    return scaler, final_model

def perform_association_rule_mining(df):
    """
    æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜ï¼Œå‘ç°é¢‘ç¹é¡¹é›†å’Œå…³è”è§„åˆ™ã€‚
    """
    print("\n   å¼€å§‹å…³è”è§„åˆ™æŒ–æ˜...")

    # ç‰¹å¾å·¥ç¨‹ï¼šå°†æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£ç‰¹å¾
    print("   ç‰¹å¾ç¦»æ•£åŒ–...")
    df['æ¶¨è·Œå¹…_ç±»åˆ«'] = pd.cut(df['æ¶¨è·Œå¹…'].apply(safe_float), bins=[-np.inf, -5, 0, 5, 10, np.inf],
                         labels=['æä½', 'ä½', 'ä¸­', 'é«˜', 'æé«˜'])
    df['å½’å±å‡€åˆ©æ¶¦_ç±»åˆ«'] = pd.cut(pd.Series([0] * len(df)), bins=[-np.inf, -10, 0, 10, 50, np.inf],
                             labels=['äºæŸ', 'å¾®åˆ©', 'ä¸€èˆ¬', 'è‰¯å¥½', 'ä¼˜ç§€']) # æ— æ³•è·å–ï¼Œç”¨0ä»£æ›¿
    df['æ¢æ‰‹ç‡_ç±»åˆ«'] = pd.cut(df['æ¢æ‰‹ç‡'].apply(safe_float), bins=[-np.inf, 1, 3, 5, 10, np.inf],
                             labels=['æä½', 'ä½', 'ä¸­', 'é«˜', 'æé«˜'])
    df['æ€»å¸‚å€¼_ç±»åˆ«'] = pd.cut(df['æ€»å¸‚å€¼'].apply(safe_float), bins=[-np.inf, 100, 500, 1000, 5000, np.inf],
                           labels=['å°å‹', 'ä¸­å‹', 'å¤§å‹', 'è¶…å¤§å‹', 'å·¨å‹'])

    # æ·»åŠ è‹æ°ç­–ç•¥è®¡ç®—çš„ç‰¹å¾
    print("   æ·»åŠ è‹æ°ç­–ç•¥ç‰¹å¾...")
    df['è‹æ°ç­–ç•¥ç‰¹å¾'] = df.apply(calculate_features, axis=1)

    # å°†è‹æ°ç­–ç•¥ç‰¹å¾å±•å¼€ä¸ºå•ç‹¬çš„åˆ—
    df[['ä»·æ ¼ä½ç½®', 'æ¶¨å¹…å’Œä»·æ ¼ä½ç½®', 'å½’å±å‡€åˆ©æ¶¦', 'å®é™…æ¢æ‰‹ç‡', 'æ€»å¸‚å€¼']] = df['è‹æ°ç­–ç•¥ç‰¹å¾'].tolist()

    # å°†è‹æ°ç­–ç•¥ç‰¹å¾è½¬æ¢ä¸ºç¦»æ•£ç‰¹å¾
    df['ä»·æ ¼ä½ç½®_ç±»åˆ«'] = df['ä»·æ ¼ä½ç½®'].apply(lambda x: 'æ»¡è¶³' if x == 1 else 'ä¸æ»¡è¶³')
    df['æ¶¨å¹…å’Œä»·æ ¼ä½ç½®_ç±»åˆ«'] = df['æ¶¨å¹…å’Œä»·æ ¼ä½ç½®'].apply(lambda x: 'æ»¡è¶³' if x == 1 else 'ä¸æ»¡è¶³')

    # é€‰æ‹©ç”¨äºå…³è”è§„åˆ™æŒ–æ˜çš„åˆ—
    print("   é€‰æ‹©ç”¨äºå…³è”è§„åˆ™æŒ–æ˜çš„åˆ—...")
    selected_columns = ['è¡Œä¸š', 'åœ°åŒº', 'æ¶¨è·Œå¹…_ç±»åˆ«', 'å½’å±å‡€åˆ©æ¶¦_ç±»åˆ«', 'æ¢æ‰‹ç‡_ç±»åˆ«', 'æ€»å¸‚å€¼_ç±»åˆ«',
                          'ä»·æ ¼ä½ç½®_ç±»åˆ«', 'æ¶¨å¹…å’Œä»·æ ¼ä½ç½®_ç±»åˆ«']
    df_selected = df[selected_columns].copy()

    # å°† DataFrame è½¬æ¢ä¸ºäº‹åŠ¡åˆ—è¡¨
    print("   è½¬æ¢ä¸ºäº‹åŠ¡åˆ—è¡¨...")
    transactions = df_selected.astype(str).values.tolist()

    # ä½¿ç”¨ TransactionEncoder è¿›è¡Œ one-hot ç¼–ç 
    print("   è¿›è¡Œ one-hot ç¼–ç ...")
    te = TransactionEncoder()
    te_result = te.fit(transactions).transform(transactions)
    df_encoded = pd.DataFrame(te_result, columns=te.columns_)

    # ä½¿ç”¨ Apriori ç®—æ³•å‘ç°é¢‘ç¹é¡¹é›†
    print("   ä½¿ç”¨ Apriori ç®—æ³•...")
    try:
        frequent_itemsets = apriori(df_encoded, min_support=0.05, use_colnames=True)
    except ValueError as e:
        print(f"   Apriori ç®—æ³•å‡ºé”™: {e}")
        print("   è¯·æ£€æŸ¥æ•°æ®æˆ–è°ƒæ•´ min_support å‚æ•°ã€‚")
        return

    # ç”Ÿæˆå…³è”è§„åˆ™
    print("   ç”Ÿæˆå…³è”è§„åˆ™...")
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

    # æ‰“å°å…³è”è§„åˆ™
    print("\n   å…³è”è§„åˆ™:")
    if not rules.empty:
        print(rules[['antecedents', 'consequents', 'confidence', 'lift']].head(10))  # æ˜¾ç¤ºå‰10æ¡è§„åˆ™
    else:
        print("   æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å…³è”è§„åˆ™ã€‚")

def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•°æ®è·å–ã€é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒã€é€‰è‚¡å’ŒæŠ•èµ„å»ºè®®ã€‚
    """
    print("="*60)
    print("ğŸš€ å¯åŠ¨åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ (é›†æˆç¥ç»ç½‘ç»œä¸å…³è”è§„åˆ™) ğŸš€")
    print("="*60)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨æ•°æ® ==========
    print("\n   å¼€å§‹è·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨æ•°æ®...")
    stock_list_df = ak.stock_zh_a_spot()
    print(f"   å…±è·å– {len(stock_list_df)} åªè‚¡ç¥¨æ•°æ®")

    # ç­›é€‰æ‰STè‚¡
    stock_list_df = stock_list_df[~stock_list_df['åç§°'].str.contains('ST')]
    stock_list_df = stock_list_df[~stock_list_df['åç§°'].str.contains('é€€')]
    print(f"   å‰”é™¤STè‚¡åå‰©ä½™ {len(stock_list_df)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè·å–è‚¡ç¥¨è¯¦ç»†æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç† ==========
    print("\n   å¼€å§‹è·å–è‚¡ç¥¨å®æ—¶æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†...")
    df_list = []
    error_codes = []
    for i, row in stock_list_df.iterrows():
        code = row['ä»£ç ']
        name = row['åç§°']
        print(f"   [{i+1}/{len(stock_list_df)}] è·å– {name}({code}) å®æ—¶æ•°æ®...", end="")
        try:
            # è·å–è‚¡ç¥¨å®æ—¶æ•°æ®
            stock_realtime_df = ak.stock_zh_a_spot(symbol=code)
            if stock_realtime_df is None or len(stock_realtime_df) == 0:
                print("âŒ è·å–å®æ—¶æ•°æ®å¤±è´¥")
                error_codes.append(code)
                continue

            # åˆå¹¶æ•°æ®
            realtime_data = stock_realtime_df.iloc[0] # è·å–ç¬¬ä¸€è¡Œæ•°æ®
            df = pd.DataFrame([realtime_data])
            df['ä»£ç '] = code
            df['åç§°'] = name
            df['è¡Œä¸š'] = row['è¡Œä¸š']
            df['åœ°åŒº'] = row['åœ°åŒº']
            df_list.append(df)
            print("âœ… å®Œæˆ")
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®å‡ºé”™: {e}")
            error_codes.append(code)

    if error_codes:
        print(f"\nâš ï¸  ä»¥ä¸‹è‚¡ç¥¨è·å–æ•°æ®å‡ºé”™ï¼Œå·²è·³è¿‡: {error_codes}")

    # åˆå¹¶æ‰€æœ‰è‚¡ç¥¨æ•°æ®
    df = pd.concat(df_list, ignore_index=True)

    # å°†æ•°æ®ä¿å­˜åˆ° CSV æ–‡ä»¶
    output_file = os.path.join("è¾“å‡ºæ•°æ®", "Aè‚¡æ•°æ®.csv")
    df.to_csv(output_file, index=False, encoding="utf_8_sig")
    print(f"\nâœ… æ‰€æœ‰Aè‚¡æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")

    # æ•°æ®æ¸…æ´—å’Œè½¬æ¢
    print("\n   æ•°æ®æ¸…æ´—å’Œè½¬æ¢...")
    df['æ¶¨å¹…'] = df['æ¶¨è·Œå¹…'].apply(safe_float)
    df['æ¶¨è·Œå¹…'] = df['æ¶¨è·Œå¹…'].apply(safe_float) # ä¿®æ­£åˆ—å
    df['æ€»å¸‚å€¼'] = df['æ€»å¸‚å€¼'].apply(safe_float)
    df['æµé€šå¸‚å€¼'] = df['æµé€šå¸‚å€¼'].apply(safe_float)
    df['æ¢æ‰‹ç‡'] = df['æ¢æ‰‹ç‡'].apply(safe_float)
    df['å¸‚ç›ˆç‡(åŠ¨)'] = df['å¸‚ç›ˆç‡(åŠ¨)'].apply(safe_float)
    df['å¸‚å‡€ç‡'] = df['å¸‚å‡€ç‡'].apply(safe_float)
    df['æœ€é«˜'] = df['æœ€é«˜'].apply(safe_float)
    df['æœ€ä½'] = df['æœ€ä½'].apply(safe_float)
    df['æœ€æ–°'] = df['æœ€æ–°'].apply(safe_float)
    df['æˆäº¤é‡'] = df['æˆäº¤é‡'].apply(safe_float)

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    df_for_scoring = df.copy() # ç”¨äºè¯„åˆ†
    df = df.dropna(subset=['æ¶¨è·Œå¹…', 'æ€»å¸‚å€¼', 'æ¢æ‰‹ç‡', 'å¸‚ç›ˆç‡(åŠ¨)']) # ä½¿ç”¨æ¶¨è·Œå¹…ä»£æ›¿æ¶¨å¹…%
    df = df[~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)]

    print(f"   æ¸…æ´—åå‰©ä½™ {len(df)} åªè‚¡ç¥¨")

    # ========== ç¬¬ä¸‰æ­¥ï¼šè®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ ==========
    # è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹
    scaler_short, model_short = train_neural_network(df.copy(), target_type='short_term')
    scaler_long, model_long = train_neural_network(df.copy(), target_type='long_term')
    scaler_comprehensive, model_comprehensive = train_neural_network(df.copy(), target_type='comprehensive')

    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦è®­ç»ƒæˆåŠŸ
    if model_short is None or model_long is None or model_comprehensive is None:
        print("\nâš ï¸  ç¥ç»ç½‘ç»œæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œé¢„æµ‹ã€‚")
        return

    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
    print("\n   ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹è¿›è¡Œé¢„æµ‹...")
    X = []
    for _, row in df_for_scoring.iterrows():
        features = calculate_features(row)
        X.append(features)

    X = np.array(X)

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1)
    X = X[mask]
    df_for_scoring = df_for_scoring[mask]

    # é¢„æµ‹
    X_scaled_short = scaler_short.transform(X) if scaler_short else X
    X_scaled_long = scaler_long.transform(X) if scaler_long else X
    X_scaled_comprehensive = scaler_comprehensive.transform(X) if scaler_comprehensive else X

    df_for_scoring['çŸ­æœŸè¯„åˆ†'] = model_short.predict(X_scaled_short) if model_short else 0
    df_for_scoring['é•¿æœŸè¯„åˆ†'] = model_long.predict(X_scaled_long) if model_long else 0
    df_for_scoring['ç»¼åˆè¯„åˆ†'] = model_comprehensive.predict(X_scaled_comprehensive) if model_comprehensive else 0

    # ========== ç¬¬å››æ­¥ï¼šç­›é€‰ä¼˜è´¨è‚¡ç¥¨ ==========
    print("\n   ç­›é€‰ä¼˜è´¨è‚¡ç¥¨...")
    # ç­›é€‰æ¡ä»¶ï¼šç»¼åˆè¯„åˆ†å¤§äºé˜ˆå€¼
    display_count = 20 # ç”¨äºè®¡ç®—é˜ˆå€¼çš„æ•°é‡
    threshold = df_for_scoring['ç»¼åˆè¯„åˆ†'].nlargest(display_count).min()

    # ç­›é€‰ä¼˜è´¨è‚¡ç¥¨
    quality_stocks_filtered = df_for_scoring[df_for_scoring['ç»¼åˆè¯„åˆ†'] >= threshold].sort_values(by='ç»¼åˆè¯„åˆ†', ascending=False)

    # è·å–æŠ€æœ¯æŒ‡æ ‡
    print("\n   è·å–æŠ€æœ¯æŒ‡æ ‡...")
    for i, row in quality_stocks_filtered.iterrows():
        sma, rsi, boll, vol_ratio = calculate_technical_indicators(row)
        quality_stocks_filtered.loc[i, 'SMA'] = sma
        quality_stocks_filtered.loc[i, 'RSI'] = rsi
        quality_stocks_filtered.loc[i, 'BOLL'] = boll
        quality_stocks_filtered.loc[i, 'æˆäº¤é‡æ¯”'] = vol_ratio

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨åˆ°æ–‡ä»¶
    output_file2 = os.path.join("è¾“å‡ºæ•°æ®", "ä¼˜è´¨è‚¡ç¥¨.txt")
    with open(output_file2, "w", encoding="utf_8") as f:
        f.write("="*50 + "\n")
        f.write("é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœ (ç¥ç»ç½‘ç»œè¯„åˆ†)\n")
        f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœ€ä½ç»¼åˆè¯„åˆ†é˜ˆå€¼ (åŸºäºå‰{display_count}åæˆ–å…¨éƒ¨): {threshold:.4f}\n")
        f.write(f"ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {len(quality_stocks_filtered)}\n")
        f.write("="*50 + "\n\n")

        for stock in quality_stocks_filtered.to_dict('records'):
            f.write(f"è‚¡ç¥¨ä»£ç : {stock['ä»£ç ']}\n")
            f.write(f"è‚¡ç¥¨åç§°: {stock['åç§°']}\n")
            f.write(f"æ‰€å±è¡Œä¸š: {stock['è¡Œä¸š']}\n")
            f.write(f"ç»¼åˆè¯„åˆ†: {stock['ç»¼åˆè¯„åˆ†']:.4f}\n")
            f.write(f"çŸ­æœŸè¯„åˆ†: {stock['çŸ­æœŸè¯„åˆ†']:.4f}\n")
            f.write(f"é•¿æœŸè¯„åˆ†: {stock['é•¿æœŸè¯„åˆ†']:.4f}\n")
            f.write(f"ä»Šæ—¥æ¶¨å¹…: {stock['æ¶¨å¹…']:.2f}%\n")
            f.write(f"æ€»å¸‚å€¼: {stock['æ€»å¸‚å€¼']:.2f} äº¿\n")
            f.write(f"æ¢æ‰‹ç‡: {stock['æ¢æ‰‹ç‡']:.2f}%\n")
            f.write(f"å¸‚ç›ˆç‡(åŠ¨): {stock['å¸‚ç›ˆç‡(åŠ¨)']:.2f}\n")
            f.write(f"æŠ€æœ¯æŒ‡æ ‡ - SMAä¿¡å·: {stock['SMA']}, RSI: {stock['RSI']:.1f}, BOLLä½ç½®: {stock['BOLL']}, æˆäº¤é‡æ¯”: {stock['æˆäº¤é‡æ¯”']:.2f}\n")
            f.write("-"*30 + "\n")

        print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨å·²ä¿å­˜: {output_file2}")
    print(f"   æ‰¾åˆ° {len(quality_stocks_filtered)} åªä¼˜è´¨è‚¡ç¥¨ï¼ˆæœ€ä½ç»¼åˆè¯„åˆ†={threshold:.4f}ï¼‰")

    if len(quality_stocks_filtered) > 0:
        print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨ (å‰{len(quality_stocks_filtered)}å)ï¼š")
        print("="*130)
        print(f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'æ¶¨å¹…%':<8} {'ç»¼åˆè¯„åˆ†':<10} {'çŸ­æœŸè¯„åˆ†':<10} {'é•¿æœŸè¯„åˆ†':<10} {'æ€»å¸‚å€¼(äº¿)':<12} {'æ¢æ‰‹ç‡(%)':<10} {'å¸‚ç›ˆç‡(åŠ¨)':<12} {'æ‰€å±è¡Œä¸š':<15}")
        print("-"*130)
        for stock in quality_stocks_filtered.to_dict('records'):
            print(f"{stock['ä»£ç ']:<10} {stock['åç§°']:<12} {stock['æ¶¨å¹…']:<8.2f} {stock['ç»¼åˆè¯„åˆ†']:<10.4f} {stock['çŸ­æœŸè¯„åˆ†']:<10.4f} {stock['é•¿æœŸè¯„åˆ†']:<10.4f} {stock['æ€»å¸‚å€¼']:<12.2f} {stock['æ¢æ‰‹ç‡']:<10.2f} {stock['å¸‚ç›ˆç‡(åŠ¨)':<12.2f} {stock['è¡Œä¸š']:<15}")

        # ========== ç¬¬äº”æ­¥ï¼šç»“åˆåˆ†æç»™å‡ºæŠ•èµ„å»ºè®® ==========
        print("\n   æŠ•èµ„å»ºè®® (åŸºäºæ¨¡å‹è¯„åˆ†ã€æŠ€æœ¯æŒ‡æ ‡å’ŒåŸºæœ¬é¢):")
        for stock in quality_stocks_filtered.to_dict('records'):
            code = stock['ä»£ç ']
            name = stock['åç§°']
            comprehensive_score = stock['ç»¼åˆè¯„åˆ†']
            short_term_score = stock['çŸ­æœŸè¯„åˆ†']
            long_term_score = stock['é•¿æœŸè¯„åˆ†']
            change_percent = stock['æ¶¨å¹…']
            market_cap = stock['æ€»å¸‚å€¼']
            turnover_rate = stock['æ¢æ‰‹ç‡']
            pe_ratio = stock['å¸‚ç›ˆç‡(åŠ¨)']
            industry = stock['è¡Œä¸š']
            sma = stock['SMA']
            rsi = stock['RSI']
            boll = stock['BOLL']
            vol_ratio = stock['æˆäº¤é‡æ¯”']

            # 1. åŸºæœ¬é¢åˆ†æ
            profitability = "ä¼˜ç§€" if pe_ratio > 0 and pe_ratio < 15 else "è‰¯å¥½" if pe_ratio < 30 else "ä¸€èˆ¬"
            growth_potential = "é«˜" if market_cap < 500 and turnover_rate > 5 else "ä¸­" if market_cap < 1000 else "ä½"
            debt_level = "å¥åº·" if market_cap > 100 else "ä¸€èˆ¬"  # ç®€åŒ–è¯„ä¼°

            # 2. æŠ€æœ¯é¢åˆ†æ
            sma_signal = "é‡‘å‰" if sma == 1 else "æ­»å‰" if sma == -1 else "ä¸­æ€§"
            rsi_signal = "è¶…ä¹°" if rsi > 70 else "è¶…å–" if rsi < 30 else "ä¸­æ€§"
            boll_signal = "ä¸Šè½¨" if boll == 1 else "ä¸‹è½¨" if boll == -1 else "ä¸­è½¨"
            volume_signal = "æ”¾é‡" if vol_ratio > 1.2 else "ç¼©é‡" if vol_ratio < 0.8 else "å¹³é‡"

            # 3. ç»¼åˆåˆ¤æ–­å’Œå»ºè®®
            print(f"\n   è‚¡ç¥¨ä»£ç : {code} ({name})")
            print(f"     æ‰€å±è¡Œä¸š: {industry}")
            print(f"     ç»¼åˆè¯„åˆ†: {comprehensive_score:.4f} | çŸ­æœŸè¯„åˆ†: {short_term_score:.4f} | é•¿æœŸè¯„åˆ†: {long_term_score:.4f}")
            print(f"     åŸºæœ¬é¢: ç›ˆåˆ©èƒ½åŠ›-{profitability}, æˆé•¿æ½œåŠ›-{growth_potential}, è´Ÿå€ºæ°´å¹³-{debt_level}")
            print(f"     æŠ€æœ¯é¢: SMA-{sma_signal}, RSI-{rsi_signal}({rsi:.1f}), BOLL-{boll_signal}, æˆäº¤é‡-{volume_signal}({vol_ratio:.2f})")

            # æŠ•èµ„å»ºè®® - æ ¹æ®è¯„åˆ†å’ŒæŠ€æœ¯æŒ‡æ ‡
            # çŸ­æœŸç­–ç•¥ (1-5ä¸ªäº¤æ˜“æ—¥)
            short_term_recommendation = ""
            if short_term_score > 0.7:
                if sma == 1 and rsi < 70 and boll != 1:
                    short_term_recommendation = "å¼ºçƒˆä¹°å…¥"
                elif sma == 1 or rsi < 30:
                    short_term_recommendation = "ä¹°å…¥"
                else:
                    short_term_recommendation = "è°¨æ…ä¹°å…¥"
            elif short_term_score > 0.5:
                short_term_recommendation = "è§‚æœ›"
            else:
                short_term_recommendation = "å›é¿"

            # é•¿æœŸç­–ç•¥ (1-6ä¸ªæœˆ)
            long_term_recommendation = ""
            if long_term_score > 0.7:
                if pe_ratio < 30 and market_cap > 50:
                    long_term_recommendation = "å¼ºçƒˆä¹°å…¥"
                elif pe_ratio < 50:
                    long_term_recommendation = "ä¹°å…¥"
                else:
                    long_term_recommendation = "è°¨æ…ä¹°å…¥"
            elif long_term_score > 0.5:
                long_term_recommendation = "è§‚æœ›"
            else:
                long_term_recommendation = "å›é¿"

            print(f"     çŸ­æœŸç­–ç•¥(1-5å¤©): {short_term_recommendation}")
            print(f"     é•¿æœŸç­–ç•¥(1-6æœˆ): {long_term_recommendation}")

            # å…·ä½“æ“ä½œå»ºè®®
            print(f"     å…·ä½“æ“ä½œ:")
            if short_term_recommendation in ["å¼ºçƒˆä¹°å…¥", "ä¹°å…¥"]:
                print(f"       - çŸ­æœŸ: å¯åœ¨å½“å‰ä»·ä½ä¹°å…¥ï¼Œç›®æ ‡æ¶¨å¹…5-8%ï¼Œæ­¢æŸè®¾åœ¨-3%")
                if rsi > 70:
                    print(f"       - æ³¨æ„: RSI({rsi:.1f})å·²è¿›å…¥è¶…ä¹°åŒºï¼Œå¯ç­‰å¾…å›è°ƒä»‹å…¥")
            elif short_term_recommendation == "è°¨æ…ä¹°å…¥":
                print(f"       - çŸ­æœŸ: å¯è½»ä»“å‚ä¸ï¼Œä¸¥æ ¼è®¾ç½®æ­¢æŸ-3%ï¼Œå¿«è¿›å¿«å‡º")

            if long_term_recommendation in ["å¼ºçƒˆä¹°å…¥", "ä¹°å…¥"]:
                print(f"       - é•¿æœŸ: å¯åˆ†æ‰¹å»ºä»“ï¼Œå…³æ³¨å­£åº¦è´¢æŠ¥ï¼Œç›®æ ‡æŒæœ‰3-6ä¸ªæœˆ")
                if pe_ratio > 30:
                    print(f"       - æ³¨æ„: å¸‚ç›ˆç‡({pe_ratio:.1f})åé«˜ï¼Œç­‰å¾…å›è°ƒè‡³åˆç†åŒºé—´")
            elif long_term_recommendation == "è°¨æ…ä¹°å…¥":
                print(f"       - é•¿æœŸ: å¯å°ä»“ä½å¸ƒå±€ï¼Œå…³æ³¨è¡Œä¸šæ”¿ç­–å’ŒåŸºæœ¬é¢å˜åŒ–")

            if short_term_recommendation == "å›é¿" and long_term_recommendation == "å›é¿":
                print("       - æš‚æ— åˆé€‚æ“ä½œç­–ç•¥ï¼Œå»ºè®®è§‚æœ›")

            print("-" * 70)

    else:
        print("\nâš ï¸ ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å¸‚åœºæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œæ¶¨å¹…ä¸è¶³")
        print("   2. æ•°æ®è·å–ä¸å®Œæ•´æˆ–è´¨é‡ä¸ä½³")
        print("   3. ç¥ç»ç½‘ç»œæ¨¡å‹éœ€è¦æ›´å¤šæ•°æ®æˆ–ä¼˜åŒ–")

    # ========== ç¬¬å››æ­¥ï¼šå…³è”è§„åˆ™æŒ–æ˜ ==========
    # åœ¨è¿™é‡Œè°ƒç”¨å…³è”è§„åˆ™æŒ–æ˜å‡½æ•°
    perform_association_rule_mining(df_for_scoring.copy()) # ä¼ å…¥åŸå§‹æ•°å€¼çš„dfå‰¯æœ¬

    print("\n" + "="*60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()

