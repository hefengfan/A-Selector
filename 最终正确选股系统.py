#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é«˜çº§é‡åŒ–é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆå¤šç»´åº¦æ•°æ®ã€ç²¾å¯†MLæ¨¡å‹ä¸ç­–ç•¥å›æµ‹)

æ ¸å¿ƒæ”¹è¿›ç‚¹ï¼š
1.  **æ•°æ®è·å–ä¸æ•´åˆï¼š** è·å–å†å²æ—¥Kçº¿æ•°æ®å’Œè´¢åŠ¡æŒ‡æ ‡æ•°æ®ã€‚
2.  **å¤šç»´åº¦ç‰¹å¾å·¥ç¨‹ï¼š** æ•´åˆæŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢æŒ‡æ ‡å’Œè‹æ°é‡åŒ–ç­–ç•¥ç‰¹å¾ã€‚
3.  **æ›´ç²¾å¯†çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼š**
    *   ç›®æ ‡å˜é‡ï¼šæœªæ¥ N æ—¥çš„æœ€é«˜æ”¶ç›Šç‡ã€‚
    *   æ¨¡å‹ï¼šXGBoostRegressorã€‚
    *   ä¼˜åŒ–ï¼šOptuna è¶…å‚æ•°ä¼˜åŒ–ã€‚
    *   éªŒè¯ï¼šTimeSeriesSplit æ—¶é—´åºåˆ—äº¤å‰éªŒè¯ï¼Œé¿å…æœªæ¥æ•°æ®æ³„éœ²ã€‚
4.  **ç­–ç•¥ç”Ÿæˆä¸ä¿¡å·ï¼š** æä¾›çŸ­æœŸ/é•¿æœŸä¹°å…¥/å–å‡º/æŒæœ‰ä¿¡å·ï¼Œå¹¶å»ºè®®æ­¢æŸæ­¢ç›ˆä½ã€‚
5.  **ç®€åŒ–çš„å›æµ‹ç³»ç»Ÿï¼š** æ¨¡æ‹Ÿå†å²äº¤æ˜“ï¼Œè¯„ä¼°ç­–ç•¥è¡¨ç°ã€‚
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import warnings
import time # ç”¨äºAPIè°ƒç”¨é—´éš”
warnings.filterwarnings('ignore')

# å¯¼å…¥æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import optuna

# å¯¼å…¥æŠ€æœ¯åˆ†æåº“
import ta

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

# ==================== é…ç½®å‚æ•° ====================
# å†å²æ•°æ®è·å–èŒƒå›´ (ç”¨äºæ¨¡å‹è®­ç»ƒå’ŒæŒ‡æ ‡è®¡ç®—)
HISTORY_YEARS = 3 # è·å–è¿‡å»3å¹´çš„æ•°æ®
# é¢„æµ‹æœªæ¥æ”¶ç›Šçš„å¤©æ•° (çŸ­æœŸ/é•¿æœŸ)
SHORT_TERM_FUTURE_DAYS = 5  # é¢„æµ‹æœªæ¥5ä¸ªäº¤æ˜“æ—¥å†…çš„æœ€é«˜æ¶¨å¹…
LONG_TERM_FUTURE_DAYS = 20  # é¢„æµ‹æœªæ¥20ä¸ªäº¤æ˜“æ—¥å†…çš„æœ€é«˜æ¶¨å¹…

# ä¿¡å·ç”Ÿæˆé˜ˆå€¼ (é¢„æµ‹çš„æœªæ¥æœ€é«˜æ”¶ç›Šç‡)
BUY_THRESHOLD_SHORT = 0.03 # çŸ­æœŸé¢„æµ‹æœ€é«˜æ¶¨å¹…é«˜äºæ­¤å€¼åˆ™ä¹°å…¥ (3%)
BUY_THRESHOLD_LONG = 0.08  # é•¿æœŸé¢„æµ‹æœ€é«˜æ¶¨å¹…é«˜äºæ­¤å€¼åˆ™ä¹°å…¥ (8%)
SELL_THRESHOLD_SHORT = -0.02 # çŸ­æœŸé¢„æµ‹æœ€é«˜è·Œå¹…ä½äºæ­¤å€¼åˆ™å–å‡º (-2%)
SELL_THRESHOLD_LONG = -0.05  # é•¿æœŸé¢„æµ‹æœ€é«˜è·Œå¹…ä½äºæ­¤å€¼åˆ™å–å‡º (-5%)

# æ­¢æŸæ­¢ç›ˆç™¾åˆ†æ¯” (åŸºäºä¹°å…¥ä»·)
STOP_LOSS_PERCENT = 0.05 # 5% æ­¢æŸ
TAKE_PROFIT_PERCENT = 0.10 # 10% æ­¢ç›ˆ

# å…³è”è§„åˆ™æŒ–æ˜å‚æ•°
AR_MIN_SUPPORT = 0.005 # é™ä½æ”¯æŒåº¦ä»¥å‘ç°æ›´å¤šè§„åˆ™
AR_MIN_THRESHOLD = 1.2 # æé«˜æå‡åº¦ä»¥å‘ç°æ›´å¼ºçš„å…³è”

# ==================== è¾…åŠ©å‡½æ•° ====================

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 100000000 # 1äº¿ = 10000ä¸‡
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 1000000000000 # 1ä¸‡äº¿ = 10000äº¿
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) * 10000 # 1ä¸‡
        return float(s)
    except ValueError:
        return np.nan

def get_stock_hist_data(symbol, start_date, end_date):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„å†å²æ—¥Kçº¿æ•°æ® (å‰å¤æƒ)ã€‚
    """
    try:
        df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        df.columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df.set_index('æ—¥æœŸ', inplace=True)
        for col in ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df['è‚¡ç¥¨ä»£ç '] = symbol # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
        return df
    except Exception as e:
        # print(f"è·å– {symbol} å†å²æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame()

def get_financial_data(symbol):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°è´¢åŠ¡åˆ†ææŒ‡æ ‡æ•°æ®ã€‚
    """
    try:
        df = ak.stock_financial_analysis_indicator_em(symbol=symbol)
        # é€‰å–æœ€æ–°ä¸€æœŸçš„è´¢åŠ¡æ•°æ®
        if not df.empty:
            df['æŠ¥å‘Šæ—¥æœŸ'] = pd.to_datetime(df['æŠ¥å‘Šæ—¥æœŸ'])
            df = df.sort_values(by='æŠ¥å‘Šæ—¥æœŸ', ascending=False).iloc[0:1] # å–æœ€æ–°ä¸€æœŸ
            df.rename(columns={
                'å¸‚ç›ˆç‡(TTM)': 'PE_TTM', 'å¸‚å‡€ç‡': 'PB', 'å‡€èµ„äº§æ”¶ç›Šç‡': 'ROE',
                'è¥ä¸šæ€»æ”¶å…¥åŒæ¯”å¢é•¿': 'Revenue_Growth', 'å½’å±å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿': 'NetProfit_Growth'
            }, inplace=True)
            # ç¡®ä¿è¿™äº›åˆ—å­˜åœ¨ä¸”ä¸ºæ•°å€¼
            for col in ['PE_TTM', 'PB', 'ROE', 'Revenue_Growth', 'NetProfit_Growth']:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            return df[['PE_TTM', 'PB', 'ROE', 'Revenue_Growth', 'NetProfit_Growth']].iloc[0]
        return pd.Series()
    except Exception as e:
        # print(f"è·å– {symbol} è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
        return pd.Series()

def calculate_technical_indicators(df):
    """
    è®¡ç®—å¤šç§æŠ€æœ¯æŒ‡æ ‡ã€‚
    è¾“å…¥: åŒ…å« 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡' çš„DataFrame
    è¾“å‡º: åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„DataFrame
    """
    if df.empty:
        return df

    # ç§»åŠ¨å¹³å‡çº¿
    df['SMA_5'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=5)
    df['SMA_10'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=10)
    df['SMA_20'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=20)
    df['SMA_60'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=60) # ç”¨äºè‹æ°ç­–ç•¥
    df['EMA_5'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=5)
    df['EMA_10'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=10)
    df['EMA_20'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=20)

    # MACD
    macd = ta.trend.MACD(df['æ”¶ç›˜'])
    df['MACD'] = macd.macd()
    df['MACD_Signal'] = macd.macd_signal()
    df['MACD_Diff'] = macd.macd_diff()

    # RSI
    df['RSI'] = ta.momentum.rsi(df['æ”¶ç›˜'], window=14)

    # KDJ (Stochastic Oscillator)
    stoch = ta.momentum.StochasticOscillator(df['æœ€é«˜'], df['æœ€ä½'], df['æ”¶ç›˜'])
    df['K'] = stoch.stoch()
    df['D'] = stoch.stoch_signal()
    df['J'] = 3 * df['K'] - 2 * df['D']

    # Bollinger Bands
    bollinger = ta.volatility.BollingerBands(df['æ”¶ç›˜'])
    df['BB_Upper'] = bollinger.bollinger_hband()
    df['BB_Lower'] = bollinger.bollinger_lband()
    df['BB_Width'] = bollinger.bollinger_wband()
    df['BB_Percent'] = bollinger.bollinger_rband() # %BæŒ‡æ ‡

    # ATR (Average True Range)
    df['ATR'] = ta.volatility.average_true_range(df['æœ€é«˜'], df['æœ€ä½'], df['æ”¶ç›˜'], window=14)

    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
    df['Volume_SMA_5'] = ta.volume.volume_sma_indicator(df['æˆäº¤é‡'], window=5)
    df['Volume_SMA_10'] = ta.volume.volume_sma_indicator(df['æˆäº¤é‡'], window=10)

    # ä»·æ ¼å˜åŠ¨ç‡
    df['Daily_Return'] = df['æ”¶ç›˜'].pct_change()
    df['Log_Return'] = np.log(df['æ”¶ç›˜'] / df['æ”¶ç›˜'].shift(1))

    # è‹æ°ç­–ç•¥ç‰¹å¾ (åŸºäºè®¡ç®—å‡ºçš„å‡çº¿)
    df['Price_Pos_F'] = 0
    # ç¡®ä¿SMA_60å’ŒSMA_20ä¸ä¸º0æˆ–NaN
    df.loc[(df['SMA_60'] > 0) & (df['æœ€ä½'] / df['SMA_60'] >= 0.85) & (df['æœ€ä½'] / df['SMA_60'] <= 1.15), 'Price_Pos_F'] = 1
    df.loc[(df['SMA_20'] > 0) & (df['æ”¶ç›˜'] / df['SMA_20'] >= 0.90) & (df['æ”¶ç›˜'] / df['SMA_20'] <= 1.10), 'Price_Pos_F'] = 1

    df['Price_Pos_G'] = 0
    # ç¡®ä¿æœ€é«˜å’Œæœ€ä½ä»·æœ‰æ•ˆï¼Œé¿å…é™¤ä»¥é›¶
    df.loc[(df['æ¶¨è·Œå¹…'] >= 5.0) & (df['æœ€é«˜'] - df['æœ€ä½'] > 0) & (df['æ”¶ç›˜'] >= (df['æœ€é«˜'] - (df['æœ€é«˜'] - df['æœ€ä½']) * 0.30)), 'Price_Pos_G'] = 1
    df.loc[(df['æ¶¨è·Œå¹…'] >= 5.0) & (df['æœ€é«˜'] == df['æœ€ä½']) & (df['æ”¶ç›˜'] == df['æœ€é«˜']), 'Price_Pos_G'] = 1 # æ¶¨åœæ¿æƒ…å†µ

    # å¡«å……è®¡ç®—åå¯èƒ½å‡ºç°çš„NaNå€¼ï¼Œé€šå¸¸ç”¨0æˆ–å‰ä¸€ä¸ªæœ‰æ•ˆå€¼å¡«å……
    df = df.fillna(method='ffill').fillna(0) # å…ˆå‘å‰å¡«å……ï¼Œå†ç”¨0å¡«å……å‰©ä½™çš„ï¼ˆé€šå¸¸æ˜¯å¼€å¤´å‡ è¡Œï¼‰

    return df

def prepare_data_for_ml(df_all_stocks, future_days):
    """
    å‡†å¤‡ç”¨äºæœºå™¨å­¦ä¹ çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç‰¹å¾å’Œç›®æ ‡å˜é‡ã€‚
    ç›®æ ‡å˜é‡ä¸ºæœªæ¥ N æ—¥å†…çš„æœ€é«˜æ”¶ç›Šç‡ã€‚
    """
    features_list = []
    targets_list = []
    stock_codes_list = []
    dates_list = []

    # å®šä¹‰æ‰€æœ‰å¯èƒ½ç”¨åˆ°çš„ç‰¹å¾åˆ—
    feature_cols = [
        'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
        'SMA_5', 'SMA_10', 'SMA_20', 'SMA_60', 'EMA_5', 'EMA_10', 'EMA_20',
        'MACD', 'MACD_Signal', 'MACD_Diff', 'RSI', 'K', 'D', 'J',
        'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Percent', 'ATR',
        'Volume_SMA_5', 'Volume_SMA_10', 'Daily_Return', 'Log_Return',
        'Price_Pos_F', 'Price_Pos_G',
        'PE_TTM', 'PB', 'ROE', 'Revenue_Growth', 'NetProfit_Growth' # è´¢åŠ¡æŒ‡æ ‡
    ]

    # éå†æ¯åªè‚¡ç¥¨ï¼Œè®¡ç®—å…¶ç‰¹å¾å’Œæœªæ¥æ”¶ç›Š
    for stock_code, stock_df in df_all_stocks.groupby('è‚¡ç¥¨ä»£ç '):
        stock_df = stock_df.sort_index() # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº

        # è®¡ç®—æœªæ¥ N æ—¥å†…çš„æœ€é«˜æ”¶ç›Šç‡ä½œä¸ºç›®æ ‡å˜é‡
        # é¿å…æœªæ¥æ•°æ®æ³„éœ²ï¼šä½¿ç”¨ shift(-future_days) è·å–æœªæ¥çš„æ”¶ç›˜ä»·
        # ç„¶åè®¡ç®—æœªæ¥ N æ—¥å†…çš„æœ€é«˜ä»·ä¸å½“å‰æ”¶ç›˜ä»·çš„æ¶¨å¹…
        stock_df['future_max_price'] = stock_df['æœ€é«˜'].rolling(window=future_days, closed='right').max().shift(-future_days + 1)
        stock_df['future_max_return'] = (stock_df['future_max_price'] / stock_df['æ”¶ç›˜']) - 1

        # æå–ç‰¹å¾å’Œç›®æ ‡
        for i in range(len(stock_df)):
            row = stock_df.iloc[i]
            # ç¡®ä¿ç›®æ ‡å˜é‡å­˜åœ¨ä¸”ä¸æ˜¯NaN
            if pd.notna(row['future_max_return']):
                current_features = []
                for col in feature_cols:
                    current_features.append(row.get(col, np.nan)) # ä½¿ç”¨.get()é¿å…KeyError

                # è¿‡æ»¤æ‰ç‰¹å¾ä¸­åŒ…å«NaNæˆ–Infçš„è¡Œ
                if not any(pd.isna(f) or np.isinf(f) for f in current_features):
                    features_list.append(current_features)
                    targets_list.append(row['future_max_return'])
                    stock_codes_list.append(stock_code)
                    dates_list.append(row.name) # è®°å½•æ—¥æœŸ

    X = np.array(features_list)
    y = np.array(targets_list)

    # å†æ¬¡æ£€æŸ¥å¹¶ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ (åŒé‡ä¿é™©)
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]
    stock_codes_filtered = np.array(stock_codes_list)[mask]
    dates_filtered = np.array(dates_list)[mask]

    return X, y, stock_codes_filtered, dates_filtered, feature_cols

def objective_xgb(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ– XGBoostRegressor çš„ç›®æ ‡å‡½æ•°
    """
    param = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),
        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-2, 1e3),
        'seed': 42,
        'n_jobs': -1,
        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 0.1),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
    }

    if param['booster'] == 'gbtree':
        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)
        param['max_depth'] = trial.suggest_int('max_depth', 3, 9)
    elif param['booster'] == 'dart':
        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])
        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])
        param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)
        param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)

    model = xgb.XGBRegressor(**param)
    model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              early_stopping_rounds=50, # æå‰åœæ­¢
              verbose=False)

    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    return rmse

def train_ml_model(df_all_stocks, future_days):
    """
    è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨æœªæ¥æœ€é«˜æ”¶ç›Šç‡ã€‚
    ä½¿ç”¨ TimeSeriesSplit è¿›è¡Œäº¤å‰éªŒè¯ã€‚
    """
    print(f"\n   å‡†å¤‡è®­ç»ƒæ•°æ® (é¢„æµ‹æœªæ¥ {future_days} å¤©æœ€é«˜æ”¶ç›Šç‡)...")
    X, y, stock_codes, dates, feature_names = prepare_data_for_ml(df_all_stocks.copy(), future_days)

    if len(X) < 100: # è‡³å°‘éœ€è¦è¶³å¤Ÿçš„æ•°æ®æ¥è®­ç»ƒå’ŒéªŒè¯
        print(f"   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ ({len(X)} æ¡)ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚")
        return None, None, None

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # TimeSeriesSplit åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # n_splits å†³å®šäº†æœ‰å¤šå°‘ä¸ªè®­ç»ƒ/æµ‹è¯•å¯¹ï¼Œmax_train_size é™åˆ¶äº†è®­ç»ƒé›†çš„å¤§å°
    # test_size å†³å®šäº†æ¯ä¸ªæµ‹è¯•é›†çš„å¤§å°
    tscv = TimeSeriesSplit(n_splits=5) # 5æŠ˜æ—¶é—´åºåˆ—äº¤å‰éªŒè¯

    best_model = None
    best_rmse = float('inf')
    
    print("   å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    # Optuna ä¼˜åŒ–åœ¨æ¯æ¬¡äº¤å‰éªŒè¯çš„ç¬¬ä¸€ä¸ªæŠ˜å ä¸Šè¿›è¡Œï¼Œä»¥æ‰¾åˆ°æœ€ä½³å‚æ•°
    # å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å…ˆç”¨ä¸€éƒ¨åˆ†æ•°æ®æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼Œå†ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒ
    
    # ä»…åœ¨ç¬¬ä¸€ä¸ªæŠ˜å ä¸Šè¿›è¡Œ Optuna ä¼˜åŒ–ï¼Œä»¥èŠ‚çœæ—¶é—´
    for fold, (train_index, test_index) in enumerate(tscv.split(X_scaled)):
        if fold == 0: # åªåœ¨ç¬¬ä¸€ä¸ªæŠ˜å è¿›è¡Œè¶…å‚æ•°æœç´¢
            X_train, X_test = X_scaled[train_index], X_scaled[test_index]
            y_train, y_test = y[train_index], y[test_index]

            study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
            try:
                study.optimize(lambda trial: objective_xgb(trial, X_train, y_train, X_test, y_test), n_trials=30, show_progress_bar=True) # å‡å°‘è¯•ç”¨æ¬¡æ•°ä»¥åŠ å¿«é€Ÿåº¦
            except Exception as e:
                print(f"   Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print("   å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
                best_params = {
                    'objective': 'reg:squareerror', 'eval_metric': 'rmse', 'booster': 'gbtree',
                    'lambda': 1, 'alpha': 0, 'subsample': 0.8, 'colsample_bytree': 0.8,
                    'min_child_weight': 1, 'learning_rate': 0.05, 'n_estimators': 500,
                    'gamma': 0, 'max_depth': 6, 'seed': 42, 'n_jobs': -1,
                }
            else:
                print("\n   Optuna ä¼˜åŒ–å®Œæˆã€‚")
                print(f"   æœ€ä½³å‡æ–¹è¯¯å·® (RMSE): {study.best_value:.4f}")
                print(f"   æœ€ä½³è¶…å‚æ•°: {study.best_params}")
                best_params = study.best_params
            break # æ‰¾åˆ°æœ€ä½³å‚æ•°åé€€å‡ºå¾ªç¯

    # ä½¿ç”¨æœ€ä½³å‚æ•°åœ¨æ‰€æœ‰æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆ XGBoost æ¨¡å‹...")
    model = xgb.XGBRegressor(**best_params)
    model.fit(X_scaled, y) # ä½¿ç”¨å…¨éƒ¨æ•°æ®è¿›è¡Œæœ€ç»ˆè®­ç»ƒ

    # è¯„ä¼°æ¨¡å‹ (åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œä¸€æ¬¡æœ€ç»ˆè¯„ä¼°)
    y_pred = model.predict(X_scaled)
    mse = mean_squared_error(y, y_pred)
    r2 = r2_score(y, y_pred)
    print(f"   æœ€ç»ˆæ¨¡å‹å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   æœ€ç»ˆæ¨¡å‹RÂ²åˆ†æ•°: {r2:.4f}")

    return model, scaler, feature_names

def predict_future_return(row, model, scaler, feature_names):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹è‚¡ç¥¨æœªæ¥æœ€é«˜æ”¶ç›Šç‡ã€‚
    """
    current_features = []
    for col in feature_names:
        current_features.append(safe_float(row.get(col, np.nan))) # ä½¿ç”¨.get()é¿å…KeyError

    if any(pd.isna(f) or np.isinf(f) for f in current_features):
        return np.nan

    features_array = np.array(current_features).reshape(1, -1)
    try:
        features_scaled = scaler.transform(features_array)
        predicted_return = model.predict(features_scaled)[0]
        return predicted_return
    except Exception as e:
        # print(f"é¢„æµ‹æœªæ¥æ”¶ç›Šæ—¶å‘ç”Ÿé”™è¯¯: {e}, ç‰¹å¾: {current_features}")
        return np.nan

def generate_trading_signals(df_current_data, short_term_model, short_term_scaler, short_term_features,
                             long_term_model, long_term_scaler, long_term_features):
    """
    æ ¹æ®æ¨¡å‹é¢„æµ‹ç”Ÿæˆä¹°å…¥/å–å‡ºä¿¡å·ï¼Œå¹¶å»ºè®®æ­¢æŸæ­¢ç›ˆä½ã€‚
    """
    print("\n3. ç”Ÿæˆäº¤æ˜“ä¿¡å·...")
    signals = []

    # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯floatç±»å‹
    for col in ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']:
        if col in df_current_data.columns:
            df_current_data[col] = df_current_data[col].apply(safe_float)

    for idx, row in df_current_data.iterrows():
        stock_code = str(row['è‚¡ç¥¨ä»£ç ']).strip()
        stock_name = str(row['åç§°']).strip()
        industry = str(row['æ‰€å±è¡Œä¸š']).strip()
        current_price = safe_float(row['æœ€æ–°'])
        current_change = safe_float(row['æ¶¨å¹…%'])

        short_pred_return = np.nan
        long_pred_return = np.nan
        signal_short = "æŒæœ‰"
        signal_long = "æŒæœ‰"
        overall_signal = "æŒæœ‰"
        stop_loss_price = np.nan
        take_profit_price = np.nan

        # é¢„æµ‹çŸ­æœŸæ”¶ç›Š
        if short_term_model and short_term_scaler and short_term_features:
            short_pred_return = predict_future_return(row, short_term_model, short_term_scaler, short_term_features)
            if pd.notna(short_pred_return):
                if short_pred_return >= BUY_THRESHOLD_SHORT:
                    signal_short = "çŸ­æœŸä¹°å…¥"
                elif short_pred_return < SELL_THRESHOLD_SHORT:
                    signal_short = "çŸ­æœŸå–å‡º"

        # é¢„æµ‹é•¿æœŸæ”¶ç›Š
        if long_term_model and long_term_scaler and long_term_features:
            long_pred_return = predict_future_return(row, long_term_model, long_term_scaler, long_term_features)
            if pd.notna(long_pred_return):
                if long_pred_return >= BUY_THRESHOLD_LONG:
                    signal_long = "é•¿æœŸä¹°å…¥"
                elif long_pred_return < SELL_THRESHOLD_LONG:
                    signal_long = "é•¿æœŸå–å‡º"

        # ç»¼åˆä¿¡å·åˆ¤æ–­
        if signal_short == "çŸ­æœŸä¹°å…¥" and signal_long == "é•¿æœŸä¹°å…¥":
            overall_signal = "å¼ºçƒˆä¹°å…¥"
        elif signal_short == "çŸ­æœŸä¹°å…¥" and signal_long == "æŒæœ‰":
            overall_signal = "çŸ­æœŸä¹°å…¥"
        elif signal_long == "é•¿æœŸä¹°å…¥" and signal_short == "æŒæœ‰":
            overall_signal = "é•¿æœŸä¹°å…¥"
        elif signal_short == "çŸ­æœŸå–å‡º" or signal_long == "é•¿æœŸå–å‡º":
            overall_signal = "å–å‡º"

        # è®¡ç®—æ­¢æŸæ­¢ç›ˆä»· (ä»…å¯¹ä¹°å…¥ä¿¡å·å»ºè®®)
        if overall_signal in ["å¼ºçƒˆä¹°å…¥", "çŸ­æœŸä¹°å…¥", "é•¿æœŸä¹°å…¥"] and pd.notna(current_price) and current_price > 0:
            stop_loss_price = current_price * (1 - STOP_LOSS_PERCENT)
            take_profit_price = current_price * (1 + TAKE_PROFIT_PERCENT)

        signals.append({
            'ä»£ç ': stock_code,
            'åç§°': stock_name,
            'è¡Œä¸š': industry,
            'æœ€æ–°ä»·': f"{current_price:.2f}" if pd.notna(current_price) else "--",
            'ä»Šæ—¥æ¶¨å¹…': f"{current_change:.2f}%" if pd.notna(current_change) else "--",
            f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š': f"{short_pred_return*100:.2f}%" if pd.notna(short_pred_return) else "--",
            f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š': f"{long_pred_return*100:.2f}%" if pd.notna(long_pred_return) else "--",
            'çŸ­æœŸä¿¡å·': signal_short,
            'é•¿æœŸä¿¡å·': signal_long,
            'ç»¼åˆä¿¡å·': overall_signal,
            'å»ºè®®æ­¢æŸä»·': f"{stop_loss_price:.2f}" if pd.notna(stop_loss_price) else "--",
            'å»ºè®®æ­¢ç›ˆä»·': f"{take_profit_price:.2f}" if pd.notna(take_profit_price) else "--"
        })
    
    signals_df = pd.DataFrame(signals)
    # æ’åºï¼Œä¼˜å…ˆæ˜¾ç¤ºå¼ºçƒˆä¹°å…¥ï¼Œç„¶åæ˜¯é•¿æœŸä¹°å…¥ï¼ŒçŸ­æœŸä¹°å…¥ï¼Œå†æŒ‰é¢„æµ‹æ”¶ç›Šæ’åº
    signal_order = {"å¼ºçƒˆä¹°å…¥": 4, "é•¿æœŸä¹°å…¥": 3, "çŸ­æœŸä¹°å…¥": 2, "æŒæœ‰": 1, "å–å‡º": 0}
    signals_df['signal_rank'] = signals_df['ç»¼åˆä¿¡å·'].map(signal_order)
    
    # å°†é¢„æµ‹æ”¶ç›Šè½¬æ¢ä¸ºæ•°å€¼ä»¥ä¾¿æ’åº
    signals_df[f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num'] = signals_df[f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š'].str.replace('%', '').apply(safe_float) / 100
    signals_df[f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num'] = signals_df[f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š'].str.replace('%', '').apply(safe_float) / 100

    signals_df = signals_df.sort_values(by=['signal_rank', f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num', f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num'], ascending=[False, False, False])
    
    # åˆ é™¤è¾…åŠ©åˆ—
    signals_df = signals_df.drop(columns=['signal_rank', f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num', f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æœ€é«˜æ”¶ç›Š_num'])

    return signals_df

def perform_association_rule_mining(df):
    """
    ä½¿ç”¨å…³è”è§„åˆ™æŒ–æ˜æ¥å‘ç°ç­–ç•¥æ¡ä»¶ä¸é«˜æ¶¨å¹…ä¹‹é—´çš„å…³ç³»ã€‚
    """
    print("\n4. æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜...")

    data_for_ar = []
    # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯floatç±»å‹
    numeric_cols = [
        'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡',
        'SMA_5', 'SMA_10', 'SMA_20', 'SMA_60', 'EMA_5', 'EMA_10', 'EMA_20',
        'MACD', 'MACD_Signal', 'MACD_Diff', 'RSI', 'K', 'D', 'J',
        'BB_Upper', 'BB_Lower', 'BB_Width', 'BB_Percent', 'ATR',
        'Volume_SMA_5', 'Volume_SMA_10', 'Daily_Return', 'Log_Return',
        'Price_Pos_F', 'Price_Pos_G',
        'PE_TTM', 'PB', 'ROE', 'Revenue_Growth', 'NetProfit_Growth'
    ]
    for col in numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(safe_float)

    for _, row in df.iterrows():
        items = []

        # è‹æ°ç­–ç•¥ç‰¹å¾
        f_condition = safe_float(row.get('Price_Pos_F'))
        g_condition = safe_float(row.get('Price_Pos_G'))
        profit_growth = safe_float(row.get('NetProfit_Growth'))
        turnover = safe_float(row.get('æ¢æ‰‹ç‡'))
        pe_ratio = safe_float(row.get('PE_TTM'))
        rsi = safe_float(row.get('RSI'))

        if pd.notna(f_condition):
            items.append("F_ä»·æ ¼ä½ç½®_æ»¡è¶³" if f_condition == 1 else "F_ä»·æ ¼ä½ç½®_ä¸æ»¡è¶³")
        if pd.notna(g_condition):
            items.append("G_æ¶¨å¹…ä½ç½®_æ»¡è¶³" if g_condition == 1 else "G_æ¶¨å¹…ä½ç½®_ä¸æ»¡è¶³")
        if pd.notna(profit_growth):
            items.append("å‡€åˆ©æ¶¦å¢é•¿_é«˜" if profit_growth >= 20 else "å‡€åˆ©æ¶¦å¢é•¿_ä½") # 20%å¢é•¿
        if pd.notna(turnover):
            items.append("æ¢æ‰‹ç‡_é€‚ä¸­" if 1 <= turnover <= 10 else "æ¢æ‰‹ç‡_æç«¯") # 1%-10%ä¸ºé€‚ä¸­
        if pd.notna(pe_ratio):
            items.append("å¸‚ç›ˆç‡_ä½" if pe_ratio > 0 and pe_ratio <= 30 else "å¸‚ç›ˆç‡_é«˜")
        if pd.notna(rsi):
            items.append("RSI_è¶…ä¹°" if rsi >= 70 else ("RSI_è¶…å–" if rsi <= 30 else "RSI_ä¸­æ€§"))

        # ç›®æ ‡å˜é‡ï¼šé«˜æ¶¨å¹… (ä¾‹å¦‚ï¼Œå½“æ—¥æ¶¨å¹… > 5%)
        change = safe_float(row.get('æ¶¨è·Œå¹…'))
        if pd.notna(change) and change > 5.0: # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
            items.append("å½“æ—¥é«˜æ¶¨å¹…")
        else:
            items.append("å½“æ—¥ä½æ¶¨å¹…")

        if items: # ç¡®ä¿æœ‰æœ‰æ•ˆé¡¹
            data_for_ar.append(items)

    if not data_for_ar:
        print("   âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ã€‚")
        return

    te = TransactionEncoder()
    te_ary = te.fit(data_for_ar).transform(data_for_ar)
    df_ar = pd.DataFrame(te_ary, columns=te.columns_)

    frequent_itemsets = apriori(df_ar, min_support=AR_MIN_SUPPORT, use_colnames=True)
    if frequent_itemsets.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·å°è¯•é™ä½ min_supportã€‚")
        return

    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=AR_MIN_THRESHOLD)

    if rules.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°æœ‰æ„ä¹‰çš„å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½ min_threshold æˆ–æ£€æŸ¥æ•°æ®ã€‚")
        return

    high_return_rules = rules[rules['consequents'].apply(lambda x: 'å½“æ—¥é«˜æ¶¨å¹…' in x)]
    high_return_rules = high_return_rules.sort_values(by=['lift', 'confidence'], ascending=False)

    print("\n   å‘ç°ä»¥ä¸‹ä¸ 'å½“æ—¥é«˜æ¶¨å¹…' ç›¸å…³çš„å…³è”è§„åˆ™ (æŒ‰ Lift é™åº):")
    if high_return_rules.empty:
        print("   æœªæ‰¾åˆ°ç›´æ¥å¯¼è‡´ 'å½“æ—¥é«˜æ¶¨å¹…' çš„å…³è”è§„åˆ™ã€‚")
    else:
        for i, rule in high_return_rules.head(10).iterrows(): # åªæ˜¾ç¤ºå‰10æ¡
            antecedent_str = ', '.join(list(rule['antecedents']))
            consequent_str = ', '.join(list(rule['consequents']))
            print(f"   è§„åˆ™ {i+1}: {antecedent_str} => {consequent_str}")
            print(f"     æ”¯æŒåº¦ (Support): {rule['support']:.4f}")
            print(f"     ç½®ä¿¡åº¦ (Confidence): {rule['confidence']:.4f}")
            print(f"     æå‡åº¦ (Lift): {rule['lift']:.4f}")
            print("-" * 40)

    print("\n   å…³è”è§„åˆ™æŒ–æ˜å®Œæˆã€‚è¿™äº›è§„åˆ™å¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ´å¯Ÿã€‚")

def simplified_backtest(signals_df, initial_capital=100000):
    """
    ç®€åŒ–çš„å›æµ‹åŠŸèƒ½ï¼Œæ¨¡æ‹Ÿæ ¹æ®ä¿¡å·è¿›è¡Œäº¤æ˜“ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ–çš„ç‰ˆæœ¬ï¼Œä¸è€ƒè™‘æ»‘ç‚¹ã€ä½£é‡‘ã€èµ„é‡‘ç®¡ç†ã€ç›˜ä¸­ä»·æ ¼æ³¢åŠ¨ç­‰å¤æ‚å› ç´ ã€‚
    ä»…ç”¨äºåˆæ­¥è¯„ä¼°ç­–ç•¥çš„æ½œåœ¨æ–¹å‘ã€‚
    """
    print("\n5. ç®€åŒ–çš„ç­–ç•¥å›æµ‹ (ä»…ä¾›å‚è€ƒï¼Œä¸å«çœŸå®äº¤æ˜“ç»†èŠ‚)...")
    
    # å‡è®¾æˆ‘ä»¬åªå…³æ³¨â€œå¼ºçƒˆä¹°å…¥â€å’Œâ€œå–å‡ºâ€ä¿¡å·
    buy_signals = signals_df[signals_df['ç»¼åˆä¿¡å·'] == 'å¼ºçƒˆä¹°å…¥']
    sell_signals = signals_df[signals_df['ç»¼åˆä¿¡å·'] == 'å–å‡º']

    print(f"   åˆå§‹èµ„é‡‘: {initial_capital:.2f} å…ƒ")
    current_capital = initial_capital
    positions = {} # {è‚¡ç¥¨ä»£ç : {'quantity': æ•°é‡, 'avg_price': å‡ä»·}}
    trade_log = []

    # æ¨¡æ‹Ÿäº¤æ˜“ (éå¸¸ç²—ç•¥çš„é€»è¾‘)
    # å‡è®¾æˆ‘ä»¬åªåœ¨ä¿¡å·ç”Ÿæˆå½“å¤©è¿›è¡Œäº¤æ˜“ï¼Œå¹¶ä¸”åªä¹°å…¥å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨ï¼Œå–å‡ºå–å‡ºä¿¡å·çš„è‚¡ç¥¨
    # å¹¶ä¸”å‡è®¾æˆ‘ä»¬èƒ½ä»¥æœ€æ–°ä»·æˆäº¤

    # å–å‡ºæ“ä½œ (å…ˆå¤„ç†å–å‡ºï¼Œé‡Šæ”¾èµ„é‡‘)
    for _, row in sell_signals.iterrows():
        code = row['ä»£ç ']
        if code in positions and positions[code]['quantity'] > 0:
            latest_price = safe_float(row['æœ€æ–°ä»·'])
            if pd.notna(latest_price) and latest_price > 0:
                profit_loss = (latest_price - positions[code]['avg_price']) * positions[code]['quantity']
                current_capital += latest_price * positions[code]['quantity']
                trade_log.append(f"   å–å‡º {code} ({row['åç§°']}): æ•°é‡ {positions[code]['quantity']}, ä»·æ ¼ {latest_price:.2f}, ç›ˆäº {profit_loss:.2f}")
                del positions[code]
            else:
                trade_log.append(f"   å–å‡º {code} ({row['åç§°']}): ä»·æ ¼æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•æ‰§è¡Œã€‚")

    # ä¹°å…¥æ“ä½œ (ç”¨å‰©ä½™èµ„é‡‘ä¹°å…¥å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨)
    if not buy_signals.empty:
        # å¹³å‡åˆ†é…èµ„é‡‘ç»™æ‰€æœ‰å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨
        num_buy_stocks = len(buy_signals)
        if num_buy_stocks > 0:
            capital_per_stock = current_capital / num_buy_stocks
            for _, row in buy_signals.iterrows():
                code = row['ä»£ç ']
                latest_price = safe_float(row['æœ€æ–°ä»·'])
                if pd.notna(latest_price) and latest_price > 0:
                    quantity = int(capital_per_stock / latest_price / 100) * 100 # è´­ä¹°100è‚¡çš„æ•´æ•°å€
                    if quantity > 0:
                        cost = quantity * latest_price
                        current_capital -= cost
                        positions[code] = {'quantity': quantity, 'avg_price': latest_price}
                        trade_log.append(f"   ä¹°å…¥ {code} ({row['åç§°']}): æ•°é‡ {quantity}, ä»·æ ¼ {latest_price:.2f}, æˆæœ¬ {cost:.2f}")
                else:
                    trade_log.append(f"   ä¹°å…¥ {code} ({row['åç§°']}): ä»·æ ¼æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•æ‰§è¡Œã€‚")

    # è®¡ç®—å½“å‰æ€»èµ„äº§
    current_portfolio_value = current_capital
    for code, pos in positions.items():
        # å°è¯•è·å–å½“å‰æŒä»“è‚¡ç¥¨çš„æœ€æ–°ä»·æ ¼ (è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨ä¿¡å·DFä¸­çš„æœ€æ–°ä»·)
        latest_price_in_df = safe_float(signals_df[signals_df['ä»£ç '] == code]['æœ€æ–°ä»·'].iloc[0])
        if pd.notna(latest_price_in_df):
            current_portfolio_value += pos['quantity'] * latest_price_in_df
        else:
            # å¦‚æœæœ€æ–°ä»·ç¼ºå¤±ï¼Œåˆ™ç”¨ä¹°å…¥æ—¶çš„å¹³å‡ä»·ä¼°ç®—
            current_portfolio_value += pos['quantity'] * pos['avg_price']


    total_return = (current_portfolio_value - initial_capital) / initial_capital * 100

    print("\n   --- äº¤æ˜“æ—¥å¿— ---")
    if not trade_log:
        print("   æ— äº¤æ˜“å‘ç”Ÿã€‚")
    for log in trade_log:
        print(log)
    print("\n   --- å›æµ‹ç»“æœ ---")
    print(f"   æœŸæœ«æ€»èµ„äº§: {current_portfolio_value:.2f} å…ƒ")
    print(f"   æ€»æ”¶ç›Šç‡: {total_return:.2f}%")
    print(f"   å½“å‰æŒä»“è‚¡ç¥¨æ•°é‡: {len(positions)}")
    if positions:
        print("   å½“å‰æŒä»“æ˜ç»†:")
        for code, pos in positions.items():
            print(f"     - {code}: æ•°é‡ {pos['quantity']}, å‡ä»· {pos['avg_price']:.2f}")
    print("   å›æµ‹ç»“æŸã€‚")


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("é«˜çº§é‡åŒ–é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆæœºå™¨å­¦ä¹ ä¸å¤šç­–ç•¥)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–å¹¶æ•´åˆæ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ® (å®æ—¶ + å†å²Kçº¿ + è´¢åŠ¡æŒ‡æ ‡)...")

    # 1.1 è·å–æ‰€æœ‰Aè‚¡å®æ—¶æ•°æ® (ç”¨äºè·å–è‚¡ç¥¨åˆ—è¡¨å’Œä»Šæ—¥å®æ—¶è¡Œæƒ…)
    df_realtime = pd.DataFrame()
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°', 'æ¶¨è·Œå¹…': 'æ¶¨å¹…%', 'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼': 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦': 'å½’å±å‡€åˆ©æ¶¦'
        }, inplace=True)
        df_realtime['è‚¡ç¥¨ä»£ç '] = df_realtime['ä»£ç '].copy() # ç»Ÿä¸€åˆ—å
        df_realtime['ä»£ç '] = df_realtime['ä»£ç '].apply(lambda x: f'= "{str(x)}"') # æ ¼å¼åŒ–ä»£ç ç”¨äºExcel

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨
        required_cols_realtime = ['ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%',
                                  'æ‰€å±è¡Œä¸š', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'è‚¡ç¥¨ä»£ç ']
        for col in required_cols_realtime:
            if col not in df_realtime.columns:
                df_realtime[col] = np.nan

    except Exception as e:
        print(f"   âŒ å®æ—¶æ•°æ®è·å–å¤±è´¥: {e}")
        print("   å°†å°è¯•ä»å‚è€ƒæ•°æ®è·å–è‚¡ç¥¨ä»£ç ã€‚")
        df_realtime = pd.DataFrame(columns=required_cols_realtime) # åˆ›å»ºç©ºDataFrameä»¥é¿å…åç»­é”™è¯¯

    stock_codes_to_fetch = df_realtime['è‚¡ç¥¨ä»£ç '].tolist()
    if not stock_codes_to_fetch: # å¦‚æœå®æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•ä»å‚è€ƒæ•°æ®è·å–ä»£ç 
        try:
            ref_df_path = 'å‚è€ƒæ•°æ®/Table.xls'
            if os.path.exists(ref_df_path):
                ref_df_codes = pd.read_csv(ref_df_path, sep='\t', encoding='gbk', dtype=str)
                stock_codes_to_fetch = ref_df_codes['ä»£ç '].str.replace('= "', '').str.replace('"', '').tolist()
                print(f"   ä»å‚è€ƒæ•°æ®è·å–äº† {len(stock_codes_to_fetch)} ä¸ªè‚¡ç¥¨ä»£ç ã€‚")
        except Exception as e:
            print(f"   âŒ æ— æ³•ä»å‚è€ƒæ•°æ®è·å–è‚¡ç¥¨ä»£ç : {e}")
            print("   è¯·ç¡®ä¿ 'å‚è€ƒæ•°æ®/Table.xls' å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ã€‚")
            return

    # 1.2 è·å–å†å²æ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡
    all_historical_data = []
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=HISTORY_YEARS * 365)).strftime('%Y%m%d') # 3å¹´å†å²æ•°æ®

    print(f"   è·å–å†å²æ—¥Kçº¿æ•°æ® ({start_date} è‡³ {end_date})...")
    # é™åˆ¶è·å–æ•°é‡ï¼Œé¿å…APIé™åˆ¶æˆ–æ—¶é—´è¿‡é•¿ (è°ƒè¯•æ—¶å¯ä»¥å–æ¶ˆæ³¨é‡Š)
    # stock_codes_to_fetch = stock_codes_to_fetch[:50] 

    for i, code in enumerate(stock_codes_to_fetch):
        if i % 100 == 0: # æ¯100åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f"     æ­£åœ¨è·å–ç¬¬ {i}/{len(stock_codes_to_fetch)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")
        hist_df = get_stock_hist_data(code, start_date, end_date)
        if not hist_df.empty:
            all_historical_data.append(hist_df)
        time.sleep(0.05) # é¿å…APIé¢‘ç‡é™åˆ¶

    if not all_historical_data:
        print("   âŒ æœªèƒ½è·å–ä»»ä½•è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œä¿¡å·ç”Ÿæˆã€‚")
        return

    df_historical = pd.concat(all_historical_data)
    print(f"   âœ… æˆåŠŸè·å– {len(df_historical)} æ¡å†å²Kçº¿æ•°æ®ã€‚")

    # 1.3 è·å–è´¢åŠ¡æ•°æ®å¹¶åˆå¹¶
    print("   è·å–è´¢åŠ¡æŒ‡æ ‡æ•°æ® (æœ€æ–°ä¸€æœŸ)...")
    all_financial_data = []
    for i, code in enumerate(stock_codes_to_fetch):
        if i % 100 == 0:
            print(f"     æ­£åœ¨è·å–ç¬¬ {i}/{len(stock_codes_to_fetch)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®...")
        fin_data = get_financial_data(code)
        if not fin_data.empty:
            fin_data['è‚¡ç¥¨ä»£ç '] = code
            all_financial_data.append(fin_data)
        time.sleep(0.05) # é¿å…APIé¢‘ç‡é™åˆ¶

    if all_financial_data:
        df_financial = pd.DataFrame(all_financial_data).set_index('è‚¡ç¥¨ä»£ç ')
        # å°†è´¢åŠ¡æ•°æ®åˆå¹¶åˆ°å†å²æ•°æ®ä¸­ (æŒ‰è‚¡ç¥¨ä»£ç åˆå¹¶ï¼Œå¹¶å‘å‰å¡«å……ï¼Œå‡è®¾è´¢åŠ¡æ•°æ®åœ¨æŠ¥å‘ŠæœŸåä¸€ç›´æœ‰æ•ˆ)
        df_historical = df_historical.reset_index().set_index('è‚¡ç¥¨ä»£ç ')
        df_historical = df_historical.merge(df_financial, left_index=True, right_index=True, how='left')
        df_historical = df_historical.reset_index().set_index('æ—¥æœŸ') # æ¢å¤æ—¥æœŸç´¢å¼•
        print(f"   âœ… æˆåŠŸè·å–å¹¶åˆå¹¶ {len(df_financial)} åªè‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®ã€‚")
    else:
        print("   âš ï¸ æœªèƒ½è·å–ä»»ä½•è‚¡ç¥¨çš„è´¢åŠ¡æ•°æ®ã€‚")

    # 1.4 è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    print("   è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
    # å¯¹æ¯ä¸ªè‚¡ç¥¨åˆ†ç»„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼Œç¡®ä¿æŒ‡æ ‡è®¡ç®—çš„æ­£ç¡®æ€§
    df_historical_with_indicators = df_historical.groupby('è‚¡ç¥¨ä»£ç ', group_keys=False).apply(calculate_technical_indicators)
    print("   âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆã€‚")

    # 1.5 å‡†å¤‡ä»Šæ—¥æ•°æ®ç”¨äºé¢„æµ‹
    # å°†å®æ—¶æ•°æ®ä¸­çš„åˆ—åæ˜ å°„åˆ°å†å²æ•°æ®ä¸­çš„åˆ—åï¼Œä»¥ä¾¿è®¡ç®—æŒ‡æ ‡
    df_current_day_data = df_realtime.copy()
    df_current_day_data['æ—¥æœŸ'] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) # è®¾ç½®ä¸ºä»Šæ—¥æ—¥æœŸ
    df_current_day_data.set_index('æ—¥æœŸ', inplace=True)
    df_current_day_data.rename(columns={
        'æœ€æ–°': 'æ”¶ç›˜', 'æ¶¨å¹…%': 'æ¶¨è·Œå¹…', 'å®é™…æ¢æ‰‹%': 'æ¢æ‰‹ç‡'
    }, inplace=True)

    # ç¡®ä¿å®æ—¶æ•°æ®åŒ…å«æ‰€æœ‰è®¡ç®—æŒ‡æ ‡æ‰€éœ€çš„åˆ—
    for col in ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']:
        if col not in df_current_day_data.columns:
            df_current_day_data[col] = np.nan
    
    # å¡«å……æˆäº¤é‡å’Œæˆäº¤é¢çš„ç¼ºå¤±å€¼ï¼Œé¿å…æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥
    df_current_day_data['æˆäº¤é‡'] = df_current_day_data['æˆäº¤é‡'].fillna(0)
    df_current_day_data['æˆäº¤é¢'] = df_current_day_data['æˆäº¤é¢'].fillna(0)

    # å°†å®æ—¶æ•°æ®è¿½åŠ åˆ°å†å²æ•°æ®ä¸­ï¼Œç„¶åé‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    # è¿™æ ·åšå¯ä»¥ç¡®ä¿å®æ—¶æ•°æ®çš„æŠ€æœ¯æŒ‡æ ‡æ˜¯åŸºäºæœ€æ–°çš„å†å²æ•°æ®è®¡ç®—çš„
    # æ³¨æ„ï¼šè¿™é‡Œåªè¿½åŠ Kçº¿ç›¸å…³æ•°æ®ï¼Œè´¢åŠ¡æ•°æ®å·²ç»åˆå¹¶åˆ°df_historical_with_indicatorsä¸­
    df_combined_for_indicators = pd.concat([
        df_historical_with_indicators[['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'è‚¡ç¥¨ä»£ç ']],
        df_current_day_data[['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡', 'è‚¡ç¥¨ä»£ç ']]
    ])
    df_combined_for_indicators = df_combined_for_indicators.groupby('è‚¡ç¥¨ä»£ç ', group_keys=False).apply(calculate_technical_indicators)

    # æå–ä»Šå¤©çš„æœ€æ–°æ•°æ® (ç”¨äºä¿¡å·ç”Ÿæˆ)
    df_today_features = df_combined_for_indicators[df_combined_for_indicators.index == datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)].copy()
    
    # å°†å®æ—¶æ•°æ®ä¸­çš„åç§°ã€è¡Œä¸šã€å¸‚ç›ˆç‡ã€æ€»å¸‚å€¼ã€å½’å±å‡€åˆ©æ¶¦ç­‰ä¿¡æ¯åˆå¹¶å›df_today_features
    df_today_features = df_today_features.merge(
        df_realtime[['è‚¡ç¥¨ä»£ç ', 'åç§°', 'æ‰€å±è¡Œä¸š', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']],
        on='è‚¡ç¥¨ä»£ç ',
        how='left'
    )
    # ç¡®ä¿'æœ€æ–°'ä»·æ˜¯å®æ—¶æ•°æ®ä¸­çš„æœ€æ–°ä»·ï¼Œè€Œä¸æ˜¯å†å²æ”¶ç›˜ä»·
    df_today_features['æœ€æ–°'] = df_today_features['æ”¶ç›˜'] # å®æ—¶æ•°æ®ä¸­'æœ€æ–°'ä»·è¢«æ˜ å°„ä¸º'æ”¶ç›˜'
    df_today_features['æ¶¨å¹…%'] = df_today_features['æ¶¨è·Œå¹…'] # å®æ—¶æ•°æ®ä¸­'æ¶¨å¹…%'è¢«æ˜ å°„ä¸º'æ¶¨è·Œå¹…'

    # å°†è´¢åŠ¡æ•°æ®åˆå¹¶åˆ°df_today_features
    if 'df_financial' in locals() and not df_financial.empty:
        df_today_features = df_today_features.merge(df_financial.reset_index(), on='è‚¡ç¥¨ä»£ç ', how='left')
    
    # æœ€ç»ˆç”¨äºæ¨¡å‹è®­ç»ƒçš„æ•°æ®é›† (åŒ…å«æ‰€æœ‰å†å²Kçº¿ã€æŠ€æœ¯æŒ‡æ ‡å’Œè´¢åŠ¡æ•°æ®)
    df_for_training = df_historical_with_indicators.copy()
    if 'df_financial' in locals() and not df_financial.empty:
        df_for_training = df_for_training.reset_index().set_index('è‚¡ç¥¨ä»£ç ')
        df_for_training = df_for_training.merge(df_financial, left_index=True, right_index=True, how='left')
        df_for_training = df_for_training.reset_index().set_index('æ—¥æœŸ')


    # ä¿å­˜æ•´åˆåçš„æ•°æ® (å¯é€‰)
    # df_for_training.to_csv('è¾“å‡ºæ•°æ®/æ•´åˆå†å²ä¸è´¢åŠ¡æ•°æ®_è®­ç»ƒé›†.csv', encoding='utf-8-sig')
    # df_today_features.to_csv('è¾“å‡ºæ•°æ®/ä»Šæ—¥é¢„æµ‹ç‰¹å¾é›†.csv', encoding='utf-8-sig')
    # print("   âœ… æ•´åˆåçš„æ•°æ®å·²ä¿å­˜ã€‚")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ ==========
    print("\n2. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")
    
    # è®­ç»ƒçŸ­æœŸæ¨¡å‹
    short_term_model, short_term_scaler, short_term_features = train_ml_model(df_for_training.copy(), SHORT_TERM_FUTURE_DAYS)
    
    # è®­ç»ƒé•¿æœŸæ¨¡å‹
    long_term_model, long_term_scaler, long_term_features = train_ml_model(df_for_training.copy(), LONG_TERM_FUTURE_DAYS)

    if short_term_model is None or long_term_model is None:
        print("   âŒ è‡³å°‘ä¸€ä¸ªæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ä¿¡å·ç”Ÿæˆã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆäº¤æ˜“ä¿¡å· ==========
    signals_df = generate_trading_signals(
        df_today_features,
        short_term_model, short_term_scaler, short_term_features,
        long_term_model, long_term_scaler, long_term_features
    )

    # ä¿å­˜äº¤æ˜“ä¿¡å·
    output_file_signals = 'è¾“å‡ºæ•°æ®/äº¤æ˜“ä¿¡å·.csv'
    signals_df.to_csv(output_file_signals, index=False, encoding='utf-8-sig')
    print(f"\nâœ… äº¤æ˜“ä¿¡å·å·²ä¿å­˜: {output_file_signals}")
    print("\nğŸ¯ ä»Šæ—¥äº¤æ˜“ä¿¡å·æ¦‚è§ˆ (å‰20å):")
    print(signals_df.head(20).to_string())

    # ========== ç¬¬å››æ­¥ï¼šå…³è”è§„åˆ™æŒ–æ˜ ==========
    # å¯¹æ‰€æœ‰å†å²æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ï¼Œä»¥å‘ç°æ™®éè§„å¾‹
    perform_association_rule_mining(df_for_training.copy())

    # ========== ç¬¬äº”æ­¥ï¼šç®€åŒ–çš„å›æµ‹ ==========
    # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ–çš„å›æµ‹ç¤ºä¾‹ï¼Œå®é™…å›æµ‹éœ€è¦æ›´å¤æ‚çš„å†å²æ•°æ®å’Œäº¤æ˜“æ¨¡æ‹Ÿ
    # è¿™é‡Œçš„å›æµ‹æ˜¯åŸºäºå½“å‰ç”Ÿæˆçš„ä¿¡å·ï¼Œæ¨¡æ‹Ÿåœ¨ä»Šå¤©è¿›è¡Œäº¤æ˜“ï¼Œå¹¶å‡è®¾è¿™äº›äº¤æ˜“æ˜¯æˆåŠŸçš„ã€‚
    # çœŸæ­£çš„å›æµ‹éœ€è¦å°†æ¨¡å‹åº”ç”¨äºå†å²æ¯ä¸€å¤©çš„æ•°æ®ï¼Œå¹¶æ¨¡æ‹Ÿäº¤æ˜“è¿‡ç¨‹ã€‚
    simplified_backtest(signals_df.copy())

    print("\n" + "="*60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
