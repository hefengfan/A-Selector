#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆæœºå™¨å­¦ä¹ ä¸å¤šç­–ç•¥)
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆæ›´ç²¾å¯†çš„æœºå™¨å­¦ä¹ æ¨¡å‹ (XGBoost) è¿›è¡Œç²¾å‡†è¯„åˆ†å’Œä¿¡å·ç”Ÿæˆ
æ–°å¢ï¼š
1. æ›´ä¸¥æ ¼çš„æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ï¼Œå¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼ã€‚
2. ç‰¹å¾é€‰æ‹©ï¼Œä½¿ç”¨ç‰¹å¾é‡è¦æ€§åˆ†æé€‰æ‹©æœ€ç›¸å…³çš„ç‰¹å¾ã€‚
3. æ›´ç¨³å®šçš„æ¨¡å‹è®­ç»ƒï¼Œå¢åŠ äº¤å‰éªŒè¯çš„æŠ˜æ•°ã€‚
4. æ›´ç²¾ç»†çš„ä¿¡å·ç”Ÿæˆï¼Œç»“åˆå¤šç§ä¿¡å·ï¼Œå¹¶å¼•å…¥ç½®ä¿¡åº¦è¯„ä¼°ã€‚
5. æ›´æ¸…æ™°çš„ç»“æœå±•ç¤ºï¼Œæä¾›æ›´è¯¦ç»†çš„äº¤æ˜“ä¿¡å·å’Œå›æµ‹æŠ¥å‘Šã€‚
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import optuna

# å¯¼å…¥æŠ€æœ¯åˆ†æåº“
import ta

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

# ==================== é…ç½®å‚æ•° ====================
# å†å²æ•°æ®è·å–èŒƒå›´ (ç”¨äºæ¨¡å‹è®­ç»ƒå’ŒæŒ‡æ ‡è®¡ç®—)
HISTORY_DAYS = 252 * 2 # çº¦2å¹´äº¤æ˜“æ—¥æ•°æ®
# é¢„æµ‹æœªæ¥æ”¶ç›Šçš„å¤©æ•° (çŸ­æœŸ/é•¿æœŸ)
SHORT_TERM_FUTURE_DAYS = 5  # é¢„æµ‹æœªæ¥5å¤©æ”¶ç›Š
LONG_TERM_FUTURE_DAYS = 20  # é¢„æµ‹æœªæ¥20å¤©æ”¶ç›Š

# ä¿¡å·ç”Ÿæˆé˜ˆå€¼
BUY_SCORE_THRESHOLD_SHORT = 0.02 # çŸ­æœŸé¢„æµ‹æ”¶ç›Šç‡é«˜äºæ­¤å€¼åˆ™ä¹°å…¥
BUY_SCORE_THRESHOLD_LONG = 0.05  # é•¿æœŸé¢„æµ‹æ”¶ç›Šç‡é«˜äºæ­¤å€¼åˆ™ä¹°å…¥

# å…³è”è§„åˆ™æŒ–æ˜å‚æ•°
AR_MIN_SUPPORT = 0.01
AR_MIN_THRESHOLD = 1.1

# äº¤å‰éªŒè¯æŠ˜æ•°
CV_FOLDS = 5

# ==================== è¾…åŠ©å‡½æ•° ====================

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 100000000 # 1äº¿ = 10000ä¸‡
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 1000000000000 # 1ä¸‡äº¿ = 10000äº¿
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) * 10000 # 1ä¸‡
        return float(s)
    except ValueError:
        return np.nan

def get_stock_data(symbol, start_date, end_date):
    """
    è·å–æŒ‡å®šè‚¡ç¥¨çš„å†å²æ—¥Kçº¿æ•°æ®ã€‚
    """
    try:
        df = ak.stock_zh_a_hist(symbol=symbol, period="daily", start_date=start_date, end_date=end_date, adjust="qfq")
        df.columns = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        df.set_index('æ—¥æœŸ', inplace=True)
        # ç¡®ä¿æ•°å€¼åˆ—ä¸ºfloat
        for col in ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        return df
    except Exception as e:
        # print(f"è·å– {symbol} å†å²æ•°æ®å¤±è´¥: {e}")
        return pd.DataFrame()

def calculate_technical_indicators(df):
    """
    è®¡ç®—å¤šç§æŠ€æœ¯æŒ‡æ ‡ã€‚
    è¾“å…¥: åŒ…å« 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡' çš„DataFrame
    è¾“å‡º: åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„DataFrame
    """
    if df.empty:
        return df

    # ç§»åŠ¨å¹³å‡çº¿
    df['SMA_5'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=5)
    df['SMA_10'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=10)
    df['SMA_20'] = ta.trend.sma_indicator(df['æ”¶ç›˜'], window=20)
    df['EMA_5'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=5)
    df['EMA_10'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=10)
    df['EMA_20'] = ta.trend.ema_indicator(df['æ”¶ç›˜'], window=20)

    # MACD
    macd = ta.trend.MACD(df['æ”¶ç›˜'])
    df['MACD'] = macd.macd()
    df['MACD_Signal'] = macd.macd_signal()
    df['MACD_Diff'] = macd.macd_diff()

    # RSI
    df['RSI'] = ta.momentum.rsi(df['æ”¶ç›˜'], window=14)

    # KDJ (Stochastic Oscillator)
    stoch = ta.momentum.StochasticOscillator(df['æœ€é«˜'], df['æœ€ä½'], df['æ”¶ç›˜'])
    df['K'] = stoch.stoch()
    df['D'] = stoch.stoch_signal()
    df['J'] = 3 * df['K'] - 2 * df['D']

    # Bollinger Bands
    bollinger = ta.volatility.BollingerBands(df['æ”¶ç›˜'])
    df['BB_Upper'] = bollinger.bollinger_hband()
    df['BB_Lower'] = bollinger.bollinger_lband()
    df['BB_Width'] = bollinger.bollinger_wband()

    # ATR (Average True Range)
    df['ATR'] = ta.volatility.average_true_range(df['æœ€é«˜'], df['æœ€ä½'], df['æ”¶ç›˜'], window=14)

    # æˆäº¤é‡ç§»åŠ¨å¹³å‡
    df['Volume_SMA_5'] = ta.volume.volume_sma_indicator(df['æˆäº¤é‡'], window=5)
    df['Volume_SMA_10'] = ta.volume.volume_sma_indicator(df['æˆäº¤é‡'], window=10)

    # ä»·æ ¼å˜åŠ¨ç‡
    df['Daily_Return'] = df['æ”¶ç›˜'].pct_change()
    df['Log_Return'] = np.log(df['æ”¶ç›˜'] / df['æ”¶ç›˜'].shift(1))

    # ä»·æ ¼ä½ç½® (è‹æ°ç­–ç•¥)
    df['Price_Pos_F'] = 0
    df.loc[(df['æœ€ä½'] / df['SMA_60'] >= 0.85) & (df['æœ€ä½'] / df['SMA_60'] <= 1.15), 'Price_Pos_F'] = 1
    df.loc[(df['æ”¶ç›˜'] / df['SMA_20'] >= 0.90) & (df['æ”¶ç›˜'] / df['SMA_20'] <= 1.10), 'Price_Pos_F'] = 1

    # æ¶¨å¹…å’Œä»·æ ¼ä½ç½® (è‹æ°ç­–ç•¥)
    df['Price_Pos_G'] = 0
    df.loc[(df['æ¶¨è·Œå¹…'] >= 5.0) & ((df['æ”¶ç›˜'] >= (df['æœ€é«˜'] - (df['æœ€é«˜'] - df['æœ€ä½']) * 0.30)) | (df['æœ€é«˜'] == df['æœ€ä½'])), 'Price_Pos_G'] = 1

    return df

def prepare_data_for_ml(df_all_stocks, future_days):
    """
    å‡†å¤‡ç”¨äºæœºå™¨å­¦ä¹ çš„æ•°æ®é›†ï¼ŒåŒ…æ‹¬ç‰¹å¾å’Œç›®æ ‡å˜é‡ã€‚
    ç›®æ ‡å˜é‡ä¸ºæœªæ¥ N æ—¥çš„æ”¶ç›Šç‡ã€‚
    """
    features = []
    targets = []
    stock_codes = []
    dates = []

    # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯floatç±»å‹
    numeric_cols = ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
                    'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜',
                    'SMA_5', 'SMA_10', 'SMA_20', 'EMA_5', 'EMA_10', 'EMA_20',
                    'MACD', 'MACD_Signal', 'MACD_Diff', 'RSI', 'K', 'D', 'J',
                    'BB_Upper', 'BB_Lower', 'BB_Width', 'ATR', 'Volume_SMA_5',
                    'Volume_SMA_10', 'Daily_Return', 'Log_Return', 'Price_Pos_F', 'Price_Pos_G']

    for col in numeric_cols:
        if col in df_all_stocks.columns:
            df_all_stocks[col] = df_all_stocks[col].apply(safe_float)

    # éå†æ¯åªè‚¡ç¥¨ï¼Œè®¡ç®—å…¶ç‰¹å¾å’Œæœªæ¥æ”¶ç›Š
    for stock_code in df_all_stocks['åŸå§‹ä»£ç '].unique():
        stock_df = df_all_stocks[df_all_stocks['åŸå§‹ä»£ç '] == stock_code].sort_index() # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº

        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡ä½œä¸ºç›®æ ‡å˜é‡
        # æœªæ¥ N æ—¥çš„æ”¶ç›˜ä»· / å½“æ—¥æ”¶ç›˜ä»· - 1
        stock_df['future_return'] = stock_df['æ”¶ç›˜'].shift(-future_days) / stock_df['æ”¶ç›˜'] - 1

        # æå–ç‰¹å¾
        for i in range(len(stock_df)):
            row = stock_df.iloc[i]
            # ç¡®ä¿ç›®æ ‡å˜é‡å­˜åœ¨ä¸”ä¸æ˜¯NaN
            if pd.notna(row['future_return']):
                # æå–æ‰€æœ‰æ•°å€¼ç‰¹å¾
                current_features = []
                for col in numeric_cols:
                    if col in row.index:
                        current_features.append(row[col])
                    else:
                        current_features.append(np.nan) # å¦‚æœåˆ—ä¸å­˜åœ¨ï¼Œåˆ™å¡«å……NaN

                # è¿‡æ»¤æ‰ç‰¹å¾ä¸­åŒ…å«NaNæˆ–Infçš„è¡Œ
                if not any(pd.isna(f) or np.isinf(f) for f in current_features):
                    features.append(current_features)
                    targets.append(row['future_return'])
                    stock_codes.append(stock_code)
                    dates.append(row.name) # è®°å½•æ—¥æœŸ

    X = np.array(features)
    y = np.array(targets)

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]
    stock_codes_filtered = np.array(stock_codes)[mask]
    dates_filtered = np.array(dates)[mask]

    # è·å–ç‰¹å¾åˆ—åï¼Œç”¨äºåç»­çš„ç‰¹å¾é‡è¦æ€§åˆ†ææˆ–è°ƒè¯•
    feature_names = numeric_cols

    return X, y, stock_codes_filtered, dates_filtered, feature_names

def objective_xgb(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ– XGBoostRegressor çš„ç›®æ ‡å‡½æ•°
    """
    param = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'booster': trial.suggest_categorical('booster', ['gbtree', 'dart']),
        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-2, 1e3),
        'seed': 42,
        'n_jobs': -1,
    }

    if param['booster'] == 'gbtree':
        param['eta'] = trial.suggest_loguniform('eta', 1e-8, 1.0)
        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)
        param['max_depth'] = trial.suggest_int('max_depth', 3, 9)
    elif param['booster'] == 'dart':
        param['eta'] = trial.suggest_loguniform('eta', 1e-8, 1.0)
        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])
        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])
        param['rate_drop'] = trial.suggest_loguniform('rate_drop', 1e-8, 1.0)
        param['skip_drop'] = trial.suggest_loguniform('skip_drop', 1e-8, 1.0)

    model = xgb.XGBRegressor(**param)
    model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              early_stopping_rounds=50, # æå‰åœæ­¢
              verbose=False)

    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    return rmse

def train_ml_model(df_all_stocks, future_days):
    """
    è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨æœªæ¥æ”¶ç›Šç‡ã€‚
    """
    print(f"\n   å‡†å¤‡è®­ç»ƒæ•°æ® (é¢„æµ‹æœªæ¥ {future_days} å¤©æ”¶ç›Šç‡)...")
    X, y, _, _, feature_names = prepare_data_for_ml(df_all_stocks.copy(), future_days)

    if len(X) < 50: # è‡³å°‘éœ€è¦ä¸€äº›æ•°æ®æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print(f"   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ ({len(X)} æ¡)ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚")
        return None, None, None, None

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # ç‰¹å¾é€‰æ‹© (åŸºäºç‰¹å¾é‡è¦æ€§)
    print("   ç‰¹å¾é€‰æ‹© (åŸºäºç‰¹å¾é‡è¦æ€§)...")
    model_for_importance = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
    model_for_importance.fit(X_scaled, y)
    importances = model_for_importance.feature_importances_
    # é€‰æ‹©é‡è¦æ€§å¤§äºå¹³å‡å€¼çš„ç‰¹å¾
    selected_features_indices = np.where(importances > np.mean(importances))[0]
    X_scaled_selected = X_scaled[:, selected_features_indices]
    selected_feature_names = [feature_names[i] for i in selected_features_indices]
    print(f"   é€‰æ‹©äº† {len(selected_feature_names)} ä¸ªé‡è¦ç‰¹å¾ã€‚")

    # äº¤å‰éªŒè¯
    print(f"   è¿›è¡Œ {CV_FOLDS} æŠ˜äº¤å‰éªŒè¯...")
    kf = KFold(n_splits=CV_FOLDS, shuffle=True, random_state=42)
    cv_scores = []

    best_model = None
    best_score = float('inf') # åˆå§‹åŒ–ä¸ºæ­£æ— ç©·

    for fold, (train_index, val_index) in enumerate(kf.split(X_scaled_selected, y)):
        print(f"     Fold {fold+1}/{CV_FOLDS}...")
        X_train, X_val = X_scaled_selected[train_index], X_scaled_selected[val_index]
        y_train, y_val = y[train_index], y[val_index]

        # Optuna è¶…å‚æ•°ä¼˜åŒ–
        print("       å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
        study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
        try:
            study.optimize(lambda trial: objective_xgb(trial, X_train, y_train, X_val, y_val), n_trials=20, show_progress_bar=False)
        except Exception as e:
            print(f"       Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            print("       å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
            best_params = {
                'objective': 'reg:squarederror',
                'eval_metric': 'rmse',
                'booster': 'gbtree',
                'lambda': 1,
                'alpha': 0,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'min_child_weight': 1,
                'eta': 0.1,
                'max_depth': 6,
                'seed': 42,
                'n_jobs': -1,
            }
        else:
            print("       Optuna ä¼˜åŒ–å®Œæˆã€‚")
            best_params = study.best_params

        # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæ¨¡å‹
        model = xgb.XGBRegressor(**best_params)
        model.fit(X_train, y_train,
                  eval_set=[(X_val, y_val)],
                  early_stopping_rounds=20,
                  verbose=False)

        # è¯„ä¼°æ¨¡å‹
        y_pred = model.predict(X_val)
        rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        cv_scores.append(rmse)

        if rmse < best_score:
            best_score = rmse
            best_model = model

    print(f"   äº¤å‰éªŒè¯å®Œæˆã€‚å¹³å‡ RMSE: {np.mean(cv_scores):.4f}")

    return best_model, scaler, selected_feature_names, selected_features_indices

def predict_future_return(row, model, scaler, feature_names, selected_features_indices):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹è‚¡ç¥¨æœªæ¥æ”¶ç›Šç‡ã€‚
    """
    # æå–å½“å‰è¡Œçš„ç‰¹å¾
    current_features = []
    for col in feature_names:
        if col in row.index:
            current_features.append(safe_float(row[col]))
        else:
            current_features.append(np.nan)

    # æ£€æŸ¥ç‰¹å¾ä¸­æ˜¯å¦æœ‰NaNæˆ–Inf
    if any(pd.isna(f) or np.isinf(f) for f in current_features):
        return np.nan

    features_array = np.array(current_features).reshape(1, -1)
    try:
        features_scaled = scaler.transform(features_array)
        # åªé€‰æ‹©è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨çš„ç‰¹å¾
        features_scaled_selected = features_scaled[:, selected_features_indices]
        predicted_return = model.predict(features_scaled_selected)[0]
        return predicted_return
    except Exception as e:
        # print(f"é¢„æµ‹æœªæ¥æ”¶ç›Šæ—¶å‘ç”Ÿé”™è¯¯: {e}, ç‰¹å¾: {current_features}")
        return np.nan

def generate_trading_signals(df_current_data, short_term_model, short_term_scaler, short_term_features, short_term_indices,
                             long_term_model, long_term_scaler, long_term_features, long_term_indices):
    """
    æ ¹æ®æ¨¡å‹é¢„æµ‹ç”Ÿæˆä¹°å…¥/å–å‡ºä¿¡å·ã€‚
    """
    print("\n3. ç”Ÿæˆäº¤æ˜“ä¿¡å·...")
    signals = []

    # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯floatç±»å‹
    numeric_cols = ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
                    'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']
    for col in numeric_cols:
        if col in df_current_data.columns:
            df_current_data[col] = df_current_data[col].apply(safe_float)

    for idx, row in df_current_data.iterrows():
        stock_code = str(row['åŸå§‹ä»£ç ']).strip()
        stock_name = str(row['åç§°']).strip()
        industry = str(row['æ‰€å±è¡Œä¸š']).strip()
        current_price = safe_float(row['æœ€æ–°'])
        current_change = safe_float(row['æ¶¨å¹…%'])

        short_pred_return = np.nan
        long_pred_return = np.nan
        signal_short = "æŒæœ‰"
        signal_long = "æŒæœ‰"
        overall_signal = "æŒæœ‰"
        confidence = 0.0 # ä¿¡å·ç½®ä¿¡åº¦

        # é¢„æµ‹çŸ­æœŸæ”¶ç›Š
        if short_term_model and short_term_scaler and short_term_features:
            short_pred_return = predict_future_return(row, short_term_model, short_term_scaler, short_term_features, short_term_indices)
            if pd.notna(short_pred_return):
                if short_pred_return >= BUY_SCORE_THRESHOLD_SHORT:
                    signal_short = "çŸ­æœŸä¹°å…¥"
                    confidence += 0.5 # å¢åŠ ç½®ä¿¡åº¦
                elif short_pred_return < -0.02: # å‡è®¾ä½äº-2%åˆ™å»ºè®®å–å‡º
                    signal_short = "çŸ­æœŸå–å‡º"
                    confidence += 0.5

        # é¢„æµ‹é•¿æœŸæ”¶ç›Š
        if long_term_model and long_term_scaler and long_term_features:
            long_pred_return = predict_future_return(row, long_term_model, long_term_scaler, long_term_features, long_term_indices)
            if pd.notna(long_pred_return):
                if long_pred_return >= BUY_SCORE_THRESHOLD_LONG:
                    signal_long = "é•¿æœŸä¹°å…¥"
                    confidence += 0.5
                elif long_pred_return < -0.05: # å‡è®¾ä½äº-5%åˆ™å»ºè®®å–å‡º
                    signal_long = "é•¿æœŸå–å‡º"
                    confidence += 0.5

        # ç»¼åˆä¿¡å·åˆ¤æ–­ (å¯æ ¹æ®å®é™…ç­–ç•¥è°ƒæ•´)
        if signal_short == "çŸ­æœŸä¹°å…¥" and signal_long == "é•¿æœŸä¹°å…¥":
            overall_signal = "å¼ºçƒˆä¹°å…¥"
            confidence = 1.0
        elif signal_short == "çŸ­æœŸä¹°å…¥" and signal_long == "æŒæœ‰":
            overall_signal = "çŸ­æœŸä¹°å…¥"
            confidence = 0.75
        elif signal_long == "é•¿æœŸä¹°å…¥" and signal_short == "æŒæœ‰":
            overall_signal = "é•¿æœŸä¹°å…¥"
            confidence = 0.75
        elif signal_short == "çŸ­æœŸå–å‡º" or signal_long == "é•¿æœŸå–å‡º":
            overall_signal = "å–å‡º"
            confidence = 0.75

        signals.append({
            'ä»£ç ': stock_code,
            'åç§°': stock_name,
            'è¡Œä¸š': industry,
            'æœ€æ–°ä»·': f"{current_price:.2f}" if pd.notna(current_price) else "--",
            'ä»Šæ—¥æ¶¨å¹…': f"{current_change:.2f}%" if pd.notna(current_change) else "--",
            f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š': f"{short_pred_return*100:.2f}%" if pd.notna(short_pred_return) else "--",
            f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š': f"{long_pred_return*100:.2f}%" if pd.notna(long_pred_return) else "--",
            'çŸ­æœŸä¿¡å·': signal_short,
            'é•¿æœŸä¿¡å·': signal_long,
            'ç»¼åˆä¿¡å·': overall_signal,
            'ä¿¡å·ç½®ä¿¡åº¦': f"{confidence:.2f}"
        })

    signals_df = pd.DataFrame(signals)
    # æ’åºï¼Œä¼˜å…ˆæ˜¾ç¤ºå¼ºçƒˆä¹°å…¥ï¼Œç„¶åæ˜¯é•¿æœŸä¹°å…¥ï¼ŒçŸ­æœŸä¹°å…¥ï¼Œå†æŒ‰é¢„æµ‹æ”¶ç›Šæ’åº
    signal_order = {"å¼ºçƒˆä¹°å…¥": 4, "é•¿æœŸä¹°å…¥": 3, "çŸ­æœŸä¹°å…¥": 2, "æŒæœ‰": 1, "å–å‡º": 0}
    signals_df['signal_rank'] = signals_df['ç»¼åˆä¿¡å·'].map(signal_order)

    # å°†é¢„æµ‹æ”¶ç›Šè½¬æ¢ä¸ºæ•°å€¼ä»¥ä¾¿æ’åº
    signals_df[f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num'] = signals_df[f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š'].str.replace('%', '').apply(safe_float) / 100
    signals_df[f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num'] = signals_df[f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š'].str.replace('%', '').apply(safe_float) / 100

    signals_df = signals_df.sort_values(by=['signal_rank', f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num', f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num'], ascending=[False, False, False])

    # åˆ é™¤è¾…åŠ©åˆ—
    signals_df = signals_df.drop(columns=['signal_rank', f'é¢„æµ‹{SHORT_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num', f'é¢„æµ‹{LONG_TERM_FUTURE_DAYS}æ—¥æ”¶ç›Š_num'])

    return signals_df

def perform_association_rule_mining(df):
    """
    ä½¿ç”¨å…³è”è§„åˆ™æŒ–æ˜æ¥å‘ç°è‹æ°é‡åŒ–ç­–ç•¥æ¡ä»¶ä¸é«˜æ¶¨å¹…ä¹‹é—´çš„å…³ç³»ã€‚
    """
    print("\n4. æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜...")

    # å‡†å¤‡æ•°æ®ï¼šå°†ç‰¹å¾å’Œç›®æ ‡å˜é‡äºŒå€¼åŒ–
    data_for_ar = []
    # ç¡®ä¿æ‰€æœ‰æ•°å€¼åˆ—éƒ½æ˜¯floatç±»å‹
    numeric_cols = ['æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
                    'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜',
                    'SMA_5', 'SMA_10', 'SMA_20', 'EMA_5', 'EMA_10', 'EMA_20',
                    'MACD', 'MACD_Signal', 'MACD_Diff', 'RSI', 'K', 'D', 'J',
                    'BB_Upper', 'BB_Lower', 'BB_Width', 'ATR', 'Volume_SMA_5',
                    'Volume_SMA_10', 'Daily_Return', 'Log_Return', 'Price_Pos_F', 'Price_Pos_G']

    for col in numeric_cols:
        if col in df.columns:
            df[col] = df[col].apply(safe_float)

    for _, row in df.iterrows():
        items = []

        # è‹æ°ç­–ç•¥ç‰¹å¾
        f_condition = safe_float(row.get('Price_Pos_F'))
        g_condition = safe_float(row.get('Price_Pos_G'))
        profit = safe_float(row.get('å½’å±å‡€åˆ©æ¶¦'))
        turnover = safe_float(row.get('å®é™…æ¢æ‰‹%'))
        cap = safe_float(row.get('æ€»å¸‚å€¼'))
        pe_ratio = safe_float(row.get('å¸‚ç›ˆç‡(åŠ¨)'))

        if pd.notna(f_condition):
            items.append("F_ä»·æ ¼ä½ç½®_æ»¡è¶³" if f_condition == 1 else "F_ä»·æ ¼ä½ç½®_ä¸æ»¡è¶³")
        if pd.notna(g_condition):
            items.append("G_æ¶¨å¹…ä½ç½®_æ»¡è¶³" if g_condition == 1 else "G_æ¶¨å¹…ä½ç½®_ä¸æ»¡è¶³")
        if pd.notna(profit):
            items.append("H_å‡€åˆ©æ¶¦_é«˜" if profit >= 0.3 else "H_å‡€åˆ©æ¶¦_ä½") # 0.3äº¿
        if pd.notna(turnover):
            items.append("I_æ¢æ‰‹ç‡_ä½" if turnover <= 20 else "I_æ¢æ‰‹ç‡_é«˜")
        if pd.notna(cap):
            items.append("J_å¸‚å€¼_å¤§" if cap >= 300 else "J_å¸‚å€¼_å°") # 300äº¿
        if pd.notna(pe_ratio):
            items.append("K_å¸‚ç›ˆç‡_ä½" if pe_ratio > 0 and pe_ratio <= 30 else "K_å¸‚ç›ˆç‡_é«˜") # 0-30ä¸ºä½

        # ç›®æ ‡å˜é‡ï¼šé«˜æ¶¨å¹… (ä¾‹å¦‚ï¼Œæ¶¨å¹… > 2%)
        change = safe_float(row.get('æ¶¨å¹…%'))
        if pd.notna(change) and change > 2.0: # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
            items.append("é«˜æ¶¨å¹…")
        else:
            items.append("ä½æ¶¨å¹…")

        if items: # ç¡®ä¿æœ‰æœ‰æ•ˆé¡¹
            data_for_ar.append(items)

    if not data_for_ar:
        print("   âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ã€‚")
        return

    te = TransactionEncoder()
    te_ary = te.fit(data_for_ar).transform(data_for_ar)
    df_ar = pd.DataFrame(te_ary, columns=te.columns_)

    # æŸ¥æ‰¾é¢‘ç¹é¡¹é›†
    frequent_itemsets = apriori(df_ar, min_support=AR_MIN_SUPPORT, use_colnames=True)
    if frequent_itemsets.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·å°è¯•é™ä½ min_supportã€‚")
        return

    # ç”Ÿæˆå…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=AR_MIN_THRESHOLD)

    if rules.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°æœ‰æ„ä¹‰çš„å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½ min_threshold æˆ–æ£€æŸ¥æ•°æ®ã€‚")
        return

    # ç­›é€‰å¹¶æ‰“å°ä¸â€œé«˜æ¶¨å¹…â€ç›¸å…³çš„è§„åˆ™
    high_return_rules = rules[rules['consequents'].apply(lambda x: 'é«˜æ¶¨å¹…' in x)]
    high_return_rules = high_return_rules.sort_values(by=['lift', 'confidence'], ascending=False)

    print("\n   å‘ç°ä»¥ä¸‹ä¸ 'é«˜æ¶¨å¹…' ç›¸å…³çš„å…³è”è§„åˆ™ (æŒ‰ Lift é™åº):")
    if high_return_rules.empty:
        print("   æœªæ‰¾åˆ°ç›´æ¥å¯¼è‡´ 'é«˜æ¶¨å¹…' çš„å…³è”è§„åˆ™ã€‚")
    else:
        for i, rule in high_return_rules.head(10).iterrows(): # åªæ˜¾ç¤ºå‰10æ¡
            antecedent_str = ', '.join(list(rule['antecedents']))
            consequent_str = ', '.join(list(rule['consequents']))
            print(f"   è§„åˆ™ {i+1}: {antecedent_str} => {consequent_str}")
            print(f"     æ”¯æŒåº¦ (Support): {rule['support']:.4f}")
            print(f"     ç½®ä¿¡åº¦ (Confidence): {rule['confidence']:.4f}")
            print(f"     æå‡åº¦ (Lift): {rule['lift']:.4f}")
            print("-" * 40)

    print("\n   å…³è”è§„åˆ™æŒ–æ˜å®Œæˆã€‚è¿™äº›è§„åˆ™å¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ´å¯Ÿã€‚")

def simplified_backtest(signals_df, initial_capital=100000):
    """
    ç®€åŒ–çš„å›æµ‹åŠŸèƒ½ï¼Œæ¨¡æ‹Ÿæ ¹æ®ä¿¡å·è¿›è¡Œäº¤æ˜“ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ–çš„ç‰ˆæœ¬ï¼Œä¸è€ƒè™‘æ»‘ç‚¹ã€ä½£é‡‘ã€èµ„é‡‘ç®¡ç†ç­‰å¤æ‚å› ç´ ã€‚
    ä»…ç”¨äºåˆæ­¥è¯„ä¼°ç­–ç•¥çš„æ½œåœ¨æ–¹å‘ã€‚
    """
    print("\n5. ç®€åŒ–çš„ç­–ç•¥å›æµ‹ (ä»…ä¾›å‚è€ƒï¼Œä¸å«çœŸå®äº¤æ˜“ç»†èŠ‚)...")

    # å‡è®¾æˆ‘ä»¬åªå…³æ³¨â€œå¼ºçƒˆä¹°å…¥â€å’Œâ€œå–å‡ºâ€ä¿¡å·
    buy_signals = signals_df[signals_df['ç»¼åˆä¿¡å·'] == 'å¼ºçƒˆä¹°å…¥']
    sell_signals = signals_df[signals_df['ç»¼åˆä¿¡å·'] == 'å–å‡º']

    if buy_signals.empty and sell_signals.empty:
        print("   æ²¡æœ‰ç”Ÿæˆä¹°å…¥æˆ–å–å‡ºä¿¡å·ï¼Œæ— æ³•è¿›è¡Œå›æµ‹ã€‚")
        return

    print(f"   åˆå§‹èµ„é‡‘: {initial_capital:.2f} å…ƒ")
    current_capital = initial_capital
    positions = {} # {è‚¡ç¥¨ä»£ç : æŒæœ‰æ•°é‡}
    trade_log = []

    # æ¨¡æ‹Ÿäº¤æ˜“ (éå¸¸ç²—ç•¥çš„é€»è¾‘)
    # å‡è®¾æˆ‘ä»¬åªåœ¨ä¿¡å·ç”Ÿæˆå½“å¤©è¿›è¡Œäº¤æ˜“ï¼Œå¹¶ä¸”åªä¹°å…¥å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨ï¼Œå–å‡ºå–å‡ºä¿¡å·çš„è‚¡ç¥¨
    # å¹¶ä¸”å‡è®¾æˆ‘ä»¬èƒ½ä»¥æœ€æ–°ä»·æˆäº¤

    # å–å‡ºæ“ä½œ (å…ˆå¤„ç†å–å‡ºï¼Œé‡Šæ”¾èµ„é‡‘)
    for _, row in sell_signals.iterrows():
        code = row['ä»£ç ']
        if code in positions and positions[code] > 0:
            latest_price = safe_float(row['æœ€æ–°ä»·'])
            if pd.notna(latest_price) and latest_price > 0:
                profit_loss = (latest_price - positions[code]['avg_price']) * positions[code]['quantity']
                current_capital += latest_price * positions[code]['quantity']
                trade_log.append(f"   å–å‡º {code} ({row['åç§°']}): æ•°é‡ {positions[code]['quantity']}, ä»·æ ¼ {latest_price:.2f}, ç›ˆäº {profit_loss:.2f}")
                del positions[code]
            else:
                trade_log.append(f"   å–å‡º {code} ({row['åç§°']}): ä»·æ ¼æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•æ‰§è¡Œã€‚")

    # ä¹°å…¥æ“ä½œ (ç”¨å‰©ä½™èµ„é‡‘ä¹°å…¥å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨)
    if not buy_signals.empty:
        # å¹³å‡åˆ†é…èµ„é‡‘ç»™æ‰€æœ‰å¼ºçƒˆä¹°å…¥çš„è‚¡ç¥¨
        num_buy_stocks = len(buy_signals)
        if num_buy_stocks > 0:
            capital_per_stock = current_capital / num_buy_stocks
            for _, row in buy_signals.iterrows():
                code = row['ä»£ç ']
                latest_price = safe_float(row['æœ€æ–°ä»·'])
                if pd.notna(latest_price) and latest_price > 0:
                    quantity = int(capital_per_stock / latest_price / 100) * 100 # è´­ä¹°100è‚¡çš„æ•´æ•°å€
                    if quantity > 0:
                        cost = quantity * latest_price
                        current_capital -= cost
                        positions[code] = {'quantity': quantity, 'avg_price': latest_price}
                        trade_log.append(f"   ä¹°å…¥ {code} ({row['åç§°']}): æ•°é‡ {quantity}, ä»·æ ¼ {latest_price:.2f}, æˆæœ¬ {cost:.2f}")
                else:
                    trade_log.append(f"   ä¹°å…¥ {code} ({row['åç§°']}): ä»·æ ¼æ•°æ®ç¼ºå¤±ï¼Œæ— æ³•æ‰§è¡Œã€‚")

    # è®¡ç®—å½“å‰æ€»èµ„äº§
    current_portfolio_value = current_capital
    for code, pos in positions.items():
        # å°è¯•è·å–å½“å‰æŒä»“è‚¡ç¥¨çš„æœ€æ–°ä»·æ ¼ (è¿™é‡Œç®€åŒ–ä¸ºä½¿ç”¨ä¿¡å·DFä¸­çš„æœ€æ–°ä»·)
        latest_price_in_df = safe_float(signals_df[signals_df['ä»£ç '] == code]['æœ€æ–°ä»·'].iloc[0])
        if pd.notna(latest_price_in_df):
            current_portfolio_value += pos['quantity'] * latest_price_in_df
        else:
            # å¦‚æœæœ€æ–°ä»·ç¼ºå¤±ï¼Œåˆ™ç”¨ä¹°å…¥æ—¶çš„å¹³å‡ä»·ä¼°ç®—
            current_portfolio_value += pos['quantity'] * pos['avg_price']


    total_return = (current_portfolio_value - initial_capital) / initial_capital * 100

    print("\n   --- äº¤æ˜“æ—¥å¿— ---")
    for log in trade_log:
        print(log)
    print("\n   --- å›æµ‹ç»“æœ ---")
    print(f"   æœŸæœ«æ€»èµ„äº§: {current_portfolio_value:.2f} å…ƒ")
    print(f"   æ€»æ”¶ç›Šç‡: {total_return:.2f}%")
    print(f"   å½“å‰æŒä»“è‚¡ç¥¨æ•°é‡: {len(positions)}")
    if positions:
        print("   å½“å‰æŒä»“æ˜ç»†:")
        for code, pos in positions.items():
            print(f"     - {code}: æ•°é‡ {pos['quantity']}, å‡ä»· {pos['avg_price']:.2f}")
    print("   å›æµ‹ç»“æŸã€‚")


# ==================== ä¸»ç¨‹åº ====================

def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆæœºå™¨å­¦ä¹ ä¸å¤šç­–ç•¥)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–å¹¶æ•´åˆæ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ® (å®æ—¶ + å†å²)...")

    # è·å–æ‰€æœ‰Aè‚¡å®æ—¶æ•°æ®
    df_realtime = pd.DataFrame()
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°', 'æ¶¨è·Œå¹…': 'æ¶¨å¹…%', 'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼': 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦': 'å½’å±å‡€åˆ©æ¶¦'
        }, inplace=True)
        df_realtime['åŸå§‹ä»£ç '] = df_realtime['ä»£ç '].copy()
        df_realtime['ä»£ç '] = df_realtime['ä»£ç '].apply(lambda x: f'= "{str(x)}"')

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨
        required_cols = ['ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%',
                         'æ‰€å±è¡Œä¸š', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'åŸå§‹ä»£ç ']
        for col in required_cols:
            if col not in df_realtime.columns:
                df_realtime[col] = np.nan

    except Exception as e:
        print(f"   âŒ å®æ—¶è·å–å¤±è´¥: {e}")
        print("   å°†å°è¯•ä»å‚è€ƒæ•°æ®è¡¥å……ã€‚")
        df_realtime = pd.DataFrame(columns=required_cols) # åˆ›å»ºç©ºDataFrameä»¥é¿å…åç»­é”™è¯¯

    # è·å–å†å²æ•°æ®å¹¶è®¡ç®—æŒ‡æ ‡
    all_historical_data = []
    end_date = datetime.now().strftime('%Y%m%d')
    start_date = (datetime.now() - timedelta(days=HISTORY_DAYS)).strftime('%Y%m%d')

    print(f"   è·å–å†å²æ—¥Kçº¿æ•°æ® ({start_date} è‡³ {end_date})...")
    # ä½¿ç”¨å®æ—¶æ•°æ®ä¸­çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
    stock_codes_to_fetch = df_realtime['åŸå§‹ä»£ç '].tolist()
    if not stock_codes_to_fetch: # å¦‚æœå®æ—¶æ•°æ®è·å–å¤±è´¥ï¼Œå°è¯•ä»å‚è€ƒæ•°æ®è·å–ä»£ç 
        try:
            ref_df_path = 'å‚è€ƒæ•°æ®/Table.xls'
            if os.path.exists(ref_df_path):
                ref_df_codes = pd.read_csv(ref_df_path, sep='\t', encoding='gbk', dtype=str)
                stock_codes_to_fetch = ref_df_codes['ä»£ç '].str.replace('= "', '').str.replace('"', '').tolist()
                print(f"   ä»å‚è€ƒæ•°æ®è·å–äº† {len(stock_codes_to_fetch)} ä¸ªè‚¡ç¥¨ä»£ç ã€‚")
        except Exception as e:
            print(f"   âŒ æ— æ³•ä»å‚è€ƒæ•°æ®è·å–è‚¡ç¥¨ä»£ç : {e}")
            print("   è¯·ç¡®ä¿ 'å‚è€ƒæ•°æ®/Table.xls' å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®ã€‚")
            return

    # é™åˆ¶è·å–æ•°é‡ï¼Œé¿å…APIé™åˆ¶æˆ–æ—¶é—´è¿‡é•¿
    # stock_codes_to_fetch = stock_codes_to_fetch[:100] # è°ƒè¯•æ—¶å¯ä»¥é™åˆ¶æ•°é‡

    for i, code in enumerate(stock_codes_to_fetch):
        if i % 500 == 0: # æ¯500åªè‚¡ç¥¨æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f"     æ­£åœ¨è·å–ç¬¬ {i}/{len(stock_codes_to_fetch)} åªè‚¡ç¥¨çš„å†å²æ•°æ®...")
        hist_df = get_stock_data(code, start_date, end_date)
        if not hist_df.empty:
            hist_df['åŸå§‹ä»£ç '] = code
            all_historical_data.append(hist_df)

    if not all_historical_data:
        print("   âŒ æœªèƒ½è·å–ä»»ä½•è‚¡ç¥¨çš„å†å²æ•°æ®ï¼Œæ— æ³•è¿›è¡Œæ¨¡å‹è®­ç»ƒå’Œä¿¡å·ç”Ÿæˆã€‚")
        return

    df_historical = pd.concat(all_historical_data)
    print(f"   âœ… æˆåŠŸè·å– {len(df_historical)} æ¡å†å²æ•°æ®ã€‚")

    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    print("   è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
    df_historical_with_indicators = df_historical.groupby('åŸå§‹ä»£ç ').apply(calculate_technical_indicators)
    df_historical_with_indicators.reset_index(level=0, inplace=True) # æ¢å¤åŸå§‹ä»£ç åˆ—
    print("   âœ… æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å®Œæˆã€‚")

    # åˆå¹¶å®æ—¶æ•°æ®å’Œå†å²æ•°æ® (ä»¥å†å²æ•°æ®ä¸ºä¸»ï¼Œè¡¥å……å®æ—¶æ•°æ®)
    # æ‰¾åˆ°æœ€æ–°çš„å†å²æ•°æ®æ—¥æœŸ
    latest_hist_date = df_historical_with_indicators.index.max()
    print(f"   æœ€æ–°å†å²æ•°æ®æ—¥æœŸ: {latest_hist_date.strftime('%Y-%m-%d')}")

    # æå–å®æ—¶æ•°æ®ä¸­æœ€æ–°çš„æ•°æ®ï¼Œå¹¶å°†å…¶æ—¥æœŸè®¾ç½®ä¸ºæœ€æ–°å†å²æ•°æ®æ—¥æœŸ+1å¤© (æ¨¡æ‹Ÿä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥)
    # æˆ–è€…æ›´å‡†ç¡®åœ°ï¼Œå°†å®æ—¶æ•°æ®ä½œä¸ºå½“æ—¥æ•°æ®ï¼Œå¹¶ç¡®ä¿å…¶æŒ‡æ ‡è®¡ç®—æ­£ç¡®
    df_current_day_data = df_realtime.copy()
    df_current_day_data['æ—¥æœŸ'] = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) # è®¾ç½®ä¸ºä»Šæ—¥æ—¥æœŸ
    df_current_day_data.set_index('æ—¥æœŸ', inplace=True)

    # å°†å®æ—¶æ•°æ®ä¸­çš„åˆ—åæ˜ å°„åˆ°å†å²æ•°æ®ä¸­çš„åˆ—åï¼Œä»¥ä¾¿è®¡ç®—æŒ‡æ ‡
    df_current_day_data.rename(columns={
        'æœ€æ–°': 'æ”¶ç›˜', 'æ¶¨å¹…%': 'æ¶¨è·Œå¹…', 'å®é™…æ¢æ‰‹%': 'æ¢æ‰‹ç‡'
    }, inplace=True)

    # ç¡®ä¿å®æ—¶æ•°æ®åŒ…å«æ‰€æœ‰è®¡ç®—æŒ‡æ ‡æ‰€éœ€çš„åˆ—
    for col in ['å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…', 'æ¶¨è·Œé¢', 'æ¢æ‰‹ç‡']:
        if col not in df_current_day_data.columns:
            df_current_day_data[col] = np.nan

    # å¡«å……æˆäº¤é‡å’Œæˆäº¤é¢çš„ç¼ºå¤±å€¼ï¼Œé¿å…æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å¤±è´¥
    df_current_day_data['æˆäº¤é‡'] = df_current_day_data['æˆäº¤é‡'].fillna(0)
    df_current_day_data['æˆäº¤é¢'] = df_current_day_data['æˆäº¤é¢'].fillna(0)

    # å°†å®æ—¶æ•°æ®è¿½åŠ åˆ°å†å²æ•°æ®ä¸­ï¼Œç„¶åé‡æ–°è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
    # è¿™æ ·åšå¯ä»¥ç¡®ä¿å®æ—¶æ•°æ®çš„æŠ€æœ¯æŒ‡æ ‡æ˜¯åŸºäºæœ€æ–°çš„å†å²æ•°æ®è®¡ç®—çš„
    df_combined = pd.concat([df_historical_with_indicators, df_current_day_data.drop(columns=['ä»£ç ', 'åç§°', 'æ‰€å±è¡Œä¸š', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'Unnamed: 16'], errors='ignore')])
    df_combined_with_indicators = df_combined.groupby('åŸå§‹ä»£ç ').apply(calculate_technical_indicators)
    df_combined_with_indicators.reset_index(level=0, inplace=True)

    # æå–ä»Šå¤©çš„æœ€æ–°æ•°æ® (ç”¨äºä¿¡å·ç”Ÿæˆ)
    df_today_features = df_combined_with_indicators[df_combined_with_indicators.index == datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)].copy()

    # å°†å®æ—¶æ•°æ®ä¸­çš„åç§°ã€è¡Œä¸šã€å¸‚ç›ˆç‡ã€æ€»å¸‚å€¼ã€å½’å±å‡€åˆ©æ¶¦ç­‰ä¿¡æ¯åˆå¹¶å›df_today_features
    # ä½¿ç”¨åŸå§‹ä»£ç ä½œä¸ºé”®è¿›è¡Œåˆå¹¶
    df_today_features = df_today_features.merge(
        df_realtime[['åŸå§‹ä»£ç ', 'åç§°', 'æ‰€å±è¡Œä¸š', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']],
        on='åŸå§‹ä»£ç ',
        how='left'
    )
    # ç¡®ä¿'æœ€æ–°'ä»·æ˜¯å®æ—¶æ•°æ®ä¸­çš„æœ€æ–°ä»·ï¼Œè€Œä¸æ˜¯å†å²æ”¶ç›˜ä»·
    df_today_features['æœ€æ–°'] = df_today_features['æ”¶ç›˜'] # å®æ—¶æ•°æ®ä¸­'æœ€æ–°'ä»·è¢«æ˜ å°„ä¸º'æ”¶ç›˜'
    df_today_features['æ¶¨å¹…%'] = df_today_features['æ¶¨è·Œå¹…'] # å®æ—¶æ•°æ®ä¸­'æ¶¨å¹…%'è¢«æ˜ å°„ä¸º'æ¶¨è·Œå¹…'


    # ä¿å­˜æ•´åˆåçš„æ•°æ® (å¯é€‰)
    # df_combined_with_indicators.to_csv('è¾“å‡ºæ•°æ®/æ•´åˆå†å²ä¸å®æ—¶æ•°æ®.csv', encoding='utf-8-sig')
    # print("   âœ… æ•´åˆåçš„å†å²ä¸å®æ—¶æ•°æ®å·²ä¿å­˜ã€‚")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ ==========
    print("\n2. è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹...")

    # è®­ç»ƒçŸ­æœŸæ¨¡å‹
    short_term_model, short_term_scaler, short_term_features, short_term_indices = train_ml_model(df_combined_with_indicators.copy(), SHORT_TERM_FUTURE_DAYS)

    # è®­ç»ƒé•¿æœŸæ¨¡å‹
    long_term_model, long_term_scaler, long_term_features, long_term_indices = train_ml_model(df_combined_with_indicators.copy(), LONG_TERM_FUTURE_DAYS)

    if short_term_model is None or long_term_model is None:
        print("   âŒ è‡³å°‘ä¸€ä¸ªæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ä¿¡å·ç”Ÿæˆã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆäº¤æ˜“ä¿¡å· ==========
    signals_df = generate_trading_signals(
        df_today_features,
        short_term_model, short_term_scaler, short_term_features, short_term_indices,
        long_term_model, long_term_scaler, long_term_features, long_term_indices
    )

    # ä¿å­˜äº¤æ˜“ä¿¡å·
    output_file_signals = 'è¾“å‡ºæ•°æ®/äº¤æ˜“ä¿¡å·.csv'
    signals_df.to_csv(output_file_signals, index=False, encoding='utf-8-sig')
    print(f"\nâœ… äº¤æ˜“ä¿¡å·å·²ä¿å­˜: {output_file_signals}")
    print("\nğŸ¯ ä»Šæ—¥äº¤æ˜“ä¿¡å·æ¦‚è§ˆ (å‰20å):")
    print(signals_df.head(20).to_string())

    # ========== ç¬¬å››æ­¥ï¼šå…³è”è§„åˆ™æŒ–æ˜ ==========
    # å¯¹æ‰€æœ‰å†å²æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ï¼Œä»¥å‘ç°æ™®éè§„å¾‹
    perform_association_rule_mining(df_historical_with_indicators.copy())

    # ========== ç¬¬äº”æ­¥ï¼šç®€åŒ–çš„å›æµ‹ ==========
    # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ä¸€ä¸ªéå¸¸ç®€åŒ–çš„å›æµ‹ç¤ºä¾‹ï¼Œå®é™…å›æµ‹éœ€è¦æ›´å¤æ‚çš„å†å²æ•°æ®å’Œäº¤æ˜“æ¨¡æ‹Ÿ
    # è¿™é‡Œçš„å›æµ‹æ˜¯åŸºäºå½“å‰ç”Ÿæˆçš„ä¿¡å·ï¼Œæ¨¡æ‹Ÿåœ¨ä»Šå¤©è¿›è¡Œäº¤æ˜“ï¼Œå¹¶å‡è®¾è¿™äº›äº¤æ˜“æ˜¯æˆåŠŸçš„ã€‚
    # çœŸæ­£çš„å›æµ‹éœ€è¦å°†æ¨¡å‹åº”ç”¨äºå†å²æ¯ä¸€å¤©çš„æ•°æ®ï¼Œå¹¶æ¨¡æ‹Ÿäº¤æ˜“è¿‡ç¨‹ã€‚
    simplified_backtest(signals_df.copy())

    print("\n" + "="*60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
