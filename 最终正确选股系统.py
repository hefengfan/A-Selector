#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - æ ¹æ®æ¯å¤©å®æ—¶æ•°æ®ç­›é€‰
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆæ›´ç²¾å¯†çš„ç®—æ³• (XGBoost) è¿›è¡Œç²¾å‡†è¯„åˆ†
æ–°å¢ï¼š
1. ä½¿ç”¨ Optuna è¿›è¡Œ XGBoost è¶…å‚æ•°ä¼˜åŒ–ï¼Œæå‡è¯„åˆ†ç²¾åº¦å’ŒåŒºåˆ†åº¦ã€‚
2. å¼•å…¥å…³è”è§„åˆ™æŒ–æ˜ï¼Œåˆ†æå“ªäº›æ¡ä»¶ç»„åˆæ›´å®¹æ˜“äº§ç”Ÿé«˜æ”¶ç›Šï¼Œæä¾›ç­–ç•¥æ´å¯Ÿã€‚
3. ä¼˜åŒ–æ•°æ®å¤„ç†å’Œè¾“å‡ºå±•ç¤ºã€‚
4. ä¼˜åŒ–ä»£ç å‡†ç¡®æ€§ã€è´¨é‡å’Œæ•ˆç‡ã€‚
5. ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç¥ç»ç½‘ç»œçš„ç›®æ ‡å˜é‡ï¼Œæé«˜æ¨¡å‹å‡†ç¡®æ€§ã€‚
6. å¼•å…¥æ›´å¤šæŠ€æœ¯å’Œè´¢åŠ¡æŒ‡æ ‡ä½œä¸ºæ¨¡å‹ç‰¹å¾ã€‚
7. æ ¹æ®æ¨¡å‹è¯„åˆ†å’Œè§„åˆ™ç”Ÿæˆæ˜ç¡®çš„â€œçŸ­æœŸä¹°å…¥â€ã€â€œé•¿æœŸä¹°å…¥â€å’Œâ€œè§„é¿â€ç­–ç•¥ä¿¡å·ã€‚
8. æä¾›æ¨¡å‹ç‰¹å¾é‡è¦æ€§è¯„ä¼°ã€‚
9. ç­–ç•¥æ¨èç»™å‡ºå¤šç‰¹å¾ç»„åˆï¼Œæ›´å…·æŒ‡å¯¼æ€§ã€‚
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥æœºå™¨å­¦ä¹ ç›¸å…³åº“
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb # ä½¿ç”¨XGBoost

# å¯¼å…¥ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–
import optuna

# å¯¼å…¥ mlxtend è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

def safe_float(value_str):
    """å®‰å…¨åœ°å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç† '--' å’Œå…¶ä»–éæ•°å­—æƒ…å†µ"""
    try:
        if isinstance(value_str, (int, float)):
            return float(value_str)
        s = str(value_str).strip()
        if s == '--' or not s:
            return np.nan
        # å¤„ç†å¸¦å•ä½çš„å­—ç¬¦ä¸²
        if 'äº¿' in s:
            return float(s.replace('äº¿', '')) * 100000000 # 1äº¿ = 10^8
        if 'ä¸‡äº¿' in s:
            return float(s.replace('ä¸‡äº¿', '')) * 1000000000000 # 1ä¸‡äº¿ = 10^12
        if 'ä¸‡' in s:
            return float(s.replace('ä¸‡', '')) * 10000 # 1ä¸‡ = 10^4
        if '%' in s: # å¤„ç†ç™¾åˆ†æ¯”
            return float(s.replace('%', ''))
        return float(s)
    except ValueError:
        return np.nan

def get_model_features(row):
    """
    æ ¹æ®è‹æ°é‡åŒ–ç­–ç•¥å’Œæ›´å¤šæŒ‡æ ‡è®¡ç®—ç‰¹å¾å€¼ï¼Œç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒã€‚
    è¿”å›ä¸€ä¸ªåŒ…å«æ•°å€¼ç‰¹å¾çš„åˆ—è¡¨ã€‚
    """
    features = []

    # æ ¸å¿ƒè‹æ°ç­–ç•¥ç‰¹å¾ (0æˆ–1)
    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶
    low = safe_float(row.get('æœ€ä½'))
    ma60 = safe_float(row.get('60æ—¥å‡ä»·'))
    ma20 = safe_float(row.get('20æ—¥å‡ä»·'))
    current = safe_float(row.get('æœ€æ–°'))

    f_condition = 0
    if pd.notna(low) and pd.notna(ma60) and ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
        f_condition = 1
    elif pd.notna(current) and pd.notna(ma20) and ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
        f_condition = 1
    features.append(f_condition) # Feature 0: F_ä»·æ ¼ä½ç½®

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½®
    change = safe_float(row.get('æ¶¨å¹…%'))
    high = safe_float(row.get('æœ€é«˜'))
    # low = safe_float(row.get('æœ€ä½')) # lowå·²åœ¨ä¸Šé¢è·å–

    g_condition = 0
    if pd.notna(change) and change >= 5.0 and pd.notna(current) and pd.notna(high) and pd.notna(low):
        if (high - low) > 0:
            threshold = high - (high - low) * 0.30
            if current >= threshold:
                g_condition = 1
        elif current == high: # å¦‚æœæœ€é«˜æœ€ä½ç›¸åŒï¼Œä¸”æ¶¨å¹…>=5ï¼Œä¹Ÿç®—æ»¡è¶³
            g_condition = 1
    features.append(g_condition) # Feature 1: G_æ¶¨å¹…ä½ç½®

    # æ›´å¤šæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ (æ•°å€¼)
    # ä»·æ ¼ç›¸å¯¹å‡çº¿ä½ç½®
    features.append(current / ma20 if pd.notna(current) and pd.notna(ma20) and ma20 > 0 else 1.0) # Feature 2: ä»·æ ¼_vs_MA20
    features.append(current / ma60 if pd.notna(current) and pd.notna(ma60) and ma60 > 0 else 1.0) # Feature 3: ä»·æ ¼_vs_MA60
    features.append(ma20 / ma60 if pd.notna(ma20) and pd.notna(ma60) and ma60 > 0 else 1.0) # Feature 4: MA20_vs_MA60

    # æ¯æ—¥æ³¢åŠ¨å¹…åº¦
    daily_range_ratio = (high - low) / current if pd.notna(high) and pd.notna(low) and pd.notna(current) and current > 0 else 0.0
    features.append(daily_range_ratio) # Feature 5: æ—¥å†…æ³¢åŠ¨ç‡

    # æ”¶ç›˜ä»·åœ¨æ—¥å†…åŒºé—´çš„ä½ç½® (è¶Šæ¥è¿‘æœ€é«˜ä»·è¶Šå¼º)
    close_pos_in_range = (current - low) / (high - low) if pd.notna(current) and pd.notna(low) and pd.notna(high) and (high - low) > 0 else 0.5
    features.append(close_pos_in_range) # Feature 6: æ”¶ç›˜ä»·_æ—¥å†…ä½ç½®

    # è´¢åŠ¡ä¸å¸‚åœºæŒ‡æ ‡ç‰¹å¾ (æ•°å€¼)
    profit = safe_float(row.get('å½’å±å‡€åˆ©æ¶¦')) # å•ä½æ˜¯äº¿ï¼Œè¿™é‡Œä¿æŒåŸå€¼
    features.append(profit if pd.notna(profit) else 0) # Feature 7: å½’å±å‡€åˆ©æ¶¦

    turnover = safe_float(row.get('å®é™…æ¢æ‰‹%'))
    features.append(turnover if pd.notna(turnover) else 0) # Feature 8: å®é™…æ¢æ‰‹ç‡

    market_cap = safe_float(row.get('æ€»å¸‚å€¼')) # å•ä½æ˜¯äº¿ï¼Œè¿™é‡Œä¿æŒåŸå€¼
    features.append(market_cap if pd.notna(market_cap) else 0) # Feature 9: æ€»å¸‚å€¼

    pe_ratio = safe_float(row.get('å¸‚ç›ˆç‡(åŠ¨)'))
    features.append(pe_ratio if pd.notna(pe_ratio) else 1000) # Feature 10: å¸‚ç›ˆç‡(åŠ¨) (ç¼ºå¤±æ—¶ç»™ä¸€ä¸ªå¤§å€¼ï¼Œè¡¨ç¤ºé«˜ä¼°)

    # æˆäº¤é¢ (å•ä½æ˜¯äº¿)
    turnover_value = safe_float(row.get('æˆäº¤é¢')) # akshareè¿”å›çš„æ˜¯äº¿ï¼Œè¿™é‡Œä¿æŒåŸå€¼
    features.append(turnover_value if pd.notna(turnover_value) else 0) # Feature 11: æˆäº¤é¢

    # æ¶¨å¹…%
    features.append(change if pd.notna(change) else 0) # Feature 12: æ¶¨å¹…%

    return features

def objective(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ–ç›®æ ‡å‡½æ•° for XGBoost
    """
    param = {
        'objective': 'reg:squarederror',
        'eval_metric': 'rmse',
        'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),
        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),
        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),
        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
        'seed': 42,
        'n_jobs': -1,
    }

    if param['booster'] == 'gbtree' or param['booster'] == 'dart':
        param['max_depth'] = trial.suggest_int('max_depth', 3, 9)
        param['eta'] = trial.suggest_loguniform('eta', 1e-8, 1.0)
        param['gamma'] = trial.suggest_loguniform('gamma', 1e-8, 1.0)
        param['grow_policy'] = trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide'])

    if param['booster'] == 'dart':
        param['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])
        param['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])
        param['rate_drop'] = trial.suggest_uniform('rate_drop', 0.0, 1.0)
        param['skip_drop'] = trial.suggest_uniform('skip_drop', 0.0, 1.0)

    model = xgb.XGBRegressor(**param)
    model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              early_stopping_rounds=50, # å¢åŠ è€å¿ƒ
              verbose=False)

    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    return mse # Optuna é»˜è®¤æœ€å°åŒ–ç›®æ ‡

def train_xgboost_model(df):
    """
    è®­ç»ƒ XGBoost æ¨¡å‹ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†ï¼Œä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–ã€‚
    ä½¿ç”¨å¤åˆè´¨é‡è¯„åˆ†ä½œä¸ºç›®æ ‡å˜é‡ã€‚
    """
    print("\n   å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X = []
    for _, row in df.iterrows():
        features = get_model_features(row)
        X.append(features)

    X = np.array(X)

    # æå–ç”¨äºè®¡ç®—è´¨é‡è¯„åˆ†çš„åˆ— (åŸå§‹æ•°å€¼ï¼Œæœªæ ¼å¼åŒ–)
    change = df['æ¶¨å¹…%'].apply(safe_float)
    profit = df['å½’å±å‡€åˆ©æ¶¦'].apply(safe_float)
    turnover = df['å®é™…æ¢æ‰‹%'].apply(safe_float)
    market_cap = df['æ€»å¸‚å€¼'].apply(safe_float)
    pe_ratio = df['å¸‚ç›ˆç‡(åŠ¨)'].apply(safe_float)
    current_price = df['æœ€æ–°'].apply(safe_float)
    ma20 = df['20æ—¥å‡ä»·'].apply(safe_float)

    # å½’ä¸€åŒ–å„ä¸ªæŒ‡æ ‡ (ä½¿ç”¨ min-max å½’ä¸€åŒ–ï¼Œå¤„ç†NaN)
    # å¯¹äºæ¶¨å¹…ã€å‡€åˆ©æ¶¦ã€å¸‚å€¼ï¼Œè¶Šå¤§è¶Šå¥½
    change_norm = (change - change.min()) / (change.max() - change.min()) if change.max() != change.min() else 0
    profit_norm = (profit - profit.min()) / (profit.max() - profit.min()) if profit.max() != profit.min() else 0
    market_cap_norm = (market_cap - market_cap.min()) / (market_cap.max() - market_cap.min()) if market_cap.max() != market_cap.min() else 0

    # æ¢æ‰‹ç‡ï¼šé€‚ä¸­æœ€å¥½ï¼Œåç¦»0.5è¶Šè¿œè¶Šå·®
    turnover_norm = (turnover - turnover.min()) / (turnover.max() - turnover.min()) if turnover.max() != turnover.min() else 0
    turnover_score = 1 - abs(turnover_norm - 0.5) * 2 # 0.5æ—¶ä¸º1ï¼Œ0æˆ–1æ—¶ä¸º0

    # å¸‚ç›ˆç‡ï¼šè¶Šä½è¶Šå¥½ï¼Œä½†è¦é¿å…è´Ÿå€¼å’Œè¿‡é«˜å€¼
    pe_ratio_capped = pe_ratio.apply(lambda x: min(x, 100) if x > 0 else 100) # å°†è¿‡é«˜æˆ–è´Ÿçš„å¸‚ç›ˆç‡é™åˆ¶åœ¨åˆç†èŒƒå›´
    pe_ratio_norm = (pe_ratio_capped - pe_ratio_capped.min()) / (pe_ratio_capped.max() - pe_ratio_capped.min()) if pe_ratio_capped.max() != pe_ratio_capped.min() else 0
    pe_score = 1 - pe_ratio_norm

    # ä»·æ ¼ç›¸å¯¹20æ—¥å‡çº¿ä½ç½®ï¼šè¶Šæ¥è¿‘å‡çº¿è¶Šå¥½ï¼Œä½†ç•¥é«˜äºå‡çº¿æ›´ä½³
    price_ma20_ratio = current_price / ma20
    # ä¹‹å‰æœ‰é”™ï¼Œç°åœ¨ä¿®æ­£
    price_ma20_ratio_score = price_ma20_ratio.apply(lambda x: max(0, 1 - abs(x - 1))) # è¶Šæ¥è¿‘1åˆ†è¶Šé«˜ï¼Œä½†ä¸èƒ½ä¸ºè´Ÿ

    # å¤„ç† NaN å€¼ï¼Œç”¨ 0 å¡«å……
    change_norm = change_norm.fillna(0)
    profit_norm = profit_norm.fillna(0)
    turnover_score = turnover_score.fillna(0)
    market_cap_norm = market_cap_norm.fillna(0)
    pe_score = pe_score.fillna(0)
    price_ma20_ratio_score = price_ma20_ratio_score.fillna(0)

    # è®¡ç®—å¤åˆè´¨é‡è¯„åˆ† (å¯ä»¥è°ƒæ•´æƒé‡)
    df['quality_score'] = (
        0.25 * change_norm +
        0.20 * profit_norm +
        0.15 * turnover_score +
        0.15 * market_cap_norm +
        0.15 * pe_score +
        0.10 * price_ma20_ratio_score
    )

    y = df['quality_score'].values

    # ç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & ~np.isnan(y) & ~np.isinf(y)
    X = X[mask]
    y = y[mask]

    if len(X) < 20: # è‡³å°‘éœ€è¦ä¸€äº›æ•°æ®æ¥åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        print("   âŒ æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹ã€‚")
        return None, None, None # è¿”å›æ¨¡å‹ã€Scalerã€ç‰¹å¾åç§°

    print(f"   æœ‰æ•ˆè®­ç»ƒæ ·æœ¬æ•°: {len(X)}")

    # æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç† (StandardScaler)...")
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

    # Optuna è¶…å‚æ•°ä¼˜åŒ–
    print("   å¯åŠ¨ Optuna è¶…å‚æ•°ä¼˜åŒ– (å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´)...")
    study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))
    try:
        study.optimize(lambda trial: objective(trial, X_train, y_train, X_test, y_test), n_trials=50, show_progress_bar=True)
    except Exception as e:
        print(f"   Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("   å°†ä½¿ç”¨é»˜è®¤æˆ–é¢„è®¾å‚æ•°è®­ç»ƒæ¨¡å‹ã€‚")
        # å¦‚æœOptunaå¤±è´¥ï¼Œä½¿ç”¨ä¸€ä¸ªåˆç†çš„é»˜è®¤é…ç½®
        best_params = {
            'objective': 'reg:squarederror',
            'eval_metric': 'rmse',
            'booster': 'gbtree',
            'lambda': 1.0,
            'alpha': 0.0,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'max_depth': 6,
            'eta': 0.1,
            'gamma': 0.0,
            'grow_policy': 'depthwise',
            'seed': 42,
            'n_jobs': -1,
        }
    else:
        print("\n   Optuna ä¼˜åŒ–å®Œæˆã€‚")
        print(f"   æœ€ä½³å‡æ–¹è¯¯å·® (MSE): {study.best_value:.4f}")
        print(f"   æœ€ä½³è¶…å‚æ•°: {study.best_params}")
        best_params = study.best_params

    # ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³å‚æ•°è®­ç»ƒæœ€ç»ˆ XGBoost æ¨¡å‹...")
    model = xgb.XGBRegressor(**best_params)
    model.fit(X_train, y_train,
              eval_set=[(X_test, y_test)],
              early_stopping_rounds=100, # å¢åŠ è€å¿ƒ
              verbose=False)

    # è¯„ä¼°æ¨¡å‹
    print("   è¯„ä¼°æœ€ç»ˆæ¨¡å‹...")
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"   æœ€ç»ˆæ¨¡å‹å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   æœ€ç»ˆæ¨¡å‹RÂ²åˆ†æ•°: {r2:.4f}")

    # è·å–ç‰¹å¾åç§° (ä¸get_model_featuresä¸­çš„é¡ºåºä¸€è‡´)
    feature_names = [
        "F_ä»·æ ¼ä½ç½®", "G_æ¶¨å¹…ä½ç½®", "ä»·æ ¼_vs_MA20", "ä»·æ ¼_vs_MA60", "MA20_vs_MA60",
        "æ—¥å†…æ³¢åŠ¨ç‡", "æ”¶ç›˜ä»·_æ—¥å†…ä½ç½®", "å½’å±å‡€åˆ©æ¶¦", "å®é™…æ¢æ‰‹ç‡", "æ€»å¸‚å€¼", "å¸‚ç›ˆç‡(åŠ¨)",
        "æˆäº¤é¢", "æ¶¨å¹…%"
    ]

    return model, scaler, feature_names

def predict_score_with_model(row, model, scaler):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """
    features = get_model_features(row)
    if any(pd.isna(f) or np.isinf(f) for f in features):
        return np.nan

    features = np.array(features).reshape(1, -1)
    try:
        features_scaled = scaler.transform(features)
        score = model.predict(features_scaled)[0]
        return score
    except Exception as e:
        # print(f"é¢„æµ‹åˆ†æ•°æ—¶å‘ç”Ÿé”™è¯¯: {e}, ç‰¹å¾: {features}")
        return np.nan

def generate_strategy_signals(stock_data, nn_score):
    """
    æ ¹æ®è‚¡ç¥¨æ•°æ®å’Œç¥ç»ç½‘ç»œè¯„åˆ†ç”Ÿæˆç­–ç•¥ä¿¡å·ã€‚
    """
    signals = []
    current = safe_float(stock_data.get('æœ€æ–°'))
    change = safe_float(stock_data.get('æ¶¨å¹…%'))
    turnover = safe_float(stock_data.get('å®é™…æ¢æ‰‹%'))
    ma20 = safe_float(stock_data.get('20æ—¥å‡ä»·'))
    ma60 = safe_float(stock_data.get('60æ—¥å‡ä»·'))
    pe_ratio = safe_float(stock_data.get('å¸‚ç›ˆç‡(åŠ¨)'))
    profit = safe_float(stock_data.get('å½’å±å‡€åˆ©æ¶¦'))
    market_cap = safe_float(stock_data.get('æ€»å¸‚å€¼'))
    high = safe_float(stock_data.get('æœ€é«˜'))
    low = safe_float(stock_data.get('æœ€ä½'))
    æˆäº¤é¢ = safe_float(stock_data.get('æˆäº¤é¢'))

    # çŸ­æœŸä¹°å…¥ä¿¡å·æ¡ä»¶åˆ—è¡¨
    short_term_buy_conditions = []
    if pd.notna(nn_score) and nn_score > 0.7:
        short_term_buy_conditions.append("NNé«˜åˆ†(>0.7)")
    if pd.notna(change) and change >= 2.0:
        short_term_buy_conditions.append(f"æ¶¨å¹…ç§¯æ(>={change:.2f}%)")
    if pd.notna(current) and pd.notna(ma20) and ma20 > 0 and current > ma20 * 1.01:
        short_term_buy_conditions.append("ç«™ä¸Š20MA(>1.01å€)")
    if pd.notna(turnover) and 1.0 < turnover < 15.0:
        short_term_buy_conditions.append(f"æ¢æ‰‹é€‚ä¸­({turnover:.2f}%)")
    if pd.notna(current) and pd.notna(high) and pd.notna(low) and (high - low) > 0 and (current - low) / (high - low) > 0.7:
        short_term_buy_conditions.append("æ”¶ç›˜å¼ºåŠ¿(è¿‘é«˜ç‚¹)")
    if pd.notna(æˆäº¤é¢) and æˆäº¤é¢ > 5.0: # æˆäº¤é¢å¤§äº5äº¿
        short_term_buy_conditions.append(f"æˆäº¤æ´»è·ƒ(>{æˆäº¤é¢:.2f}äº¿)")

    # åˆ¤æ–­çŸ­æœŸä¹°å…¥ä¿¡å·
    if len(short_term_buy_conditions) >= 3: # æ»¡è¶³è‡³å°‘3ä¸ªæ¡ä»¶
        signals.append(f"çŸ­æœŸä¹°å…¥ ({', '.join(short_term_buy_conditions)})")

    # é•¿æœŸä¹°å…¥ä¿¡å·æ¡ä»¶åˆ—è¡¨
    long_term_buy_conditions = []
    if pd.notna(nn_score) and nn_score > 0.6:
        long_term_buy_conditions.append("NNä¸­é«˜åˆ†(>0.6)")
    if pd.notna(pe_ratio) and 0 < pe_ratio < 40:
        long_term_buy_conditions.append(f"PEåˆç†({pe_ratio:.2f})")
    if pd.notna(profit) and profit > 0.5: # å½’å±å‡€åˆ©æ¶¦å¤§äº5000ä¸‡ (0.5äº¿)
        long_term_buy_conditions.append(f"å‡€åˆ©æ¶¦è‰¯å¥½(>{profit:.2f}äº¿)")
    if pd.notna(market_cap) and market_cap > 100: # æ€»å¸‚å€¼å¤§äº100äº¿
        long_term_buy_conditions.append(f"å¸‚å€¼è¾ƒå¤§(>{market_cap:.2f}äº¿)")
    if pd.notna(current) and pd.notna(ma60) and ma60 > 0 and current > ma60 * 0.95:
        long_term_buy_conditions.append("ä»·æ ¼è¿‘60MA(>0.95å€)")
    if pd.notna(ma20) and pd.notna(ma60) and ma60 > 0 and ma20 > ma60: # 20æ—¥å‡çº¿åœ¨60æ—¥å‡çº¿ä¹‹ä¸Š
        long_term_buy_conditions.append("å‡çº¿å¤šå¤´æ’åˆ—(20MA>60MA)")

    # åˆ¤æ–­é•¿æœŸä¹°å…¥ä¿¡å·
    if len(long_term_buy_conditions) >= 3: # æ»¡è¶³è‡³å°‘3ä¸ªæ¡ä»¶
        signals.append(f"é•¿æœŸä¹°å…¥ ({', '.join(long_term_buy_conditions)})")

    # è§„é¿/è­¦ç¤ºä¿¡å·æ¡ä»¶åˆ—è¡¨
    avoid_conditions = []
    if pd.notna(change) and change < -5.0:
        avoid_conditions.append(f"å¤§å¹…ä¸‹è·Œ(<{change:.2f}%)")
    if pd.notna(turnover) and turnover > 25.0:
        avoid_conditions.append(f"æ¢æ‰‹è¿‡é«˜(>{turnover:.2f}%)")
    if pd.notna(pe_ratio) and (pe_ratio < 0 or pe_ratio > 150):
        avoid_conditions.append(f"PEå¼‚å¸¸({pe_ratio:.2f})")
    if pd.notna(profit) and profit < 0:
        avoid_conditions.append(f"å‡€åˆ©æ¶¦ä¸ºè´Ÿ({profit:.2f}äº¿)")
    if pd.notna(current) and pd.notna(ma20) and ma20 > 0 and current < ma20 * 0.95:
        avoid_conditions.append("è·Œç ´20MA(<0.95å€)")
    if pd.notna(market_cap) and market_cap < 30: # æ€»å¸‚å€¼å°äº30äº¿
        avoid_conditions.append(f"å¸‚å€¼è¿‡å°(<{market_cap:.2f}äº¿)")

    # åˆ¤æ–­è§„é¿/è­¦ç¤ºä¿¡å·
    if len(avoid_conditions) >= 2: # æ»¡è¶³è‡³å°‘2ä¸ªæ¡ä»¶
        signals.append(f"è§„é¿/è­¦ç¤º ({', '.join(avoid_conditions)})")

    if not signals:
        signals.append("æ— æ˜ç¡®ä¿¡å·")

    return "; ".join(signals)

def perform_association_rule_mining(df):
    """
    ä½¿ç”¨å…³è”è§„åˆ™æŒ–æ˜æ¥å‘ç°è‹æ°é‡åŒ–ç­–ç•¥æ¡ä»¶ä¸é«˜æ”¶ç›Šä¹‹é—´çš„å…³ç³»ã€‚
    """
    print("\n4. æ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜...")

    # å‡†å¤‡æ•°æ®ï¼šå°†ç‰¹å¾å’Œç›®æ ‡å˜é‡ç¦»æ•£åŒ–
    data_for_ar = []
    for _, row in df.iterrows():
        # ä½¿ç”¨ get_model_features è·å–åŸå§‹æ•°å€¼ç‰¹å¾
        raw_features = get_model_features(row)
        items = []

        # è‹æ°ç­–ç•¥ç‰¹å¾
        if raw_features[0] == 1: items.append("F_ä»·æ ¼ä½ç½®_æ»¡è¶³")
        else: items.append("F_ä»·æ ¼ä½ç½®_ä¸æ»¡è¶³")

        if raw_features[1] == 1: items.append("G_æ¶¨å¹…ä½ç½®_æ»¡è¶³")
        else: items.append("G_æ¶¨å¹…ä½ç½®_ä¸æ»¡è¶³")

        # å½’å±å‡€åˆ©æ¶¦ (Feature 7): åˆ†ä¸ºé«˜ã€ä¸­ã€ä½ä¸‰æ¡£
        profit = raw_features[7]
        if profit >= 10: items.append("H_å‡€åˆ©æ¶¦_é«˜(>=10äº¿)")
        elif 1 <= profit < 10: items.append("H_å‡€åˆ©æ¶¦_ä¸­(1-10äº¿)")
        else: items.append("H_å‡€åˆ©æ¶¦_ä½(<1äº¿)")

        # å®é™…æ¢æ‰‹ç‡ (Feature 8): åˆ†ä¸ºé«˜ã€ä½ä¸¤æ¡£
        turnover = raw_features[8]
        if turnover >= 10: items.append("I_æ¢æ‰‹ç‡_é«˜(>=10%)")
        else: items.append("I_æ¢æ‰‹ç‡_ä½(<10%)")

        # æ€»å¸‚å€¼ (Feature 9): åˆ†ä¸ºå¤§ã€ä¸­ã€å°ä¸‰æ¡£
        market_cap = raw_features[9]
        if market_cap >= 500: items.append("J_å¸‚å€¼_å¤§(>=500äº¿)")
        elif 100 <= market_cap < 500: items.append("J_å¸‚å€¼_ä¸­(100-500äº¿)")
        else: items.append("J_å¸‚å€¼_å°(<100äº¿)")

        # ä»·æ ¼ç›¸å¯¹20æ—¥å‡çº¿ä½ç½® (Feature 2): åˆ†ä¸ºé«˜äºã€æ¥è¿‘ã€ä½äºä¸‰æ¡£
        price_vs_ma20 = raw_features[2]
        if price_vs_ma20 > 1.05: items.append("ä»·æ ¼_è¿œé«˜äº20MA")
        elif 0.95 <= price_vs_ma20 <= 1.05: items.append("ä»·æ ¼_è¿‘20MA")
        else: items.append("ä»·æ ¼_è¿œä½äº20MA")

        # æ”¶ç›˜ä»·åœ¨æ—¥å†…åŒºé—´çš„ä½ç½® (Feature 6): åˆ†ä¸ºå¼ºåŠ¿ã€å¼±åŠ¿ä¸¤æ¡£
        close_pos_in_range = raw_features[6]
        if close_pos_in_range > 0.7: items.append("æ”¶ç›˜ä»·_æ—¥å†…å¼ºåŠ¿")
        else: items.append("æ”¶ç›˜ä»·_æ—¥å†…å¼±åŠ¿")

        # ç›®æ ‡å˜é‡ï¼šé«˜æ¶¨å¹… (ä¾‹å¦‚ï¼Œæ¶¨å¹… > 3%)
        change = safe_float(row.get('æ¶¨å¹…%'))
        if pd.notna(change) and change > 3.0: # å¯ä»¥è°ƒæ•´è¿™ä¸ªé˜ˆå€¼
            items.append("é«˜æ¶¨å¹…")
        else:
            items.append("ä½æ¶¨å¹…")

        data_for_ar.append(items)

    if not data_for_ar:
        print("   âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå…³è”è§„åˆ™æŒ–æ˜ã€‚")
        return

    te = TransactionEncoder()
    te_ary = te.fit(data_for_ar).transform(data_for_ar)
    df_ar = pd.DataFrame(te_ary, columns=te.columns_)

    # æŸ¥æ‰¾é¢‘ç¹é¡¹é›†
    # min_support å¯ä»¥æ ¹æ®æ•°æ®é‡è°ƒæ•´ï¼Œå¤ªå°è§„åˆ™å¤ªå¤šï¼Œå¤ªå¤§è§„åˆ™å¤ªå°‘
    frequent_itemsets = apriori(df_ar, min_support=0.01, use_colnames=True) # é™ä½æ”¯æŒåº¦ä»¥å‘ç°æ›´å¤šè§„åˆ™
    if frequent_itemsets.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°é¢‘ç¹é¡¹é›†ï¼Œè¯·å°è¯•é™ä½ min_supportã€‚")
        return

    # ç”Ÿæˆå…³è”è§„åˆ™
    rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1.1) # æé«˜æå‡åº¦é˜ˆå€¼
    if rules.empty:
        print("   âš ï¸ æœªæ‰¾åˆ°æœ‰æ„ä¹‰çš„å…³è”è§„åˆ™ï¼Œè¯·å°è¯•é™ä½ min_threshold æˆ–æ£€æŸ¥æ•°æ®ã€‚")
        return

    # ç­›é€‰å¹¶æ‰“å°ä¸â€œé«˜æ¶¨å¹…â€ç›¸å…³çš„è§„åˆ™
    high_return_rules = rules[rules['consequents'].apply(lambda x: 'é«˜æ¶¨å¹…' in x)]
    high_return_rules = high_return_rules.sort_values(by=['lift', 'confidence'], ascending=False)

    print("\n   å‘ç°ä»¥ä¸‹ä¸ 'é«˜æ¶¨å¹…' ç›¸å…³çš„å…³è”è§„åˆ™ (æŒ‰ Lift é™åº):")
    if high_return_rules.empty:
        print("   æœªæ‰¾åˆ°ç›´æ¥å¯¼è‡´ 'é«˜æ¶¨å¹…' çš„å…³è”è§„åˆ™ã€‚")
    else:
        for i, rule in high_return_rules.head(10).iterrows(): # åªæ˜¾ç¤ºå‰10æ¡
            antecedent_str = ', '.join(list(rule['antecedents']))
            consequent_str = ', '.join(list(rule['consequents']))
            print(f"   è§„åˆ™ {i+1}: {antecedent_str} => {consequent_str}")
            print(f"     æ”¯æŒåº¦ (Support): {rule['support']:.4f}")
            print(f"     ç½®ä¿¡åº¦ (Confidence): {rule['confidence']:.4f}")
            print(f"     æå‡åº¦ (Lift): {rule['lift']:.4f}")
            print("-" * 40)

    print("\n   å…³è”è§„åˆ™æŒ–æ˜å®Œæˆã€‚è¿™äº›è§„åˆ™å¯ä»¥ä¸ºç­–ç•¥ä¼˜åŒ–æä¾›æ´å¯Ÿã€‚")


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ (é›†æˆXGBoostä¸å…³è”è§„åˆ™)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)
    os.makedirs('å‚è€ƒæ•°æ®', exist_ok=True) # ç¡®ä¿å‚è€ƒæ•°æ®ç›®å½•å­˜åœ¨

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ®...")

    df = pd.DataFrame()
    # å°è¯•è·å–å®æ—¶æ•°æ®
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ® (akshare.stock_zh_a_spot_em)...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        # ç»Ÿä¸€åˆ—å
        df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°',
            'æ¶¨è·Œå¹…': 'æ¶¨å¹…%',
            'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)',
            'æˆäº¤é¢': 'æˆäº¤é¢' # ç¡®ä¿æˆäº¤é¢åˆ—åæ­£ç¡®
        }, inplace=True)

        # ä¿å­˜åŸå§‹ä»£ç 
        df_realtime['åŸå§‹ä»£ç '] = df_realtime['ä»£ç '].copy()

        # æ ¼å¼åŒ–ä»£ç ä»¥ä¾¿Excelè¯†åˆ«ä¸ºæ–‡æœ¬
        df_realtime['ä»£ç '] = df_realtime['ä»£ç '].apply(lambda x: f'= "{str(x)}"')

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨ï¼Œå¹¶åˆå§‹åŒ–ä¸ºNaN
        required_cols = ['ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', 'æˆäº¤é¢',
                         'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)','æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜']
        for col in required_cols:
            if col not in df_realtime.columns:
                df_realtime[col] = np.nan

        df = df_realtime.copy()

    except Exception as e:
        print(f"   âŒ è·å–å®æ—¶æ•°æ®å¤±è´¥: {e}")
        df = pd.DataFrame()

    # å°è¯•ä»æœ¬åœ° Excel æ–‡ä»¶è¡¥å……æ•°æ®
    try:
        table_path = os.path.join('å‚è€ƒæ•°æ®', 'Table.xls')
        if os.path.exists(table_path):
            print("   å°è¯•ä»æœ¬åœ° Table.xls è¡¥å……æ•°æ®...")
            df_table = pd.read_csv(table_path, sep='\t', encoding='gbk') # ç¡®ä¿ç¼–ç æ­£ç¡®
            # ç»Ÿä¸€ä»£ç æ ¼å¼
            df_table['ä»£ç '] = df_table['ä»£ç '].astype(str).str.replace('="', '').str.replace('"', '')

            # å°†df_tableä¸­çš„'ä»£ç 'åˆ—è®¾ç½®ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df_table['ä»£ç '] = df_table['ä»£ç '].astype(str)

            # å°†dfä¸­çš„'ä»£ç 'åˆ—è®¾ç½®ä¸ºå­—ç¬¦ä¸²ç±»å‹
            df['ä»£ç '] = df['ä»£ç '].astype(str).str.replace('="', '').str.replace('"', '')

            # ç¡®ä¿df_tableä¸­çš„åˆ—å­˜åœ¨äºdfä¸­
            for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']:
                if col not in df_table.columns:
                    df_table[col] = np.nan

            # è¡¥å……ç¼ºå¤±æ•°æ®
            merged_df = pd.merge(df, df_table[['ä»£ç ', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']],
                                  on='ä»£ç ', how='left', suffixes=('', '_table'))

            for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦']:
                df[col] = merged_df[col + '_table'].fillna(merged_df[col])

            print(f"   âœ… è¡¥å……äº† {len(df)} æ¡å‚è€ƒæ•°æ®ä¸­çš„ç¼ºå¤±ä¿¡æ¯ (å¦‚æœTable.xlså­˜åœ¨ä¸”æœ‰åŒ¹é…æ•°æ®)")
        else:
            print("   âš ï¸ æœªæ‰¾åˆ° Table.xlsï¼Œè·³è¿‡æ•°æ®è¡¥å……")
    except Exception as e:
        print(f"   âŒ ä» Table.xls è¡¥å……æ•°æ®å¤±è´¥: {e}")

    # ä¿å­˜Aè‚¡æ•°æ®åˆ°CSV
    csv_path = os.path.join('è¾“å‡ºæ•°æ®', 'Aè‚¡æ•°æ®.csv')
    df.to_csv(csv_path, encoding='utf_8_sig', index=True, header=True)
    print(f"\nâœ… Aè‚¡æ•°æ®å·²ä¿å­˜: {csv_path}")
    print(f"   å…± {len(df)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒ XGBoost æ¨¡å‹ ==========
    model, scaler, feature_names = train_xgboost_model(df.copy()) # ä¼ é€’ df çš„å‰¯æœ¬

    if model is None or scaler is None:
        print("   âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­é¢„æµ‹ã€‚")
        return

    # æ˜¾ç¤ºæ¨¡å‹ç‰¹å¾é‡è¦æ€§
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
        feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
        feature_importance = feature_importance.sort_values(by='Importance', ascending=False)
        print("\n   æ¨¡å‹ç‰¹å¾é‡è¦æ€§ (Feature Importance):")
        print(feature_importance.to_string(index=False))
    else:
        print("\n   âš ï¸ æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§è¯„ä¼°ã€‚")

    # ========== ç¬¬ä¸‰æ­¥ï¼šåŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨å¹¶ç”Ÿæˆç­–ç•¥ä¿¡å· ==========
    print("\n3. åŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨å¹¶ç”Ÿæˆç­–ç•¥ä¿¡å· (åŸºäºXGBoostè¯„åˆ†)...")

    # ä½¿ç”¨æ¨¡å‹é¢„æµ‹è¯„åˆ†
    df['predicted_score'] = df.apply(lambda row: predict_score_with_model(row, model, scaler), axis=1)

    # ç§»é™¤ predicted_score ä¸º NaN çš„è¡Œ
    df = df.dropna(subset=['predicted_score']).copy()

    # æ ¹æ® predicted_score æ’åº
    df = df.sort_values(by='predicted_score', ascending=False)

    # ç¡®å®šæœ€ä½ä¼˜è´¨ç‡é˜ˆå€¼ (åŸºäºå‰20åæˆ–å…¨éƒ¨)
    num_to_consider = min(20, len(df))
    min_quality_threshold = df['predicted_score'].iloc[num_to_consider - 1] if num_to_consider > 0 else 0

    # ç­›é€‰ç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨
    qualified_stocks = df[df['predicted_score'] >= min_quality_threshold].copy()

    # ç”Ÿæˆç­–ç•¥ä¿¡å·
    qualified_stocks['strategy_signal'] = qualified_stocks.apply(lambda row: generate_strategy_signals(row, row['predicted_score']), axis=1)

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨åŠç­–ç•¥ä¿¡å·åˆ°æ–‡æœ¬æ–‡ä»¶
    output_path = os.path.join('è¾“å‡ºæ•°æ®', 'ä¼˜è´¨è‚¡ç¥¨_ç­–ç•¥ä¿¡å·.txt')
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("è‹æ°é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœä¸ç­–ç•¥ä¿¡å· (XGBoostè¯„åˆ†)\n")
        f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æœ€ä½ä¼˜è´¨ç‡é˜ˆå€¼ (åŸºäºå‰20åæˆ–å…¨éƒ¨): {min_quality_threshold:.4f}\n")

        buy_signals = qualified_stocks[qualified_stocks['strategy_signal'].str.contains('ä¹°å…¥')].copy()
        num_buy_signals = len(buy_signals)
        f.write(f"ç¬¦åˆä¹°å…¥ä¿¡å·çš„ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {num_buy_signals}\n")

        f.write("=" * 60 + "\n")
        for index, row in buy_signals.iterrows():
            f.write(f"è‚¡ç¥¨ä»£ç : {row['ä»£ç ']}\n")
            f.write(f"è‚¡ç¥¨åç§°: {row['åç§°']}\n")
            f.write(f"æ‰€å±è¡Œä¸š: {row['æ‰€å±è¡Œä¸š']}\n")
            f.write(f"ä¼˜è´¨ç‡ (XGBoostè¯„åˆ†): {row['predicted_score']:.4f}\n")
            f.write(f"ä»Šæ—¥æ¶¨å¹…: {row['æ¶¨å¹…%']}\n")
            f.write(f"ç­–ç•¥ä¿¡å·: {row['strategy_signal']}\n")
            f.write("-" * 30 + "\n")

    print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨åŠç­–ç•¥ä¿¡å·å·²ä¿å­˜: {output_path}")
    print(f"   æ‰¾åˆ° {num_buy_signals} åªç¬¦åˆä¹°å…¥æ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨ï¼ˆæœ€ä½ä¼˜è´¨ç‡={min_quality_threshold:.4f}ï¼‰")

    # è¾“å‡ºä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨ (å¸¦ä¹°å…¥ä¿¡å·çš„å‰ N å)
    N = min(4, num_buy_signals)
    print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨ (å‰{N}åï¼Œä»…æ˜¾ç¤ºä¹°å…¥ä¿¡å·)ï¼š")
    print("=" * 85)
    print(f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<15} {'æ¶¨å¹…':<8} {'ä¼˜è´¨ç‡':<8} {'æ‰€å±è¡Œä¸š':<15} {'ç­–ç•¥ä¿¡å·':<40}")
    print("-" * 85)
    for index, row in buy_signals.head(N).iterrows():
        print(f"{row['ä»£ç ']:<10} {row['åç§°']:<15} {str(row['æ¶¨å¹…%']):<8} {row['predicted_score']:.4f:<8} {row['æ‰€å±è¡Œä¸š']:<15} {row['strategy_signal']:<40}")
    print("=" * 85)

    # ========== ç¬¬å››æ­¥ï¼šæ‰§è¡Œå…³è”è§„åˆ™æŒ–æ˜ ==========
    perform_association_rule_mining(df.copy()) # ä¼ é€’ df çš„å‰¯æœ¬

    print("\n" + "="*60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*60 + "\n")

if __name__ == "__main__":
    main()

