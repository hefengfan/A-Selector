#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - æ ¹æ®æ¯å¤©å®æ—¶æ•°æ®ç­›é€‰
åŸºäºè‹æ°é‡åŒ–ç­–ç•¥çš„çœŸå®è®¡ç®—é€»è¾‘
é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ† (Scikit-learn + Optuna)
é›†æˆå…³è”è§„åˆ™æŒ–æ˜ (Apriori)
"""

import akshare as ak
import pandas as pd
import numpy as np
from datetime import datetime
import os
import warnings
warnings.filterwarnings('ignore')

# å¯¼å…¥ Scikit-learn å’Œ Optuna ç›¸å…³åº“
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error, r2_score
import optuna

# å¯¼å…¥å…³è”è§„åˆ™æŒ–æ˜åº“
from mlxtend.frequent_patterns import apriori, association_rules

# æ¸…é™¤ä»£ç†è®¾ç½®
os.environ['HTTP_PROXY'] = ''
os.environ['HTTPS_PROXY'] = ''
os.environ['ALL_PROXY'] = ''
os.environ['NO_PROXY'] = '*'

# å®šä¹‰éœ€è¦è½¬æ¢ä¸ºæ•°å€¼çš„åˆ—
NUMERIC_COLS = [
    'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½', 'å®é™…æ¢æ‰‹%', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
    'å½’å±å‡€åˆ©æ¶¦', 'æ€»å¸‚å€¼', 'å¸‚ç›ˆç‡(åŠ¨)', 'æ˜¨æ”¶', 'å¼€ç›˜'
]

def clean_and_convert_numeric(df, cols_to_convert):
    """
    å°†æŒ‡å®šåˆ—è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œå¹¶å¤„ç†ç¼ºå¤±å€¼ã€‚
    """
    for col in cols_to_convert:
        if col in df.columns:
            # å°è¯•æ›¿æ¢å¸¸è§çš„éæ•°å€¼è¡¨ç¤º
            df[col] = df[col].astype(str).str.replace('--', '0').str.replace(' ', '').str.replace(',', '')

            # å¤„ç†äº¿ã€ä¸‡äº¿ç­‰å•ä½
            def parse_value(val):
                if 'ä¸‡äº¿' in val:
                    return float(val.replace('ä¸‡äº¿', '')) * 1_000_000_000_000
                elif 'äº¿' in val:
                    return float(val.replace('äº¿', '')) * 1_000_000_00
                elif 'ä¸‡' in val:
                    return float(val.replace('ä¸‡', '')) * 10_000
                return val

            df[col] = df[col].apply(parse_value)

            # è½¬æ¢ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
            # å¡«å……NaNï¼Œè¿™é‡Œé€‰æ‹©0ï¼Œä¹Ÿå¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©ä¸­ä½æ•°æˆ–å‡å€¼
            df[col] = df[col].fillna(0)
    return df


def calculate_features(row):
    """
    æ ¹æ®è‹æ°é‡åŒ–ç­–ç•¥è®¡ç®—ç‰¹å¾å€¼ï¼Œç›´æ¥ä½¿ç”¨æ•°å€¼åˆ—
    """
    features = []

    # Fåˆ—ï¼šä»·æ ¼ä½ç½®æ¡ä»¶
    low = row['æœ€ä½']
    ma60 = row['60æ—¥å‡ä»·']
    ma20 = row['20æ—¥å‡ä»·']
    current = row['æœ€æ–°']

    condition_met_F = 0
    if ma60 > 0 and 0.85 <= low / ma60 <= 1.15:
        condition_met_F = 1
    elif ma20 > 0 and 0.90 <= current / ma20 <= 1.10:
        condition_met_F = 1
    features.append(condition_met_F)

    # Gåˆ—ï¼šæ¶¨å¹…å’Œä»·æ ¼ä½ç½®
    change = row['æ¶¨å¹…%']
    high = row['æœ€é«˜']
    low_price = row['æœ€ä½'] # é¿å…å˜é‡åå†²çª
    current_price = row['æœ€æ–°'] # é¿å…å˜é‡åå†²çª

    condition_met_G = 0
    if change >= 5.0:
        # ç¡®ä¿ high å’Œ low_price æ˜¯æœ‰æ•ˆæ•°å­—ï¼Œé¿å…é™¤ä»¥0æˆ–NaN
        if high > low_price:
            threshold = high - (high - low_price) * 0.30
            if current_price >= threshold:
                condition_met_G = 1
    features.append(condition_met_G)

    # Håˆ—ï¼šå‡€åˆ©æ¶¦>=3000ä¸‡ (0.3äº¿)
    profit = row['å½’å±å‡€åˆ©æ¶¦']
    features.append(profit)

    # Iåˆ—ï¼šæ¢æ‰‹ç‡<=20%
    turnover = row['å®é™…æ¢æ‰‹%']
    features.append(turnover)

    # Jåˆ—ï¼šå¸‚å€¼>=300äº¿
    cap = row['æ€»å¸‚å€¼']
    features.append(cap)

    return features


def create_model(trial, input_shape):
    """
    ä½¿ç”¨ Optuna å»ºè®®çš„è¶…å‚æ•°åˆ›å»º Scikit-learn MLPRegressor æ¨¡å‹
    """
    n_layers = trial.suggest_int('n_layers', 1, 4)  # å¢åŠ å±‚æ•°
    hidden_layer_sizes = []
    for i in range(n_layers):
        num_units = trial.suggest_int(f'n_units_{i}', 64, 512)  # å¢åŠ èŠ‚ç‚¹æ•°
        hidden_layer_sizes.append(num_units)

    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'logistic', 'identity']) # å¢åŠ æ¿€æ´»å‡½æ•°
    solver = trial.suggest_categorical('solver', ['adam', 'lbfgs'])
    alpha = trial.suggest_float('alpha', 1e-6, 1e-1, log=True)  # è°ƒæ•´ alpha èŒƒå›´
    learning_rate_init = trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True) # è°ƒæ•´å­¦ä¹ ç‡

    model = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,
                         activation=activation,
                         solver=solver,
                         alpha=alpha,
                         learning_rate_init=learning_rate_init,
                         random_state=42,
                         max_iter=500,  # å¢åŠ è¿­ä»£æ¬¡æ•°
                         early_stopping=True, # å¯ç”¨æ—©åœ
                         n_iter_no_change=20, # å¢åŠ å®¹å¿åº¦
                         tol=1e-4) # å¢åŠ æ”¶æ•›å®¹å¿åº¦

    return model


def objective(trial, X_train, y_train, X_test, y_test):
    """
    Optuna ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°
    """
    model = create_model(trial, X_train.shape[1])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    # å°è¯•ä¼˜åŒ–R2åˆ†æ•°ï¼Œå› ä¸ºR2æ›´èƒ½ä½“ç°æ¨¡å‹çš„è§£é‡Šèƒ½åŠ›
    # Optuna é»˜è®¤æ˜¯æœ€å°åŒ–ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ€å°åŒ– (1 - R2)
    # ä¹Ÿå¯ä»¥å°è¯•æœ€å°åŒ– MSE
    return 1 - r2


def train_neural_network(df):
    """
    è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹ (Scikit-learn + Optuna)ï¼Œé¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """

    # 1. å‡†å¤‡è®­ç»ƒæ•°æ®
    print("\n   å‡†å¤‡è®­ç»ƒæ•°æ®...")
    X = []
    y = []  # ç›®æ ‡å˜é‡ï¼šæ¶¨å¹…ä½œä¸ºè¯„åˆ†çš„ä¾æ®

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾åˆ—å’Œç›®æ ‡åˆ—éƒ½æ˜¯æ•°å€¼ç±»å‹ä¸”æ— NaN
    df_for_nn = df.copy()
    # ç¡®ä¿ calculate_features ä¾èµ–çš„åˆ—éƒ½æ˜¯æ•°å€¼
    df_for_nn = clean_and_convert_numeric(df_for_nn, NUMERIC_COLS)

    for _, row in df_for_nn.iterrows():
        features = calculate_features(row)
        X.append(features)
        y.append(row['æ¶¨å¹…%']) # æ¶¨å¹…% å·²ç»é€šè¿‡ clean_and_convert_numeric å¤„ç†è¿‡

    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)

    # è”åˆæ¸…æ´— X å’Œ yï¼Œç§»é™¤åŒ…å« NaN æˆ–æ— ç©·å¤§çš„è¡Œ
    combined_mask = ~np.any(np.isnan(X) | np.isinf(X), axis=1) & \
                    ~np.isnan(y) & ~np.isinf(y)

    X = X[combined_mask]
    y = y[combined_mask]

    if len(X) == 0:
        print("   âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®ï¼Œæ— æ³•è®­ç»ƒç¥ç»ç½‘ç»œã€‚")
        return None, None, 0 # è¿”å›0ä½œä¸ºR2åˆ†æ•°

    # 2. æ•°æ®é¢„å¤„ç†
    print("   æ•°æ®é¢„å¤„ç†...")
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # 3. åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    print("   åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # 4. ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°
    print("   ä½¿ç”¨ Optuna ä¼˜åŒ–è¶…å‚æ•°...")
    study = optuna.create_study(direction='minimize') # æœ€å°åŒ– (1-R2)

    from functools import partial
    objective_partial = partial(objective, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)

    try:
        study.optimize(objective_partial, n_trials=30, timeout=180)  # å¢åŠ  trials å’Œ timeout
    except Exception as e:
        print(f"   âš ï¸ Optuna ä¼˜åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("   å°†ä½¿ç”¨é»˜è®¤å‚æ•°æˆ–å·²æ‰¾åˆ°çš„æœ€ä½³å‚æ•°ã€‚")
        if not study.trials: # å¦‚æœæ²¡æœ‰æˆåŠŸè¿è¡Œä»»ä½•trial
            print("   âŒ Optuna æœªèƒ½å®Œæˆä»»ä½• trialï¼Œæ— æ³•å¾—åˆ°æœ€ä½³å‚æ•°ã€‚")
            return None, None, 0

    # 5. ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åˆ›å»ºæ¨¡å‹
    print("   ä½¿ç”¨æœ€ä½³è¶…å‚æ•°åˆ›å»ºæ¨¡å‹...")
    best_model = create_model(study.best_trial, X_train.shape[1])
    best_model.fit(X_train, y_train)

    # 6. è¯„ä¼°æœ€ä½³æ¨¡å‹
    print("   è¯„ä¼°æœ€ä½³æ¨¡å‹...")
    y_pred = best_model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"   å‡æ–¹è¯¯å·® (MSE): {mse:.4f}")
    print(f"   R^2 Score: {r2:.4f}")

    print("   æœ€ä½³è¶…å‚æ•°:")
    print(study.best_params)

    return best_model, scaler, r2


def predict_score_with_nn(row, model, scaler):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„ Scikit-learn ç¥ç»ç½‘ç»œæ¨¡å‹é¢„æµ‹è‚¡ç¥¨è¯„åˆ†
    """
    # ç¡®ä¿ row ä¸­çš„ç‰¹å¾æ˜¯æ•°å€¼ç±»å‹
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ row å·²ç»ç»è¿‡äº† clean_and_convert_numeric å¤„ç†
    features = calculate_features(row)
    features = np.array(features).reshape(1, -1)  # è½¬æ¢ä¸ºäºŒç»´æ•°ç»„

    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼æˆ–æ— ç©·å€¼
    if np.any(np.isnan(features)) or np.any(np.isinf(features)):
        return 0  # å¦‚æœæœ‰ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å€¼

    features_scaled = scaler.transform(features)
    score = model.predict(features_scaled)[0]
    return score


def analyze_association_rules(df):
    """
    ä½¿ç”¨ Apriori ç®—æ³•åˆ†æè‚¡ç¥¨æ•°æ®ä¸­çš„å…³è”è§„åˆ™
    """
    print("\n   åˆ†æå…³è”è§„åˆ™...")

    # é€‰æ‹©ç”¨äºå…³è”è§„åˆ™åˆ†æçš„ç‰¹å¾åˆ—ï¼Œå¹¶å®šä¹‰å®ƒä»¬çš„äºŒå€¼åŒ–é˜ˆå€¼
    # è¿™é‡Œé€‰æ‹©ä¸€äº›æœ‰æ„ä¹‰çš„æ•°å€¼ç‰¹å¾è¿›è¡ŒäºŒå€¼åŒ–
    # ç¡®ä¿è¿™äº›åˆ—åœ¨ df ä¸­æ˜¯æ•°å€¼ç±»å‹ä¸”æ— NaN
    df_apriori = df[['æ¶¨å¹…%', 'å®é™…æ¢æ‰‹%', 'å½’å±å‡€åˆ©æ¶¦', 'æ€»å¸‚å€¼']].copy()
    df_apriori = clean_and_convert_numeric(df_apriori, df_apriori.columns.tolist())

    # å°†æ•°å€¼ç‰¹å¾è½¬æ¢ä¸ºå¸ƒå°”å€¼ (0 æˆ– 1)
    # å®šä¹‰ä¸€äº›æœ‰æ„ä¹‰çš„é˜ˆå€¼æ¥åˆ›å»ºäºŒå€¼åŒ–ç‰¹å¾
    df_encoded = pd.DataFrame()
    df_encoded['æ¶¨å¹…_é«˜'] = df_apriori['æ¶¨å¹…%'] > 3.0 # æ¶¨å¹…å¤§äº3%
    df_encoded['æ¢æ‰‹ç‡_ä½'] = df_apriori['å®é™…æ¢æ‰‹%'] < 10.0 # æ¢æ‰‹ç‡ä½äº10%
    df_encoded['å‡€åˆ©æ¶¦_é«˜'] = df_apriori['å½’å±å‡€åˆ©æ¶¦'] > 0.5 # å‡€åˆ©æ¶¦å¤§äº0.5äº¿
    df_encoded['å¸‚å€¼_å¤§'] = df_apriori['æ€»å¸‚å€¼'] > 500.0 # å¸‚å€¼å¤§äº500äº¿

    # è½¬æ¢ä¸ºå¸ƒå°”ç±»å‹ DataFrame
    df_encoded = df_encoded.astype(bool)

    # ç§»é™¤å…¨ä¸º False çš„è¡Œï¼Œè¿™äº›è¡Œå¯¹å…³è”è§„åˆ™æ²¡æœ‰è´¡çŒ®
    df_encoded = df_encoded[df_encoded.any(axis=1)]

    if df_encoded.empty:
        print("   âš ï¸ æ²¡æœ‰è¶³å¤Ÿçš„äºŒå€¼åŒ–æ•°æ®è¿›è¡Œå…³è”è§„åˆ™åˆ†æã€‚")
        return pd.DataFrame()

    # ä½¿ç”¨ Apriori ç®—æ³•æ‰¾åˆ°é¢‘ç¹é¡¹é›†
    # è°ƒæ•´ min_supportï¼Œå¦‚æœæ•°æ®é‡å¤§ï¼Œå¯ä»¥é€‚å½“æé«˜
    frequent_itemsets = apriori(df_encoded, min_support=0.01, use_colnames=True)

    if frequent_itemsets.empty:
        print("   âš ï¸ æ²¡æœ‰æ‰¾åˆ°é¢‘ç¹é¡¹é›†ã€‚")
        return pd.DataFrame()

    # ç”Ÿæˆå…³è”è§„åˆ™
    # è°ƒæ•´ metric å’Œ min_threshold
    rules = association_rules(frequent_itemsets, metric="confidence", min_threshold=0.5)

    print(f"   æ‰¾åˆ° {len(rules)} æ¡å…³è”è§„åˆ™")
    return rules


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "="*60)
    print("åŠ¨æ€é€‰è‚¡ç³»ç»Ÿ - å®æ—¶è®¡ç®—ç‰ˆ")
    print("é›†æˆç¥ç»ç½‘ç»œè¿›è¡Œç²¾å‡†è¯„åˆ† (Scikit-learn + Optuna)")
    print("é›†æˆå…³è”è§„åˆ™æŒ–æ˜ (Apriori)")
    print(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs('è¾“å‡ºæ•°æ®', exist_ok=True)

    # ========== ç¬¬ä¸€æ­¥ï¼šè·å–æ•°æ® ==========
    print("\n1. è·å–Aè‚¡æ•°æ®...")

    df = pd.DataFrame() # åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„DataFrame
    # å…ˆå°è¯•è·å–å®æ—¶æ•°æ®
    try:
        print("   å°è¯•è·å–å®æ—¶æ•°æ®...")
        df_realtime = ak.stock_zh_a_spot_em()
        print(f"   âœ… æˆåŠŸè·å– {len(df_realtime)} åªè‚¡ç¥¨çš„å®æ—¶æ•°æ®")

        # ç»Ÿä¸€åˆ—åä»¥åŒ¹é…å‚è€ƒæ•°æ®ï¼Œå¹¶è¿›è¡Œåˆæ­¥å¤„ç†
        df_realtime = df_realtime.rename(columns={
            'æœ€æ–°ä»·': 'æœ€æ–°',
            'æ¶¨è·Œå¹…': 'æ¶¨å¹…%',
            'æ¢æ‰‹ç‡': 'å®é™…æ¢æ‰‹%',
            'æ€»å¸‚å€¼': 'æ€»å¸‚å€¼',
            'å¸‚ç›ˆç‡-åŠ¨æ€': 'å¸‚ç›ˆç‡(åŠ¨)' # æ³¨æ„è¿™é‡Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…æ•°æ®è°ƒæ•´
        })

        # ç¡®ä¿æ‰€æœ‰å…³é”®åˆ—å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºå¹¶å¡«å……é»˜è®¤å€¼
        for col in ['20æ—¥å‡ä»·', '60æ—¥å‡ä»·', 'æ‰€å±è¡Œä¸š', 'å½’å±å‡€åˆ©æ¶¦']:
            if col not in df_realtime.columns:
                df_realtime[col] = ' --'

        df = df_realtime.copy()

    except Exception as e:
        print(f"   âŒ å®æ—¶è·å–å¤±è´¥: {e}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")  # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯
        print("   ä½¿ç”¨å‚è€ƒæ•°æ®ä½œä¸ºå¤‡é€‰...")

        # ä½¿ç”¨å‚è€ƒæ•°æ®
        try:
            df = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
            print(f"   âœ… ä»å‚è€ƒæ–‡ä»¶åŠ è½½äº† {len(df)} æ¡æ•°æ®")
            # ç§»é™¤Excelå…¬å¼å‰ç¼€
            df['ä»£ç '] = df['ä»£ç '].str.replace('= "', '').str.replace('"', '')
        except Exception as e2:
            print(f"   âŒ æ— æ³•åŠ è½½å‚è€ƒæ•°æ®: {e2}")
            return

    # ä¿å­˜åŸå§‹ä»£ç ï¼Œç”¨äºåç»­åˆå¹¶å’ŒæŸ¥æ‰¾
    df['åŸå§‹ä»£ç '] = df['ä»£ç '].copy()

    # å°è¯•è¡¥å……å‡çº¿å’Œè´¢åŠ¡æ•°æ® (å¦‚æœå®æ—¶æ•°æ®ç¼ºå¤±)
    try:
        ref_df = pd.read_csv('å‚è€ƒæ•°æ®/Table.xls', sep='\t', encoding='gbk', dtype=str)
        ref_df['ä»£ç '] = ref_df['ä»£ç '].str.replace('= "', '').str.replace('"', '') # æ¸…ç†å‚è€ƒæ•°æ®ä»£ç 
        ref_map = ref_df.set_index('ä»£ç ').to_dict('index')

        # åˆå¹¶å‚è€ƒæ•°æ®åˆ°ä¸»df
        # ä½¿ç”¨ merge æ›´é«˜æ•ˆå’Œå¥å£®
        df = df.set_index('åŸå§‹ä»£ç ').combine_first(ref_df.set_index('ä»£ç ')).reset_index()
        df = df.rename(columns={'index': 'åŸå§‹ä»£ç '}) # æ¢å¤åˆ—å

        print(f"   âœ… è¡¥å……äº† {len(ref_map)} æ¡å‚è€ƒæ•°æ®")
    except Exception as e:
        print(f"   âš ï¸ æ— æ³•è¡¥å……å‚è€ƒæ•°æ®: {e}")

    # å¯¹æ‰€æœ‰éœ€è¦æ•°å€¼åŒ–çš„åˆ—è¿›è¡Œæ¸…æ´—å’Œè½¬æ¢
    df = clean_and_convert_numeric(df, NUMERIC_COLS)

    # æ ¼å¼åŒ–ä»£ç ä¸ºExcelå…¬å¼ï¼Œä¾¿äºåœ¨Excelä¸­ç‚¹å‡»
    df['ä»£ç '] = df['åŸå§‹ä»£ç '].apply(lambda x: f'= "{str(x)}"')

    # æ ¼å¼åŒ–æ•°å€¼åˆ—ä¸ºå­—ç¬¦ä¸²ï¼Œç”¨äºæœ€ç»ˆè¾“å‡ºï¼Œä¿ç•™ä¸¤ä½å°æ•°
    for col in [c for c in NUMERIC_COLS if c in df.columns]:
        df[col] = df[col].apply(lambda x: f" {x:.2f}" if pd.notna(x) else " --")

    # å¤„ç†åç§°
    df['åç§°'] = df['åç§°'].apply(lambda x: f" {x}" if pd.notna(x) and not str(x).startswith(' ') else str(x))

    # æ·»åŠ åºå·
    df['åº'] = range(1, len(df) + 1)
    df['Unnamed: 16'] = '' # ç©ºåˆ—

    # é€‰æ‹©è¾“å‡ºåˆ—
    output_columns = [
        'åº', 'ä»£ç ', 'åç§°', 'æœ€æ–°', 'æ¶¨å¹…%', 'æœ€é«˜', 'æœ€ä½',
        'å®é™…æ¢æ‰‹%', 'æ‰€å±è¡Œä¸š', '20æ—¥å‡ä»·', '60æ—¥å‡ä»·',
        'å¸‚ç›ˆç‡(åŠ¨)', 'æ€»å¸‚å€¼', 'å½’å±å‡€åˆ©æ¶¦', 'æ˜¨æ”¶', 'å¼€ç›˜', 'Unnamed: 16'
    ]

    # ç¡®ä¿æ‰€æœ‰è¾“å‡ºåˆ—éƒ½å­˜åœ¨
    for col in output_columns:
        if col not in df.columns:
            df[col] = ' --' if col != 'Unnamed: 16' else ''

    final_df = df[output_columns].copy() # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…SettingWithCopyWarning

    # ä¿å­˜Aè‚¡æ•°æ®
    output_file1 = 'è¾“å‡ºæ•°æ®/Aè‚¡æ•°æ®.csv'
    try:
        final_df.to_csv(output_file1, index=False, encoding='utf-8-sig')
        print(f"\nâœ… Aè‚¡æ•°æ®å·²ä¿å­˜: {output_file1}")
    except Exception as e:
        print(f"\nâŒ æ— æ³•ä¿å­˜ A è‚¡æ•°æ®: {e}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")

    print(f"   å…± {len(final_df)} åªè‚¡ç¥¨")

    # ========== ç¬¬äºŒæ­¥ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ ==========
    print("\n2. è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹...")
    # ä¼ é€’åŸå§‹çš„dfï¼Œè®©train_neural_networkå†…éƒ¨è¿›è¡Œæ•°å€¼åŒ–å’Œæ¸…æ´—
    model, scaler, r2_score_nn = train_neural_network(df.copy()) # ä¼ é€’å‰¯æœ¬

    if model is None:
        print("   âŒ ç¥ç»ç½‘ç»œè®­ç»ƒå¤±è´¥ï¼Œæ— æ³•è¿›è¡Œåç»­ç­›é€‰ã€‚")
        return

    # ========== ç¬¬ä¸‰æ­¥ï¼šåˆ†æå…³è”è§„åˆ™ ==========
    print("\n3. åˆ†æå…³è”è§„åˆ™...")
    rules = analyze_association_rules(df.copy()) # ä¼ é€’å‰¯æœ¬

    # ========== ç¬¬å››æ­¥ï¼šåŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨ ==========
    print("\n4. åŠ¨æ€ç­›é€‰ä¼˜è´¨è‚¡ç¥¨...")

    # é‡æ–°åŠ è½½æˆ–ç¡®ä¿ df åŒ…å«åŸå§‹æ•°å€¼æ•°æ®ï¼Œä»¥ä¾¿ç¥ç»ç½‘ç»œè¯„åˆ†
    # è¿™é‡Œä½¿ç”¨åŸå§‹çš„ df (å·²ç»è¿‡ clean_and_convert_numeric å¤„ç†çš„)
    df_for_scoring = df.copy()
    df_for_scoring = clean_and_convert_numeric(df_for_scoring, NUMERIC_COLS)

    # åˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰è‚¡ç¥¨è¯„åˆ†çš„åˆ—
    df_for_scoring['ç¥ç»ç½‘ç»œè¯„åˆ†'] = df_for_scoring.apply(lambda row: predict_score_with_nn(row, model, scaler), axis=1)

    quality_stocks = []
    # åˆå§‹é˜ˆå€¼å¯ä»¥æ ¹æ®ç¥ç»ç½‘ç»œè¯„åˆ†åˆ†å¸ƒæ¥å®šï¼Œæˆ–è€…å…ˆè®¾ä¸€ä¸ªè¾ƒä½çš„å€¼
    # æ¯”å¦‚ï¼Œå–æ‰€æœ‰è‚¡ç¥¨è¯„åˆ†çš„20%åˆ†ä½æ•°ä½œä¸ºåˆå§‹é˜ˆå€¼
    if not df_for_scoring['ç¥ç»ç½‘ç»œè¯„åˆ†'].empty:
        initial_threshold = df_for_scoring['ç¥ç»ç½‘ç»œè¯„åˆ†'].quantile(0.75) # å–75%åˆ†ä½æ•°
    else:
        initial_threshold = 0.0
    
    threshold = initial_threshold
    print(f"   åˆå§‹ç­›é€‰é˜ˆå€¼: {threshold:.4f}")

    # ç»Ÿè®¡
    stats = {'F': 0, 'G': 0, 'H': 0, 'I': 0, 'J': 0}

    for idx, row in df_for_scoring.iterrows():
        score_nn = row['ç¥ç»ç½‘ç»œè¯„åˆ†']  # ç¥ç»ç½‘ç»œè¯„åˆ†
        conditions = ""

        # ç»Ÿè®¡ï¼ˆåŸå§‹è¯„åˆ†æ–¹å¼çš„ç»Ÿè®¡ï¼‰
        features = calculate_features(row)
        if features[0] == 1: stats['F'] += 1
        if features[1] == 1: stats['G'] += 1
        if features[2] > 0.3: stats['H'] += 1
        if features[3] <= 20: stats['I'] += 1 # æ¢æ‰‹ç‡ <= 20%
        if features[4] >= 300: stats['J'] += 1 # å¸‚å€¼ >= 300äº¿

        # ç»¼åˆè¯„åˆ†ï¼šç¥ç»ç½‘ç»œè¯„åˆ† + å…³è”è§„åˆ™åŠ æƒ
        # è¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œä½ éœ€è¦æ ¹æ®ä½ çš„å…³è”è§„åˆ™åˆ†æç»“æœæ¥è®¾è®¡åŠ æƒç­–ç•¥
        score_rules = 0.0  # åˆå§‹å…³è”è§„åˆ™è¯„åˆ†
        # ç¤ºä¾‹ï¼šå¦‚æœè‚¡ç¥¨æ»¡è¶³æŸäº›å…³è”è§„åˆ™ï¼Œåˆ™å¢åŠ è¯„åˆ†
        # ä½ éœ€è¦æ ¹æ®ä½ çš„å…³è”è§„åˆ™åˆ†æç»“æœæ¥è®¾è®¡å…·ä½“çš„è§„åˆ™åˆ¤æ–­é€»è¾‘
        # ä¾‹å¦‚ï¼š
        # if not rules.empty:
        #     for _, rule_row in rules.iterrows():
        #         antecedent = list(rule_row['antecedents'])
        #         consequent = list(rule_row['consequents'])
        #         # å‡è®¾ä½ çš„è§„åˆ™æ˜¯ 'æ¶¨å¹…_é«˜' -> 'å‡€åˆ©æ¶¦_é«˜'
        #         if 'æ¶¨å¹…_é«˜' in antecedent and row['æ¶¨å¹…%'] > 3.0 and \
        #            'å‡€åˆ©æ¶¦_é«˜' in consequent and row['å½’å±å‡€åˆ©æ¶¦'] > 0.5:
        #             score_rules += rule_row['confidence'] * 0.1 # ç®€å•åŠ æƒ

        final_score = score_nn + score_rules  # ç»¼åˆè¯„åˆ†

        # åˆ¤æ–­æ˜¯å¦è¾¾æ ‡
        if final_score >= threshold:
            code = str(row['åŸå§‹ä»£ç ']).strip() # ä½¿ç”¨åŸå§‹ä»£ç 
            quality_stocks.append({
                'ä»£ç ': code,
                'åç§°': str(row['åç§°']).strip(),
                'è¡Œä¸š': str(row['æ‰€å±è¡Œä¸š']).strip(),
                'ä¼˜è´¨ç‡': final_score,
                'æ»¡è¶³æ¡ä»¶': conditions,
                'æ¶¨å¹…': str(row['æ¶¨å¹…%']).strip()
            })

    # æ‰“å°ç»Ÿè®¡
    total_stocks_evaluated = len(df_for_scoring)
    if total_stocks_evaluated > 0:
        print(f"\n   æ¡ä»¶æ»¡è¶³ç»Ÿè®¡ï¼ˆå…±{total_stocks_evaluated}åªè‚¡ç¥¨ï¼‰ï¼š")
        print(f"   Fåˆ—(ä»·æ ¼ä½ç½®): {stats['F']}åª ({stats['F']/total_stocks_evaluated*100:.1f}%)")
        print(f"   Gåˆ—(æ¶¨å¹…æ¡ä»¶): {stats['G']}åª ({stats['G']/total_stocks_evaluated*100:.1f}%)")
        print(f"   Håˆ—(å‡€åˆ©æ¶¦>=0.3äº¿): {stats['H']}åª ({stats['H']/total_stocks_evaluated*100:.1f}%)")
        print(f"   Iåˆ—(æ¢æ‰‹ç‡<=20%): {stats['I']}åª ({stats['I']/total_stocks_evaluated*100:.1f}%)")
        print(f"   Jåˆ—(å¸‚å€¼>=300äº¿): {stats['J']}åª ({stats['J']/total_stocks_evaluated*100:.1f}%)")

    # æŒ‰ä¼˜è´¨ç‡é™åºæ’åº
    quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)

    # å¦‚æœç»“æœå¤ªå°‘ï¼Œå°è¯•é™ä½é˜ˆå€¼ï¼Œæˆ–è€…ç›´æ¥å–å‰Nå
    if len(quality_stocks) < 10 and len(df_for_scoring) > 0:
        print(f"\n   âš ï¸ åªæ‰¾åˆ°{len(quality_stocks)}åªè‚¡ç¥¨ï¼Œå°è¯•é™ä½é˜ˆå€¼å¹¶å–å‰12å...")
        # é‡æ–°æ ¹æ®æ‰€æœ‰è‚¡ç¥¨çš„ç¥ç»ç½‘ç»œè¯„åˆ†æ’åºï¼Œå–å‰12å
        all_stocks_sorted_by_nn_score = sorted(
            df_for_scoring.to_dict('records'),
            key=lambda x: x['ç¥ç»ç½‘ç»œè¯„åˆ†'] if pd.notna(x['ç¥ç»ç½‘ç»œè¯„åˆ†']) else -np.inf,
            reverse=True
        )
        
        quality_stocks = []
        for stock_data in all_stocks_sorted_by_nn_score[:12]:
            code = str(stock_data['åŸå§‹ä»£ç ']).strip()
            quality_stocks.append({
                'ä»£ç ': code,
                'åç§°': str(stock_data['åç§°']).strip(),
                'è¡Œä¸š': str(stock_data['æ‰€å±è¡Œä¸š']).strip(),
                'ä¼˜è´¨ç‡': stock_data['ç¥ç»ç½‘ç»œè¯„åˆ†'],
                'æ»¡è¶³æ¡ä»¶': "", # æ­¤æ—¶æ¡ä»¶ä¸æ˜ç¡®ï¼Œæ¸…ç©º
                'æ¶¨å¹…': str(stock_data['æ¶¨å¹…%']).strip()
            })
        
        # é‡æ–°æ’åºï¼Œç¡®ä¿æœ€ç»ˆç»“æœçš„ä¼˜è´¨ç‡æ˜¯æ­£ç¡®çš„
        quality_stocks = sorted(quality_stocks, key=lambda x: (x['ä¼˜è´¨ç‡'], x['ä»£ç ']), reverse=True)
        # æ›´æ–°é˜ˆå€¼ï¼Œä»¥ä¾¿æŠ¥å‘Š
        if quality_stocks:
            threshold = quality_stocks[-1]['ä¼˜è´¨ç‡'] # æ­¤æ—¶é˜ˆå€¼æ˜¯ç¬¬12åçš„ä¼˜è´¨ç‡
        else:
            threshold = 0.0


    # è®¡ç®—æœ€ç»ˆæ¨¡å‹è¯„åˆ† (ç¤ºä¾‹)
    final_model_score = (r2_score_nn * 100 + len(rules) * 0.5) # ç¥ç»ç½‘ç»œ R^2 ä¹˜ä»¥100ï¼Œå…³è”è§„åˆ™æ•°é‡åŠ æƒ
    # è¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è¯„åˆ†è®¡ç®—æ–¹å¼

    # ä¿å­˜ä¼˜è´¨è‚¡ç¥¨
    output_file2 = 'è¾“å‡ºæ•°æ®/ä¼˜è´¨è‚¡ç¥¨.txt'
    try:
        with open(output_file2, 'w', encoding='utf-8') as f:
            f.write("è‹æ°é‡åŒ–ç­–ç•¥ - ä¼˜è´¨è‚¡ç¥¨ç­›é€‰ç»“æœ (Scikit-learn + Optuna ç¥ç»ç½‘ç»œè¯„åˆ† + Apriori å…³è”è§„åˆ™)\n")
            f.write(f"ç­›é€‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ¨¡å‹æœ€ç»ˆè¯„åˆ†: {final_model_score:.4f}\n")  # æ˜¾ç¤ºæ¨¡å‹æœ€ç»ˆè¯„åˆ†
            f.write(f"ç¥ç»ç½‘ç»œ R^2: {r2_score_nn:.4f}\n")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„ R^2
            f.write(f"å…³è”è§„åˆ™æ•°é‡: {len(rules)}\n")  # æ˜¾ç¤ºå…³è”è§„åˆ™æ•°é‡
            f.write(f"æœ€ç»ˆç­›é€‰é˜ˆå€¼: {threshold:.4f}\n")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„é˜ˆå€¼
            f.write(f"ä¼˜è´¨è‚¡ç¥¨æ•°é‡: {len(quality_stocks)}\n")
            f.write("=" * 50 + "\n\n")

            for stock in quality_stocks:
                f.write(f"è‚¡ç¥¨ä»£ç : {stock['ä»£ç ']}\n")
                f.write(f"è‚¡ç¥¨åç§°: {stock['åç§°']}\n")
                f.write(f"æ‰€å±è¡Œä¸š: {stock['è¡Œä¸š']}\n")
                f.write(f"ä¼˜è´¨ç‡: {stock['ä¼˜è´¨ç‡']:.4f}\n")  # æ˜¾ç¤ºç¥ç»ç½‘ç»œçš„è¯„åˆ†
                f.write(f"æ»¡è¶³æ¡ä»¶: {stock['æ»¡è¶³æ¡ä»¶']}\n")
                f.write(f"ä»Šæ—¥æ¶¨å¹…: {stock['æ¶¨å¹…']}\n")
                f.write("-" * 30 + "\n")

        print(f"\nâœ… ä¼˜è´¨è‚¡ç¥¨å·²ä¿å­˜: {output_file2}")
    except Exception as e:
        print(f"\nâŒ æ— æ³•ä¿å­˜ä¼˜è´¨è‚¡ç¥¨: {e}")
        print(f"   é”™è¯¯ä¿¡æ¯: {e}")

    print(f"   æ‰¾åˆ° {len(quality_stocks)} åªä¼˜è´¨è‚¡ç¥¨ï¼ˆæœ€ç»ˆé˜ˆå€¼={threshold:.4f}ï¼‰")

    if len(quality_stocks) > 0:
        print(f"\nğŸ¯ ä»Šæ—¥ä¼˜è´¨è‚¡ç¥¨åˆ—è¡¨ï¼š")
        print("=" * 60)
        print(f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'æ¶¨å¹…%':<8} {'ä¼˜è´¨ç‡':<10}")
        print("-" * 60)
        for stock in quality_stocks[:12]: # ç¡®ä¿åªæ‰“å°å‰12ä¸ª
            print(f"{stock['ä»£ç ']:<10} {stock['åç§°']:<12} {stock['æ¶¨å¹…']:<8} {stock['ä¼˜è´¨ç‡']:.4f}")
    else:
        print("\nâš ï¸ ä»Šæ—¥æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ä¼˜è´¨è‚¡ç¥¨")
        print("   å¯èƒ½åŸå› ï¼š")
        print("   1. å¸‚åœºæ•´ä½“è¡¨ç°ä¸ä½³ï¼Œæ¶¨å¹…ä¸è¶³")
        print("   2. æ•°æ®è·å–ä¸å®Œæ•´")
        print("3. ç­›é€‰æ¡ä»¶è¿‡äºä¸¥æ ¼ï¼Œæˆ–æ¨¡å‹åŒºåˆ†åº¦ä¸è¶³")

    print("\n" + "=" * 60)
    print("âœ… ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()

